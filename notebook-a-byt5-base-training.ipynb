{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "76a4c494",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:08:08.388312Z",
                    "iopub.status.busy": "2026-01-08T08:08:08.387988Z",
                    "iopub.status.idle": "2026-01-08T08:08:08.619248Z",
                    "shell.execute_reply": "2026-01-08T08:08:08.616178Z",
                    "shell.execute_reply.started": "2026-01-08T08:08:08.388289Z"
                },
                "papermill": {
                    "duration": 0.184686,
                    "end_time": "2025-12-25T10:28:15.031482",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:14.846796",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Thu Jan  8 08:08:08 2026       \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
                        "|-----------------------------------------+------------------------+----------------------+\n",
                        "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
                        "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
                        "|                                         |                        |               MIG M. |\n",
                        "|=========================================+========================+======================|\n",
                        "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
                        "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
                        "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
                        "|                                         |                        |                  N/A |\n",
                        "+-----------------------------------------+------------------------+----------------------+\n",
                        "                                                                                         \n",
                        "+-----------------------------------------------------------------------------------------+\n",
                        "| Processes:                                                                              |\n",
                        "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
                        "|        ID   ID                                                               Usage      |\n",
                        "|=========================================================================================|\n",
                        "|  No running processes found                                                             |\n",
                        "+-----------------------------------------------------------------------------------------+\n"
                    ]
                }
            ],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "853653de",
            "metadata": {
                "papermill": {
                    "duration": 0.006431,
                    "end_time": "2025-12-25T10:28:15.044668",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:15.038237",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A1. Install required libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "964c2326",
            "metadata": {
                "_kg_hide-input": true,
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:08:08.621873Z",
                    "iopub.status.busy": "2026-01-08T08:08:08.621507Z",
                    "iopub.status.idle": "2026-01-08T08:08:13.845850Z",
                    "shell.execute_reply": "2026-01-08T08:08:13.844989Z",
                    "shell.execute_reply.started": "2026-01-08T08:08:08.621830Z"
                },
                "papermill": {
                    "duration": 4.824313,
                    "end_time": "2025-12-25T10:28:19.875330",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:15.051017",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h"
                    ]
                }
            ],
            "source": [
                "!pip install -q evaluate sacrebleu"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5dd5f368",
            "metadata": {
                "papermill": {
                    "duration": 0.006597,
                    "end_time": "2025-12-25T10:28:19.889807",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:19.883210",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A2. Imports & config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "ec80cf0e",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:08:13.847573Z",
                    "iopub.status.busy": "2026-01-08T08:08:13.847199Z",
                    "iopub.status.idle": "2026-01-08T08:08:45.115144Z",
                    "shell.execute_reply": "2026-01-08T08:08:45.114538Z",
                    "shell.execute_reply.started": "2026-01-08T08:08:13.847522Z"
                },
                "papermill": {
                    "duration": 33.038891,
                    "end_time": "2025-12-25T10:28:52.935206",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:19.896315",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-08 08:08:28.427385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
                        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                        "E0000 00:00:1767859708.621266      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "E0000 00:00:1767859708.681230      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "W0000 00:00:1767859709.163048      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "W0000 00:00:1767859709.163087      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "W0000 00:00:1767859709.163090      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "W0000 00:00:1767859709.163093      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import gc\n",
                "import re\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSeq2SeqLM,\n",
                "    DataCollatorForSeq2Seq,\n",
                "    Seq2SeqTrainer,\n",
                "    Seq2SeqTrainingArguments,\n",
                "    set_seed\n",
                ")\n",
                "import evaluate\n",
                "\n",
                "# Memory/precision safety tweaks (helps avoid OOM on P100/T4)\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "try:\n",
                "    torch.backends.cuda.matmul.allow_tf32 = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "    torch.set_float32_matmul_precision(\"medium\")\n",
                "except Exception:\n",
                "    pass\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b38e3a6c",
            "metadata": {
                "papermill": {
                    "duration": 0.006759,
                    "end_time": "2025-12-25T10:28:52.949032",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:52.942273",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A3. Set constants (DO NOT change yet)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cbc9780c",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:08:45.116661Z",
                    "iopub.status.busy": "2026-01-08T08:08:45.116044Z",
                    "iopub.status.idle": "2026-01-08T08:08:45.121485Z",
                    "shell.execute_reply": "2026-01-08T08:08:45.120695Z",
                    "shell.execute_reply.started": "2026-01-08T08:08:45.116634Z"
                },
                "papermill": {
                    "duration": 0.013576,
                    "end_time": "2025-12-25T10:28:52.969057",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:52.955481",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [],
            "source": [
                "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\n",
                "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
                "OUTPUT_DIR = \"/kaggle/working/byt5-base-saved\"\n",
                "\n",
                "# ByT5 is character-based. 256 balances coverage with memory efficiency\n",
                "MAX_LENGTH = 256\n",
                "PREFIX = \"translate Akkadian to English: \"\n",
                "\n",
                "# OOM guard: allow dynamic reduction controlled by env var\n",
                "try:\n",
                "    env_max_len = int(os.getenv(\"BYT5_MAX_LENGTH\", \"0\"))\n",
                "    if env_max_len >= 200:\n",
                "        MAX_LENGTH = env_max_len\n",
                "        print(f\"[INFO] MAX_LENGTH overridden by env: {MAX_LENGTH}\")\n",
                "except Exception:\n",
                "    pass"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9041d778",
            "metadata": {
                "papermill": {
                    "duration": 0.006442,
                    "end_time": "2025-12-25T10:28:52.982062",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:52.975620",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A4. Data Loading & Cleaning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e9e6077f",
            "metadata": {},
            "source": [
                "# A3.5. DATA PREPARATION GUIDE: Handling Akkadian Formatting Issues\n",
                "\n",
                "## Problem: \"Garbage In, Garbage Out\"\n",
                "Akkadian texts contain complex formatting that can break ML pipelines if not handled properly.\n",
                "\n",
                "## Formatting Issues to Handle\n",
                "\n",
                "### 1. Scribal Notations (Remove)\n",
                "- `!` - Certain reading (remove)\n",
                "- `?` - Questionable reading (remove)\n",
                "- `/` - Line divider (remove)\n",
                "- `:` or `.` - Word divider (remove)\n",
                "- `< >` - Scribal insertions (keep content, remove brackets)\n",
                "- `( )` - Comments/erasures (remove entirely)\n",
                "- `˹ ˺` - Half brackets for partially broken signs (remove)\n",
                "- `[ ]` - Clearly broken signs (keep content, remove brackets)\n",
                "- `<< >>` - Errant signs (remove entirely)\n",
                "\n",
                "### 2. Gaps & Lacunae (Standardize)\n",
                "- `[x]` → `<gap>`\n",
                "- `x` → `<gap>`\n",
                "- `xx` → `<gap>`\n",
                "- `…` → `<big_gap>`\n",
                "- `……` → `<big_gap>`\n",
                "- `[... ...]` → `<big_gap>`\n",
                "- Multiple `.3` or `...` sequences → `<big_gap>`\n",
                "\n",
                "### 3. Determinatives (Keep content, remove brackets)\n",
                "- `{d}` - Deity (remove brackets)\n",
                "- `{ki}` - Earth/location (remove brackets)\n",
                "- `{lu₂}` - Person (remove brackets)\n",
                "- `{e₂}` - Building (remove brackets)\n",
                "- And 10+ others...\n",
                "\n",
                "### 4. Subscripts & Superscripts (Normalize)\n",
                "- `a₂` → `a2`, `a₃` → `a3`, etc.\n",
                "- `il₅` → `il5`, etc.\n",
                "- Works with Unicode characters (U+2080-U+2089)\n",
                "\n",
                "### 5. Special Characters (Handle as-is or normalize)\n",
                "- `š` (U+0161), `Š` (U+0160)\n",
                "- `ṣ` (U+1E63), `Ṣ` (U+1E62)\n",
                "- `ṭ` (U+1E6D), `Ṭ` (U+1E6C)\n",
                "- `ḫ` (U+1E2B), `Ḫ` (U+1E2A)\n",
                "- `ʾ` (U+02BE) - Akkadian letter marker\n",
                "\n",
                "### 6. Capitalization Rules (Preserve)\n",
                "- First letter capital = Proper noun (personal/place name)\n",
                "- ALL CAPS = Sumerian logogram (preserve for domain knowledge)\n",
                "\n",
                "## Processing Order\n",
                "1. Normalize subscripts FIRST (₀-₉ → 0-9)\n",
                "2. Handle gaps (complex patterns first, then simple)\n",
                "3. Remove scribal notations\n",
                "4. Extract content from bracketed structures\n",
                "5. Clean whitespace\n",
                "6. Validate output (length checks, character validation)\n",
                "\n",
                "## Data Validation Checks\n",
                "✓ No empty strings after cleaning\n",
                "✓ Source length >= 3 words\n",
                "✓ Target length >= 3 words\n",
                "✓ Length ratio between 0.2 and 5.0\n",
                "✓ No duplicate pairs\n",
                "✓ All special characters properly handled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9035f86f",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:08:45.123948Z",
                    "iopub.status.busy": "2026-01-08T08:08:45.123665Z",
                    "iopub.status.idle": "2026-01-08T08:09:40.314471Z",
                    "shell.execute_reply": "2026-01-08T08:09:40.313446Z",
                    "shell.execute_reply.started": "2026-01-08T08:08:45.123915Z"
                },
                "papermill": {
                    "duration": 0.469755,
                    "end_time": "2025-12-25T10:28:53.458249",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:52.988494",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Raw documents: 1561\n",
                        "✓ Loading sentence alignment data...\n",
                        "Building aligned dataset using sentence map translations...\n",
                        "✓ Extracted 51 sentence pairs from map file\n",
                        "Aligned training examples (pre-filter): 1561\n",
                        "Aligned training examples (post-filter): 1528\n",
                        "\n",
                        "============================================================\n",
                        "MINING PUBLICATIONS FOR ADDITIONAL TRAINING DATA (FAST MODE)\n",
                        "============================================================\n",
                        "Total publication pages: 216602\n",
                        "Pages with translation keywords: 12500\n",
                        "Searching for matches...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bc564c056d4d4614a826ef142aac4f31",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Mining:   0%|          | 0/1500 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "⚠️  No additional pairs extracted (try adjusting regex or increasing candidates)\n",
                        "\n",
                        "============================================================\n",
                        "CHECKING PUBLISHED TEXTS\n",
                        "============================================================\n",
                        "Published texts available: 7953\n",
                        "Note: Will use these for monolingual pre-training\n",
                        "\n",
                        "Final dataset:\n",
                        "  Train: 1451 examples\n",
                        "  Validation: 77 examples\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"\n",
                "COMPREHENSIVE DATA PREPROCESSING FOR AKKADIAN TEXTS\n",
                "Handles all formatting issues mentioned in competition guidelines\n",
                "\"\"\"\n",
                "\n",
                "# ============================================================================\n",
                "# SUBSCRIPT & SUPERSCRIPT NORMALIZATION\n",
                "# ============================================================================\n",
                "SUBSCRIPT_TRANS = str.maketrans({\n",
                "    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \n",
                "    \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \n",
                "    \"ₓ\": \"x\"\n",
                "})\n",
                "\n",
                "def normalize_subscripts(text: str) -> str:\n",
                "    \"\"\"Convert subscript Unicode characters to regular numbers\"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    return text.translate(SUBSCRIPT_TRANS)\n",
                "\n",
                "# ============================================================================\n",
                "# GAP & LACUNAE HANDLING\n",
                "# ============================================================================\n",
                "def replace_gaps(text, keep_gaps=True):\n",
                "    \"\"\"\n",
                "    Replace various gap notations with standardized tokens.\n",
                "    Handles all gap patterns mentioned in competition guidelines.\n",
                "    \n",
                "    Args:\n",
                "        text: Input text with gaps\n",
                "        keep_gaps: If True, keeps <gap> and <big_gap> tokens.\n",
                "                  If False, removes them completely.\n",
                "    \n",
                "    Returns:\n",
                "        Text with normalized gap tokens\n",
                "    \"\"\"\n",
                "    if pd.isna(text): \n",
                "        return text\n",
                "    \n",
                "    # STEP 1: Complex gap patterns (order matters!)\n",
                "    # [...] patterns for multiple dots\n",
                "    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\.\\s*\\.\\s*\\]', '<big_gap>', text)  # [......]\n",
                "    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\.\\s*\\]', '<big_gap>', text)       # [....]\n",
                "    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\]', '<gap>', text)                 # [...] \n",
                "    \n",
                "    # Multiple .3 patterns with multiple dots\n",
                "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
                "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
                "    \n",
                "    # Multiple dots (.....)\n",
                "    text = re.sub(r'\\.{4,}', '<big_gap>', text)  # 4+ dots = big gap\n",
                "    \n",
                "    # STEP 2: Unicode gap markers\n",
                "    text = re.sub(r'……', '<big_gap>', text)      # Unicode horizontal ellipsis\n",
                "    text = re.sub(r'…', '<big_gap>', text)        # Unicode single ellipsis\n",
                "    \n",
                "    # STEP 3: Standard dot patterns\n",
                "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)  # Multiple ... groups\n",
                "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)  # Three dots\n",
                "    text = re.sub(r'\\.\\.', '<gap>', text)        # Two dots\n",
                "    \n",
                "    # STEP 4: [x] and [xx] patterns\n",
                "    text = re.sub(r'\\[x+\\]', '<gap>', text)      # [x] or [xx]\n",
                "    \n",
                "    # STEP 5: Bare x patterns\n",
                "    text = re.sub(r'(?:^|\\s)xx(?:\\s|$)', ' <gap> ', text)  # xx as separate word\n",
                "    text = re.sub(r'(?:^|\\s)x(?:\\s|$)', ' <gap> ', text)   # x as separate word\n",
                "    \n",
                "    # STEP 6: Remove gaps if not needed\n",
                "    if not keep_gaps:\n",
                "        text = re.sub(r'<big_gap>', '', text)\n",
                "        text = re.sub(r'<gap>', '', text)\n",
                "    \n",
                "    return text\n",
                "\n",
                "# ============================================================================\n",
                "# SCRIBAL NOTATION REMOVAL\n",
                "# ============================================================================\n",
                "def remove_scribal_notations(text):\n",
                "    \"\"\"\n",
                "    Remove modern scribal notations that are not meaningful for translation.\n",
                "    These are editorial marks added by scholars, not part of the original text.\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # Remove line number markers (1, 5, 10, 1', 1'')\n",
                "    text = re.sub(r'\\b\\d+\\'?\\s*\\'?\\s*\\b', ' ', text)\n",
                "    \n",
                "    # Remove uncertainty markers\n",
                "    text = re.sub(r'[!?]', ' ', text)  # ! = certain, ? = uncertain\n",
                "    \n",
                "    # Remove other scribal punctuation\n",
                "    text = re.sub(r'[/:·]', ' ', text)  # / = line divider, : = word divider, · = separator\n",
                "    \n",
                "    return text\n",
                "\n",
                "# ============================================================================\n",
                "# BRACKETED CONTENT HANDLING\n",
                "# ============================================================================\n",
                "def handle_brackets(text):\n",
                "    \"\"\"\n",
                "    Handle various bracket types according to guidelines.\n",
                "    \n",
                "    - ( ) Remove entirely (comments/erasures)\n",
                "    - < > Keep content (scribal insertions)\n",
                "    - [ ] Keep content (clearly broken signs)\n",
                "    - { } Keep content (determinatives)\n",
                "    - << >> Remove entirely (errant signs)\n",
                "    - ˹ ˺ Remove (half brackets for partially broken)\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # Remove comments and erasures (keep nothing)\n",
                "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
                "    \n",
                "    # Keep content from scribal insertions and broken signs\n",
                "    text = re.sub(r'<([^>]*)>', r'\\1', text)      # <content> → content\n",
                "    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)   # [content] → content\n",
                "    \n",
                "    # Determinatives: {content} → content (removes classifier brackets)\n",
                "    text = re.sub(r'\\{([^}]*)\\}', r'\\1', text)\n",
                "    \n",
                "    # Remove half brackets for partially broken signs\n",
                "    text = re.sub(r'[˹˺]', ' ', text)\n",
                "    \n",
                "    # Remove errant/erroneous signs entirely\n",
                "    text = re.sub(r'<<[^>]*>>', ' ', text)\n",
                "    \n",
                "    return text\n",
                "\n",
                "# ============================================================================\n",
                "# MAIN TRANSLITERATION CLEANING FUNCTION\n",
                "# ============================================================================\n",
                "def clean_translit(text, keep_gaps=True):\n",
                "    \"\"\"\n",
                "    Comprehensive normalization of Akkadian transliteration.\n",
                "    Handles all formatting issues in proper order.\n",
                "    \n",
                "    Processing order:\n",
                "    1. Normalize subscripts\n",
                "    2. Handle gaps\n",
                "    3. Remove scribal notations\n",
                "    4. Handle bracket types\n",
                "    5. Clean whitespace\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # STEP 1: Normalize subscripts/superscripts FIRST\n",
                "    text = normalize_subscripts(text)\n",
                "    \n",
                "    # STEP 2: Handle gaps (complex patterns)\n",
                "    text = replace_gaps(text, keep_gaps=keep_gaps)\n",
                "    \n",
                "    # STEP 3: Remove scribal notations\n",
                "    text = remove_scribal_notations(text)\n",
                "    \n",
                "    # STEP 4: Handle all bracket types\n",
                "    text = handle_brackets(text)\n",
                "    \n",
                "    # STEP 5: Clean whitespace\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    \n",
                "    return text.strip()\n",
                "\n",
                "# ============================================================================\n",
                "# TRANSLATION CLEANING FUNCTION\n",
                "# ============================================================================\n",
                "def clean_translation(text, has_gaps=False):\n",
                "    \"\"\"\n",
                "    Clean translation with minimal processing.\n",
                "    Keep as much content as possible.\n",
                "    \"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # Handle gap indicators if source has gaps\n",
                "    if not has_gaps:\n",
                "        text = text.replace(\"…\", \" \")\n",
                "    \n",
                "    # Clean whitespace\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    \n",
                "    return text.strip()\n",
                "\n",
                "# ============================================================================\n",
                "# DATA QUALITY FILTERING\n",
                "# ============================================================================\n",
                "def filter_quality(df):\n",
                "    \"\"\"\n",
                "    Filter out low-quality pairs based on validation checks.\n",
                "    \n",
                "    Validation criteria:\n",
                "    - Minimum 3 words in source and target\n",
                "    - Length ratio between 0.2 and 5.0\n",
                "    - No duplicate pairs\n",
                "    \"\"\"\n",
                "    # Calculate lengths\n",
                "    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n",
                "    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n",
                "    \n",
                "    # Minimum length check\n",
                "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
                "    \n",
                "    # Length ratio check (one language often longer than other)\n",
                "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
                "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
                "    \n",
                "    # Remove exact duplicates\n",
                "    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n",
                "    \n",
                "    # Cleanup\n",
                "    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
                "\n",
                "# ============================================================================\n",
                "# VALIDATION & REPORTING\n",
                "# ============================================================================\n",
                "def validate_preprocessing(original_df, cleaned_df):\n",
                "    \"\"\"\n",
                "    Report on preprocessing impact.\n",
                "    \"\"\"\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"DATA PREPROCESSING VALIDATION\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"Original samples: {len(original_df)}\")\n",
                "    print(f\"After cleaning: {len(cleaned_df)}\")\n",
                "    print(f\"Removed: {len(original_df) - len(cleaned_df)} samples\")\n",
                "    \n",
                "    if len(cleaned_df) > 0:\n",
                "        avg_src = cleaned_df[\"transliteration\"].str.split().str.len().mean()\n",
                "        avg_tgt = cleaned_df[\"translation\"].str.split().str.len().mean()\n",
                "        print(f\"Avg source length: {avg_src:.1f} words\")\n",
                "        print(f\"Avg target length: {avg_tgt:.1f} words\")\n",
                "        print(f\"Avg ratio (src/tgt): {avg_src/avg_tgt:.2f}\")\n",
                "    print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "# Replace gaps function (with corrected newlines and indentation)\n",
                "def replace_gaps(text, keep_gaps=True):\n",
                "    \"\"\"Replace various gap notations with standardized tokens\n",
                "    \n",
                "    Args:\n",
                "        keep_gaps: If True, keeps gap tokens (for test-like data).\n",
                "                   If False, removes them (for clean training).\n",
                "    \"\"\"\n",
                "    if pd.isna(text): \n",
                "        return text\n",
                "    \n",
                "    # Complex gap patterns (order matters)\n",
                "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
                "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
                "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
                "\n",
                "    # Simple gap patterns\n",
                "    text = re.sub(r'xx', '<gap>', text)\n",
                "    text = re.sub(r' x ', ' <gap> ', text)\n",
                "    text = re.sub(r'……', '<big_gap>', text)\n",
                "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
                "    text = re.sub(r'…', '<big_gap>', text)\n",
                "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
                "    \n",
                "    # Bracketed gaps\n",
                "    text = re.sub(r'\\[\\.\\.\\.+\\]', '<big_gap>', text)\n",
                "    text = re.sub(r'\\[x+\\]', '<gap>', text)\n",
                "    \n",
                "    if not keep_gaps:\n",
                "        # Remove gaps for clean training\n",
                "        text = re.sub(r'<big_gap>', '', text)\n",
                "        text = re.sub(r'<gap>', '', text)\n",
                "\n",
                "    return text\n",
                "\n",
                "def clean_translit(text, keep_gaps=True):\n",
                "    \"\"\"Normalize transliteration following competition guidance.\"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    text = normalize_subscripts(text)\n",
                "    # Apply gap replacement - KEEP gaps for domain matching\n",
                "    text = replace_gaps(text, keep_gaps=keep_gaps)\n",
                "    # Only remove scribal markers, keep gaps\n",
                "    text = re.sub(r\"<<[^>]*>>\", \" \", text)               # errant signs\n",
                "    text = re.sub(r\"[˹˺]\", \" \", text)                    # half brackets\n",
                "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)             # comments/erasures\n",
                "    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)         # determinatives\n",
                "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)            # scribal insertions keep content\n",
                "    text = re.sub(r\"[!?/:·]\", \" \", text)                 # scribal punctuation\n",
                "    text = re.sub(r\"\\s+\", \" \", text)\n",
                "    return text.strip()\n",
                "\n",
                "def clean_translation(text, has_gaps=False):\n",
                "    \"\"\"Clean translation, optionally keeping gap indicators\"\"\"\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    if not has_gaps:\n",
                "        text = text.replace(\"…\", \" \")\n",
                "    # Keep ... if source has gaps\n",
                "    text = re.sub(r\"\\s+\", \" \", text)\n",
                "    return text.strip()\n",
                "\n",
                "def filter_quality(df):\n",
                "    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n",
                "    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n",
                "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
                "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
                "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
                "    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n",
                "    return df.drop(columns=[\"src_len\", \"tgt_len\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "373e0f72",
            "metadata": {
                "papermill": {
                    "duration": 0.006698,
                    "end_time": "2025-12-25T10:28:53.471962",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:53.465264",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A5 . Tokenization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "af1a0a55",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:09:40.315996Z",
                    "iopub.status.busy": "2026-01-08T08:09:40.315665Z",
                    "iopub.status.idle": "2026-01-08T08:09:44.388152Z",
                    "shell.execute_reply": "2026-01-08T08:09:44.387424Z",
                    "shell.execute_reply.started": "2026-01-08T08:09:40.315966Z"
                },
                "papermill": {
                    "duration": 3.528045,
                    "end_time": "2025-12-25T10:28:57.006773",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:53.478728",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Tokenizer from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "999dfb221f974217845a0b5904c1b537",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9abd1a38cb6d47c299cb8c035d75b4aa",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "print(\"Loading Tokenizer from:\", MODEL_PATH)\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
                "\n",
                "def preprocess_function(examples):\n",
                "    inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n",
                "    targets = examples[\"translation\"]\n",
                "\n",
                "    model_inputs = tokenizer(\n",
                "        inputs, \n",
                "        max_length=MAX_LENGTH, \n",
                "        truncation=True, \n",
                "        padding=\"max_length\" # Consistent padding helps training stability\n",
                "    )\n",
                "    \n",
                "    labels = tokenizer(\n",
                "        targets, \n",
                "        max_length=MAX_LENGTH, \n",
                "        truncation=True, \n",
                "        padding=\"max_length\"\n",
                "    )\n",
                "\n",
                "    # Replace padding token id with -100 so it's ignored by the loss function\n",
                "    labels[\"input_ids\"] = [\n",
                "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
                "        for label in labels[\"input_ids\"]\n",
                "    ]\n",
                "\n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "# Process datasets\n",
                "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
                "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "680c9278",
            "metadata": {
                "papermill": {
                    "duration": 0.007545,
                    "end_time": "2025-12-25T10:28:57.023322",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:57.015777",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A6. Model Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "447be19d",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:09:44.389418Z",
                    "iopub.status.busy": "2026-01-08T08:09:44.389097Z",
                    "iopub.status.idle": "2026-01-08T08:09:45.231488Z",
                    "shell.execute_reply": "2026-01-08T08:09:45.230953Z",
                    "shell.execute_reply.started": "2026-01-08T08:09:44.389394Z"
                },
                "papermill": {
                    "duration": 1.398574,
                    "end_time": "2025-12-25T10:28:58.434786",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:57.036212",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Model from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n"
                    ]
                }
            ],
            "source": [
                "print(\"Loading Model from:\", MODEL_PATH)\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
                "\n",
                "# Data Collator handles dynamic padding during batching\n",
                "data_collator = DataCollatorForSeq2Seq(\n",
                "    tokenizer=tokenizer, \n",
                "    model=model,\n",
                "    label_pad_token_id=-100\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "314f0dc9",
            "metadata": {},
            "source": [
                "# A6. Optional: Monolingual Pre-Training on Akkadian Texts\n",
                "\n",
                "This step teaches the model Akkadian grammar and morphology BEFORE translation training.\n",
                "Uses published_texts.csv (8,000+ Akkadian texts) with Masked Language Modeling (MLM).\n",
                "\n",
                "Benefits:\n",
                "- Model learns to handle gaps naturally\n",
                "- Better understanding of Akkadian word structure\n",
                "- Improves low-resource translation performance\n",
                "\n",
                "Set ENABLE_MONO_PRETRAIN=True to enable (adds ~30min training time)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a86171f5",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:09:45.232827Z",
                    "iopub.status.busy": "2026-01-08T08:09:45.232532Z",
                    "iopub.status.idle": "2026-01-08T08:39:38.576328Z",
                    "shell.execute_reply": "2026-01-08T08:39:38.575692Z",
                    "shell.execute_reply.started": "2026-01-08T08:09:45.232802Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\n",
                        "============================================================\n",
                        "Loaded 5000 Akkadian texts for pre-training\n",
                        "Created 5000 MLM training examples\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f33dc53fd2a5406699e195b625901f66",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
                        "  warnings.warn(\n",
                        "/tmp/ipykernel_55/57318026.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
                        "  mlm_trainer = Seq2SeqTrainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting monolingual pre-training (1 epoch on Akkadian texts)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [157/157 29:11, Epoch 1/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>1.689100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>100</td>\n",
                            "      <td>1.261500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>150</td>\n",
                            "      <td>1.182600</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Monolingual pre-training complete\n",
                        "Model now understands Akkadian grammar and gaps better!\n"
                    ]
                }
            ],
            "source": [
                "# Monolingual Pre-Training Configuration\n",
                "ENABLE_MONO_PRETRAIN = bool(int(os.getenv(\"ENABLE_MONO_PRETRAIN\", \"1\")))  # Set to 1 to enable\n",
                "\n",
                "if ENABLE_MONO_PRETRAIN:\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
                "    \n",
                "    if os.path.exists(pub_texts_path):\n",
                "        # Load Akkadian-only texts\n",
                "        pub_texts_df = pd.read_csv(pub_texts_path)\n",
                "        akkadian_texts = pub_texts_df['transliteration'].dropna().astype(str).tolist()\n",
                "        akkadian_texts = [clean_translit(t, keep_gaps=True) for t in akkadian_texts]\n",
                "        akkadian_texts = [t for t in akkadian_texts if len(t.split()) >= 5 and len(t.split()) <= 200]\n",
                "        akkadian_texts = akkadian_texts[:5000]  # Limit for time\n",
                "        \n",
                "        print(f\"Loaded {len(akkadian_texts)} Akkadian texts for pre-training\")\n",
                "        \n",
                "        # Simple MLM approach: Mask random spans\n",
                "        from transformers import DataCollatorForSeq2Seq\n",
                "        \n",
                "        def create_mlm_examples(texts):\n",
                "            \"\"\"Create masked language modeling examples\"\"\"\n",
                "            mlm_examples = []\n",
                "            for text in texts:\n",
                "                tokens = text.split()\n",
                "                if len(tokens) < 5:\n",
                "                    continue\n",
                "                \n",
                "                # Mask 15% of tokens\n",
                "                n_mask = max(1, int(len(tokens) * 0.15))\n",
                "                mask_positions = np.random.choice(len(tokens), size=n_mask, replace=False)\n",
                "                \n",
                "                masked_text = []\n",
                "                for i, token in enumerate(tokens):\n",
                "                    if i in mask_positions:\n",
                "                        masked_text.append(\"<extra_id_0>\")  # T5-style sentinel\n",
                "                    else:\n",
                "                        masked_text.append(token)\n",
                "                \n",
                "                input_text = \" \".join(masked_text)\n",
                "                target_text = \" \".join([tokens[i] for i in mask_positions])\n",
                "                \n",
                "                mlm_examples.append({\n",
                "                    \"transliteration\": input_text,\n",
                "                    \"translation\": target_text\n",
                "                })\n",
                "            \n",
                "            return mlm_examples\n",
                "        \n",
                "        mlm_data = create_mlm_examples(akkadian_texts)\n",
                "        print(f\"Created {len(mlm_data)} MLM training examples\")\n",
                "        \n",
                "        # Create MLM dataset\n",
                "        mlm_dataset = Dataset.from_pandas(pd.DataFrame(mlm_data))\n",
                "        \n",
                "        def preprocess_mlm(examples):\n",
                "            inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n",
                "            targets = examples[\"translation\"]\n",
                "            model_inputs = tokenizer(\n",
                "                inputs,\n",
                "                max_length=MAX_LENGTH,\n",
                "                truncation=True,\n",
                "                padding=\"max_length\"\n",
                "            )\n",
                "            with tokenizer.as_target_tokenizer():\n",
                "                labels = tokenizer(\n",
                "                    targets,\n",
                "                    max_length=MAX_LENGTH,\n",
                "                    truncation=True,\n",
                "                    padding=\"max_length\"\n",
                "                )\n",
                "            model_inputs[\"labels\"] = [\n",
                "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
                "                for label in labels[\"input_ids\"]\n",
                "            ]\n",
                "            return model_inputs\n",
                "        \n",
                "        tokenized_mlm = mlm_dataset.map(preprocess_mlm, batched=True)\n",
                "        \n",
                "        # Short MLM pre-training (1-2 epochs)\n",
                "        mlm_args = Seq2SeqTrainingArguments(\n",
                "            output_dir=f\"{OUTPUT_DIR}_mlm\",\n",
                "            num_train_epochs=1,\n",
                "            learning_rate=3e-4,\n",
                "            per_device_train_batch_size=2,\n",
                "            gradient_accumulation_steps=8,\n",
                "            fp16=True,\n",
                "            save_strategy=\"no\",\n",
                "            eval_strategy=\"no\",\n",
                "            logging_steps=50,\n",
                "            report_to=\"none\"\n",
                "        )\n",
                "        \n",
                "        mlm_trainer = Seq2SeqTrainer(\n",
                "            model=model,\n",
                "            args=mlm_args,\n",
                "            train_dataset=tokenized_mlm,\n",
                "            tokenizer=tokenizer,\n",
                "            data_collator=data_collator,\n",
                "        )\n",
                "        \n",
                "        print(\"Starting monolingual pre-training (1 epoch on Akkadian texts)...\")\n",
                "        try:\n",
                "            mlm_trainer.train()\n",
                "            print(\"✓ Monolingual pre-training complete\")\n",
                "            print(\"Model now understands Akkadian grammar and gaps better!\")\n",
                "        except Exception as e:\n",
                "            print(f\"⚠️  MLM pre-training failed: {e}\")\n",
                "            print(\"Continuing with main training...\")\n",
                "    \n",
                "    else:\n",
                "        print(\"⚠️  published_texts.csv not found, skipping monolingual pre-training\")\n",
                "else:\n",
                "    print(\"\\n⚠️  Monolingual pre-training disabled (set ENABLE_MONO_PRETRAIN=1 to enable)\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c71300a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick data stats after mining and merge\n",
                "sup_count_est = len(train_df) - (len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0)\n",
                "print(\"\\n=== DATASET COUNTS ===\")\n",
                "print(f\"Supervised pairs (est.): {sup_count_est}\")\n",
                "print(f\"Mined pairs: {len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0}\")\n",
                "print(f\"Total pairs: {len(train_df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bfb3cc6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear GPU memory after monolingual pre-training to prevent OOM\n",
                "import gc\n",
                "del mlm_trainer\n",
                "del mlm_dataset\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "print(\"Memory cleared for main training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "31e880a1",
            "metadata": {
                "papermill": {
                    "duration": 0.007375,
                    "end_time": "2025-12-25T10:28:58.449624",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:58.442249",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A7. Training Arguments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f5dcb1e",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:39:38.577529Z",
                    "iopub.status.busy": "2026-01-08T08:39:38.577280Z",
                    "iopub.status.idle": "2026-01-08T08:39:38.613054Z",
                    "shell.execute_reply": "2026-01-08T08:39:38.612457Z",
                    "shell.execute_reply.started": "2026-01-08T08:39:38.577507Z"
                },
                "papermill": {
                    "duration": 0.169346,
                    "end_time": "2025-12-25T10:28:58.626361",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:58.457015",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# --- A7. Training Arguments (OPTIMIZED FOR MAXIMUM SCORE) ---\n",
                "print(\"=\"*60)\n",
                "print(\"TRAINING CONFIGURATION - OPTIMIZED FOR COMPETITION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "\n",
                "    # --- VALIDATION & CHECKPOINTING STRATEGY ---\n",
                "    save_strategy=\"epoch\",                # Save checkpoints each epoch\n",
                "    eval_strategy=\"epoch\",                # Evaluate each epoch to track progress\n",
                "    load_best_model_at_end=True,         # Load best model based on metric\n",
                "    metric_for_best_model=\"eval_loss\",   # Track eval loss\n",
                "    greater_is_better=False,             # Lower loss is better\n",
                "    save_total_limit=2,                  # Keep only 2 best checkpoints\n",
                "    \n",
                "    # --- LEARNING RATE & OPTIMIZATION ---\n",
                "    learning_rate=5e-5,                  # Lower LR for ByT5 stability\n",
                "    lr_scheduler_type=\"cosine_with_restarts\",  # Better than simple cosine\n",
                "    warmup_ratio=0.1,                    # 10% warmup for stable start\n",
                "    warmup_steps=None,                   # Use ratio instead\n",
                "    \n",
                "    # --- BATCH SIZE & GRADIENT ACCUMULATION ---\n",
                "    per_device_train_batch_size=1,       # ByT5 requires small batches\n",
                "    per_device_eval_batch_size=2,        # Can be larger for eval\n",
                "    gradient_accumulation_steps=16,      # Effective batch = 16\n",
                "    gradient_checkpointing=True,         # Memory efficiency\n",
                "    \n",
                "    # --- EPOCHS & TRAINING DURATION ---\n",
                "    num_train_epochs=20,                 # More epochs for better convergence\n",
                "    max_steps=-1,                        # No step limit, use epochs\n",
                "    \n",
                "    # --- REGULARIZATION ---\n",
                "    weight_decay=0.01,                   # Prevent overfitting\n",
                "    label_smoothing_factor=0.1,          # Smooth labels for generalization\n",
                "    max_grad_norm=1.0,                   # Gradient clipping\n",
                "    \n",
                "    # --- GENERATION SETTINGS FOR EVAL ---\n",
                "    predict_with_generate=True,          # Generate during evaluation\n",
                "    generation_max_length=512,           # Match test expectations\n",
                "    generation_num_beams=8,              # Higher beams for quality\n",
                "    \n",
                "    # --- PRECISION & PERFORMANCE ---\n",
                "    fp16=False,                          # ByT5 can be unstable with fp16\n",
                "    bf16=False,                          # Use full precision\n",
                "    \n",
                "    # --- LOGGING & REPORTING ---\n",
                "    report_to=\"none\",\n",
                "    logging_strategy=\"steps\",\n",
                "    logging_steps=25,                    # Log frequently to monitor\n",
                "    logging_first_step=True,\n",
                "    \n",
                "    # --- ADDITIONAL OPTIMIZATIONS ---\n",
                "    dataloader_num_workers=2,            # Parallel data loading\n",
                "    dataloader_pin_memory=True,          # Faster data transfer\n",
                "    remove_unused_columns=True,\n",
                "    \n",
                "    # --- EARLY STOPPING (if needed) ---\n",
                "    # Uncomment to enable early stopping\n",
                "    # load_best_model_at_end=True,\n",
                "    # metric_for_best_model=\"eval_loss\",\n",
                ")\n",
                "\n",
                "print(f\"\\nKey Settings:\")\n",
                "print(f\"  Effective batch size: {1 * 16} (1 × 16 accumulation)\")\n",
                "print(f\"  Total epochs: 20\")\n",
                "print(f\"  Learning rate: 5e-5 with cosine restarts\")\n",
                "print(f\"  Evaluation: Every epoch\")\n",
                "print(f\"  Generation beams: 8\")\n",
                "print(\"=\"*60 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77b0d460",
            "metadata": {
                "papermill": {
                    "duration": 0.006845,
                    "end_time": "2025-12-25T10:28:58.640314",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:58.633469",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A8. Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "34c0a3fd",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:39:38.614126Z",
                    "iopub.status.busy": "2026-01-08T08:39:38.613826Z",
                    "iopub.status.idle": "2026-01-08T08:39:39.370781Z",
                    "shell.execute_reply": "2026-01-08T08:39:39.369857Z",
                    "shell.execute_reply.started": "2026-01-08T08:39:38.614094Z"
                },
                "papermill": {
                    "duration": 22.854214,
                    "end_time": "2025-12-25T10:29:21.501227",
                    "exception": false,
                    "start_time": "2025-12-25T10:28:58.647013",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_55/1667860881.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer = Seq2SeqTrainer(\n"
                    ]
                }
            ],
            "source": [
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "\n",
                "# Force aggressive memory cleanup\n",
                "import gc\n",
                "torch.cuda.empty_cache()\n",
                "gc.collect()\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_train,\n",
                "    eval_dataset=tokenized_val,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7d909d2f",
            "metadata": {
                "papermill": {
                    "duration": 0.006946,
                    "end_time": "2025-12-25T10:29:21.515664",
                    "exception": false,
                    "start_time": "2025-12-25T10:29:21.508718",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A9. Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8847867",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2026-01-08T08:39:39.372178Z",
                    "iopub.status.busy": "2026-01-08T08:39:39.371813Z",
                    "iopub.status.idle": "2026-01-08T08:43:10.281101Z",
                    "shell.execute_reply": "2026-01-08T08:43:10.279836Z",
                    "shell.execute_reply.started": "2026-01-08T08:39:39.372152Z"
                },
                "papermill": {
                    "duration": 5732.305735,
                    "end_time": "2025-12-25T12:04:53.828320",
                    "exception": false,
                    "start_time": "2025-12-25T10:29:21.522585",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Training with Memory Fixes...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='2' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [  2/552 : < :, Epoch 0.02/12]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[WARNING] CUDA OOM detected. Attempting recovery: reducing MAX_LENGTH and accumulation.\n",
                        "New MAX_LENGTH: 342\n"
                    ]
                },
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.19 MiB is free. Process 3364 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 350.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_55/1183986820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Continue training from current state if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    309\u001b[0m             )\n\u001b[1;32m    310\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             ) if torch.amp.is_autocast_available(ctx.device_type) else contextlib.nullcontext()\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_autocast_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_autocast_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdetached_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.19 MiB is free. Process 3364 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 350.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
                    ]
                }
            ],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STARTING OPTIMIZED TRAINING - ByT5\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nTraining Strategy:\")\n",
                "print(\"✓ 20 epochs with evaluation each epoch\")\n",
                "print(\"✓ Cosine learning rate with restarts\")\n",
                "print(\"✓ Best model selection based on eval loss\")\n",
                "print(\"✓ Label smoothing for generalization\")\n",
                "print(\"✓ Gradient clipping for stability\")\n",
                "print(\"\\nExpected improvements:\")\n",
                "print(\"• Better handling of Akkadian morphology (character-level)\")\n",
                "print(\"• Reduced overfitting through regularization\")\n",
                "print(\"• Higher BLEU/chrF++ scores from beam search\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "# OOM-safe training wrapper with recovery\n",
                "try:\n",
                "    trainer.train()\n",
                "    print(\"\\n✓ Training completed successfully!\")\n",
                "    \n",
                "except RuntimeError as e:\n",
                "    if \"out of memory\" in str(e).lower():\n",
                "        print(\"\\n[WARNING] CUDA OOM detected. Implementing recovery strategy...\")\n",
                "        \n",
                "        # Strategy 1: Reduce gradient accumulation\n",
                "        training_args.gradient_accumulation_steps = max(8, training_args.gradient_accumulation_steps // 2)\n",
                "        print(f\"  → Reduced gradient accumulation to {training_args.gradient_accumulation_steps}\")\n",
                "        \n",
                "        # Strategy 2: Clear memory\n",
                "        torch.cuda.empty_cache()\n",
                "        gc.collect()\n",
                "        \n",
                "        # Strategy 3: Reduce max length slightly\n",
                "        try:\n",
                "            MAX_LENGTH = max(200, int(MAX_LENGTH * 0.9))\n",
                "            print(f\"  → Reduced MAX_LENGTH to {MAX_LENGTH}\")\n",
                "        except Exception:\n",
                "            pass\n",
                "        \n",
                "        # Retry with adjusted settings\n",
                "        print(\"  → Retrying training with adjusted settings...\")\n",
                "        try:\n",
                "            # Recreate trainer with new settings\n",
                "            trainer = Seq2SeqTrainer(\n",
                "                model=model,\n",
                "                args=training_args,\n",
                "                train_dataset=tokenized_train,\n",
                "                eval_dataset=tokenized_val,\n",
                "                tokenizer=tokenizer,\n",
                "                data_collator=data_collator,\n",
                "            )\n",
                "            trainer.train()\n",
                "            print(\"✓ Training completed with adjusted settings!\")\n",
                "        except Exception as retry_error:\n",
                "            print(f\"✗ Training failed even after adjustment: {retry_error}\")\n",
                "            print(\"Suggestions:\")\n",
                "            print(\"  1. Reduce num_train_epochs\")\n",
                "            print(\"  2. Set gradient_accumulation_steps=8\")\n",
                "            print(\"  3. Disable gradient_checkpointing\")\n",
                "            raise\n",
                "    else:\n",
                "        raise\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TRAINING PHASE COMPLETE\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9c28d5b8",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2026-01-08T08:43:10.281721Z",
                    "iopub.status.idle": "2026-01-08T08:43:10.282019Z",
                    "shell.execute_reply": "2026-01-08T08:43:10.281862Z",
                    "shell.execute_reply.started": "2026-01-08T08:43:10.281847Z"
                },
                "papermill": {
                    "duration": 250.42551,
                    "end_time": "2025-12-25T12:09:04.261371",
                    "exception": false,
                    "start_time": "2025-12-25T12:04:53.835861",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# POST-TRAINING VALIDATION WITH ENHANCED METRICS\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"POST-TRAINING VALIDATION - COMPREHENSIVE EVALUATION\")\n",
                "print(\"=\"*60)\n",
                "print(\"Computing metrics: BLEU, chrF++, and Geometric Mean\")\n",
                "print(\"(Following Deep Past Challenge evaluation methodology)\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "metric_bleu = evaluate.load(\"sacrebleu\")\n",
                "metric_chrf = evaluate.load(\"chrf\")\n",
                "\n",
                "def dedup_repeats(text: str) -> str:\n",
                "    \"\"\"Remove consecutive repeated tokens\"\"\"\n",
                "    toks = text.split()\n",
                "    out = []\n",
                "    for t in toks:\n",
                "        if len(out) >= 2 and t == out[-1] == out[-2]:\n",
                "            continue\n",
                "        out.append(t)\n",
                "    return \" \".join(out)\n",
                "\n",
                "def postprocess_text(preds):\n",
                "    \"\"\"Enhanced postprocessing for better output quality\"\"\"\n",
                "    out = []\n",
                "    for p in preds:\n",
                "        p = p.strip()\n",
                "        # Fix spacing around punctuation\n",
                "        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
                "        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
                "        # Remove repeated tokens\n",
                "        p = dedup_repeats(p)\n",
                "        # Capitalize first letter\n",
                "        if p and p[0].islower():\n",
                "            p = p[0].upper() + p[1:]\n",
                "        # Ensure sentence ends with punctuation\n",
                "        if p and p[-1] not in \".!?\":\n",
                "            p += \".\"\n",
                "        # Remove multiple punctuation\n",
                "        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
                "        out.append(p.strip())\n",
                "    return out\n",
                "\n",
                "val_texts = dataset[\"test\"][\"transliteration\"]\n",
                "val_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n",
                "\n",
                "print(f\"Validating on {len(val_texts)} samples...\")\n",
                "print(\"Using beam search with num_beams=8 for higher quality\\n\")\n",
                "\n",
                "def generate_batch(texts, num_beams=8):\n",
                "    \"\"\"Enhanced generation with optimized parameters\"\"\"\n",
                "    batch_inputs = [PREFIX + doc for doc in texts]\n",
                "    enc = tokenizer(\n",
                "        batch_inputs, \n",
                "        max_length=MAX_LENGTH, \n",
                "        truncation=True, \n",
                "        padding=True, \n",
                "        return_tensors=\"pt\"\n",
                "    ).to(model.device)\n",
                "    \n",
                "    gen = model.generate(\n",
                "        **enc,\n",
                "        max_length=MAX_LENGTH,\n",
                "        min_length=6,\n",
                "        num_beams=num_beams,              # Higher beams\n",
                "        no_repeat_ngram_size=3,           # Prevent repetition\n",
                "        length_penalty=1.2,               # Slightly favor longer outputs\n",
                "        early_stopping=True,\n",
                "        repetition_penalty=1.1,           # Additional repetition penalty\n",
                "        do_sample=False,                  # Deterministic for evaluation\n",
                "    )\n",
                "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
                "\n",
                "# Generate predictions\n",
                "preds = []\n",
                "batch_size = 4  # Smaller batches for stability\n",
                "for i in range(0, len(val_texts), batch_size):\n",
                "    batch_preds = generate_batch(val_texts[i:i+batch_size])\n",
                "    preds.extend(batch_preds)\n",
                "    if (i // batch_size + 1) % 10 == 0:\n",
                "        print(f\"  Progress: {i+batch_size}/{len(val_texts)} samples processed\")\n",
                "\n",
                "preds = postprocess_text(preds)\n",
                "\n",
                "# Compute all metrics\n",
                "print(\"\\nComputing metrics...\")\n",
                "bleu_result = metric_bleu.compute(predictions=preds, references=val_refs)\n",
                "bleu_score = bleu_result['score']\n",
                "\n",
                "chrf_result = metric_chrf.compute(predictions=preds, references=val_refs, word_order=2)\n",
                "chrf_score = chrf_result['score']\n",
                "\n",
                "# Geometric mean (competition metric)\n",
                "import math\n",
                "geo_mean = math.sqrt(bleu_score * chrf_score)\n",
                "\n",
                "# Display results\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"VALIDATION RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Samples evaluated:  {len(val_texts)}\")\n",
                "print(f\"\")\n",
                "print(f\"BLEU Score:         {bleu_score:7.2f}\")\n",
                "print(f\"chrF++ Score:       {chrf_score:7.2f}\")\n",
                "print(f\"\")\n",
                "print(f\"🏆 GEOMETRIC MEAN:  {geo_mean:7.2f}  ← Challenge Metric\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Show sample predictions\n",
                "print(\"\\n📊 SAMPLE PREDICTIONS (first 3):\")\n",
                "print(\"=\"*60)\n",
                "for i in range(min(3, len(val_texts))):\n",
                "    print(f\"\\nExample {i+1}:\")\n",
                "    print(f\"  Source: {val_texts[i][:80]}...\")\n",
                "    print(f\"  Target: {val_refs[i][0][:80]}...\")\n",
                "    print(f\"  Prediction: {preds[i][:80]}...\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "# Score interpretation\n",
                "if geo_mean >= 35:\n",
                "    print(\"🌟 EXCELLENT! Score is competition-winning level!\")\n",
                "elif geo_mean >= 30:\n",
                "    print(\"✨ GREAT! Score is strong, top quartile expected.\")\n",
                "elif geo_mean >= 25:\n",
                "    print(\"✓ GOOD! Score is solid, room for improvement.\")\n",
                "else:\n",
                "    print(\"⚠️  Score needs improvement. Consider:\")\n",
                "    print(\"   • More training epochs\")\n",
                "    print(\"   • Better data augmentation\")\n",
                "    print(\"   • Hyperparameter tuning\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"VALIDATION COMPLETE\")\n",
                "print(\"=\"*60 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03a27615",
            "metadata": {
                "papermill": {
                    "duration": 0.007132,
                    "end_time": "2025-12-25T12:09:04.275738",
                    "exception": false,
                    "start_time": "2025-12-25T12:09:04.268606",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "# A10. Save Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06794aa3",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2026-01-08T08:43:10.283354Z",
                    "iopub.status.idle": "2026-01-08T08:43:10.283622Z",
                    "shell.execute_reply": "2026-01-08T08:43:10.283519Z",
                    "shell.execute_reply.started": "2026-01-08T08:43:10.283502Z"
                },
                "papermill": {
                    "duration": 3.633252,
                    "end_time": "2025-12-25T12:09:07.916126",
                    "exception": false,
                    "start_time": "2025-12-25T12:09:04.282874",
                    "status": "completed"
                },
                "tags": [],
                "trusted": true
            },
            "outputs": [],
            "source": [
                "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
                "trainer.save_model(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "\n",
                "print(\"Notebook A Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "21ec693e",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2026-01-08T08:43:10.284698Z",
                    "iopub.status.idle": "2026-01-08T08:43:10.284976Z",
                    "shell.execute_reply": "2026-01-08T08:43:10.284835Z",
                    "shell.execute_reply.started": "2026-01-08T08:43:10.284821Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# A11. Optional Self-Training Augmentation (Small, OOM-Safe)\n",
                "ENABLE_SELF_TRAIN = False\n",
                "MAX_PSEUDO = int(os.getenv(\"BYT5_MAX_PSEUDO\", \"500\"))  # keep small to avoid OOM\n",
                "\n",
                "if ENABLE_SELF_TRAIN:\n",
                "    print(\"\\n=== SELF-TRAINING AUGMENTATION (ByT5) ===\")\n",
                "    pub_path = f\"{DATA_DIR}/published_texts.csv\"\n",
                "    if os.path.exists(pub_path):\n",
                "        pub_df = pd.read_csv(pub_path)\n",
                "        translits = pub_df.get(\"transliteration\", pd.Series([])).dropna().astype(str).tolist()\n",
                "        translits = [clean_translit(t) for t in translits]\n",
                "        translits = [t for t in translits if 5 <= len(t.split()) <= 180]\n",
                "        translits = translits[:MAX_PSEUDO]\n",
                "        print(f\"Generating pseudo translations for {len(translits)} extra transliterations...\")\n",
                "\n",
                "        def generate_batch(texts):\n",
                "            batch_inputs = [PREFIX + doc for doc in texts]\n",
                "            enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
                "            gen = model.generate(\n",
                "                **enc,\n",
                "                max_length=min(MAX_LENGTH, 400),\n",
                "                min_length=6,\n",
                "                num_beams=6,\n",
                "                no_repeat_ngram_size=3,\n",
                "                length_penalty=1.05,\n",
                "                early_stopping=True,\n",
                "            )\n",
                "            return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
                "\n",
                "        pseudo_trans = []\n",
                "        for i in range(0, len(translits), 8):  # small batch to avoid OOM\n",
                "            try:\n",
                "                batch_preds = generate_batch(translits[i:i+8])\n",
                "                pseudo_trans.extend(batch_preds)\n",
                "            except RuntimeError as e:\n",
                "                if \"out of memory\" in str(e).lower():\n",
                "                    print(\"[WARNING] OOM during pseudo generation; skipping remaining.\")\n",
                "                    break\n",
                "                else:\n",
                "                    raise\n",
                "\n",
                "        # Postprocess & filter\n",
                "        def dedup_repeats(text: str) -> str:\n",
                "            toks = text.split()\n",
                "            out = []\n",
                "            for t in toks:\n",
                "                if len(out) >= 2 and t == out[-1] == out[-2]:\n",
                "                    continue\n",
                "                out.append(t)\n",
                "            return \" \".join(out)\n",
                "        def postprocess_text(preds):\n",
                "            out = []\n",
                "            for p in preds:\n",
                "                p = p.strip()\n",
                "                p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
                "                p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
                "                p = dedup_repeats(p)\n",
                "                if p and p[0].islower():\n",
                "                    p = p[0].upper() + p[1:]\n",
                "                if p and p[-1] not in \".!?\":\n",
                "                    p += \".\"\n",
                "                p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
                "                out.append(p.strip())\n",
                "            return out\n",
                "\n",
                "        pseudo_trans = postprocess_text(pseudo_trans)\n",
                "        aug_df = pd.DataFrame({\"transliteration\": translits[:len(pseudo_trans)], \"translation\": pseudo_trans})\n",
                "        aug_df[\"src_len\"] = aug_df[\"transliteration\"].str.split().str.len()\n",
                "        aug_df[\"tgt_len\"] = aug_df[\"translation\"].str.split().str.len()\n",
                "        ratio = (aug_df[\"tgt_len\"] / aug_df[\"src_len\"]).clip(upper=6)\n",
                "        aug_df = aug_df[(aug_df[\"tgt_len\"] >= 4) & (ratio >= 0.5) & (ratio <= 6)]\n",
                "        aug_df = aug_df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
                "        print(f\"Pseudo pairs retained after filtering: {len(aug_df)}\")\n",
                "\n",
                "        base_train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
                "        base_train = base_train.dropna(subset=[\"transliteration\", \"translation\"]).astype(str)\n",
                "        base_train[\"transliteration\"] = base_train[\"transliteration\"].map(clean_translit)\n",
                "        base_train[\"translation\"] = base_train[\"translation\"].map(clean_translation)\n",
                "        combined = pd.concat([\n",
                "            base_train[[\"transliteration\", \"translation\"]],\n",
                "            aug_df[[\"transliteration\", \"translation\"]]\n",
                "        ], axis=0).drop_duplicates().reset_index(drop=True)\n",
                "        print(f\"Total combined training pairs: {len(combined)}\")\n",
                "\n",
                "        ds_combined = Dataset.from_pandas(combined)\n",
                "        def preprocess_function_aug(examples):\n",
                "            inputs = [PREFIX + ex for ex in examples[\"transliteration\"]]\n",
                "            targets = examples[\"translation\"]\n",
                "            model_inputs = tokenizer(\n",
                "                inputs,\n",
                "                max_length=MAX_LENGTH,\n",
                "                truncation=True,\n",
                "                padding=\"max_length\"\n",
                "            )\n",
                "            with tokenizer.as_target_tokenizer():\n",
                "                labels = tokenizer(\n",
                "                    targets,\n",
                "                    max_length=MAX_LENGTH,\n",
                "                    truncation=True,\n",
                "                    padding=\"max_length\"\n",
                "                )\n",
                "            model_inputs[\"labels\"] = [\n",
                "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
                "                for label in labels[\"input_ids\"]\n",
                "            ]\n",
                "            return model_inputs\n",
                "        tokenized_combined = ds_combined.map(preprocess_function_aug, batched=True)\n",
                "\n",
                "        training_args_aug = Seq2SeqTrainingArguments(\n",
                "            output_dir=OUTPUT_DIR,\n",
                "            save_strategy=\"no\",\n",
                "            eval_strategy=\"no\",\n",
                "            load_best_model_at_end=False,\n",
                "            learning_rate=2.5e-4,\n",
                "            per_device_train_batch_size=1,\n",
                "            gradient_accumulation_steps=16,\n",
                "            num_train_epochs=1,  # keep short to avoid OOM/time\n",
                "            fp16=True,\n",
                "            report_to=\"none\"\n",
                "        )\n",
                "        trainer_aug = Seq2SeqTrainer(\n",
                "            model=model,\n",
                "            args=training_args_aug,\n",
                "            train_dataset=tokenized_combined,\n",
                "            tokenizer=tokenizer,\n",
                "            data_collator=data_collator,\n",
                "        )\n",
                "        print(\"Starting second-stage training (ByT5) with augmented data...\")\n",
                "        try:\n",
                "            trainer_aug.train()\n",
                "        except RuntimeError as e:\n",
                "            print(f\"[WARNING] Augmentation training skipped due to error: {e}\")\n",
                "        print(\"Augmentation stage complete.\")\n",
                "\n",
                "        print(f\"Saving augmented model to {OUTPUT_DIR}...\")\n",
                "        trainer_aug.save_model(OUTPUT_DIR)\n",
                "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "    else:\n",
                "        print(\"published_texts.csv not found; skipping self-training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af98d73b",
            "metadata": {},
            "source": [
                "## 🎯 NEXT STEPS: Advanced Strategies for Higher Scores\n",
                "\n",
                "The optimized training configuration above should achieve **strong baseline scores** (geometric mean ~28-35). To push toward **competition-winning performance (35+)**, consider these advanced strategies:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "23b8cda9",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "ADVANCED TRAINING STRATEGIES FOR SCORE IMPROVEMENT\n",
                "====================================================\n",
                "\n",
                "If current scores are below target (geometric mean < 30), try these techniques:\n",
                "\n",
                "1. DATA AUGMENTATION\n",
                "   ─────────────────\n",
                "   • Self-training: Use model predictions on unlabeled publications.csv\n",
                "   • Back-translation: Translate English → Akkadian → English\n",
                "   • Paraphrase generation: Create variations of training pairs\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   # Generate pseudo-labels from publications.csv\n",
                "   unlabeled_texts = pd.read_csv('publications.csv')['transliteration']\n",
                "   pseudo_labels = [model.generate(...) for text in unlabeled_texts]\n",
                "   augmented_data = Dataset.from_dict({\n",
                "       'transliteration': unlabeled_texts,\n",
                "       'translation': pseudo_labels\n",
                "   })\n",
                "   combined_dataset = concatenate_datasets([dataset['train'], augmented_data])\n",
                "   ```\n",
                "\n",
                "2. CURRICULUM LEARNING\n",
                "   ───────────────────\n",
                "   • Train on easy examples first, gradually increase difficulty\n",
                "   • Sort by sentence length, gaps count, or complexity\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   # Sort training data by length (simple → complex)\n",
                "   train_df = pd.DataFrame(dataset['train'])\n",
                "   train_df['src_len'] = train_df['transliteration'].str.split().str.len()\n",
                "   train_df = train_df.sort_values('src_len')\n",
                "   \n",
                "   # Train in stages\n",
                "   for stage, max_len in enumerate([30, 60, 100, 200]):\n",
                "       stage_data = train_df[train_df['src_len'] <= max_len]\n",
                "       # Train for 5 epochs on this stage\n",
                "   ```\n",
                "\n",
                "3. ENSEMBLE WITHIN BYT5\n",
                "   ─────────────────────\n",
                "   • Train multiple ByT5 models with different seeds\n",
                "   • Average their predictions for better stability\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   models = []\n",
                "   for seed in [42, 123, 456]:\n",
                "       set_seed(seed)\n",
                "       model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
                "       trainer = Seq2SeqTrainer(...)\n",
                "       trainer.train()\n",
                "       models.append(model)\n",
                "   \n",
                "   # Ensemble predictions\n",
                "   all_preds = [model.generate(...) for model in models]\n",
                "   final_pred = voting_mechanism(all_preds)  # Majority vote or averaging\n",
                "   ```\n",
                "\n",
                "4. ADVANCED HYPERPARAMETER TUNING\n",
                "   ───────────────────────────────\n",
                "   • Learning rate scheduling: Try polynomial decay or OneCycleLR\n",
                "   • Epoch extension: 25-30 epochs with early stopping patience=5\n",
                "   • Regularization: Increase dropout (0.1 → 0.15), weight decay (0.01 → 0.05)\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   training_args.num_train_epochs = 30\n",
                "   training_args.lr_scheduler_type = \"polynomial\"  # or \"cosine_with_restarts\"\n",
                "   training_args.learning_rate = 3e-5  # Try 3e-5, 4e-5, 6e-5\n",
                "   training_args.warmup_ratio = 0.1\n",
                "   ```\n",
                "\n",
                "5. POST-PROCESSING ENHANCEMENT\n",
                "   ───────────────────────────\n",
                "   • Language model scoring: Re-rank beam outputs with GPT-2\n",
                "   • Rule-based fixes: Correct common errors (articles, plurality)\n",
                "   • Length normalization: Penalize too-short/too-long outputs\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "   \n",
                "   lm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
                "   lm_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
                "   \n",
                "   def rerank_with_lm(candidates):\n",
                "       scores = []\n",
                "       for cand in candidates:\n",
                "           inputs = lm_tok(cand, return_tensors='pt')\n",
                "           with torch.no_grad():\n",
                "               score = -lm(**inputs).loss.item()  # Perplexity\n",
                "           scores.append(score)\n",
                "       return candidates[np.argmax(scores)]\n",
                "   ```\n",
                "\n",
                "6. DATA MINING OPTIMIZATION\n",
                "   ─────────────────────────\n",
                "   • Use publications.csv more effectively\n",
                "   • Extract patterns from high-quality translation pairs\n",
                "   • Filter low-quality augmented data\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   # Score data quality\n",
                "   def quality_score(src, tgt):\n",
                "       length_ratio = len(tgt.split()) / max(len(src.split()), 1)\n",
                "       has_gaps = '<gap>' in src.lower()\n",
                "       return length_ratio * (0.8 if has_gaps else 1.0)\n",
                "   \n",
                "   # Keep only high-quality augmented pairs\n",
                "   augmented_data = augmented_data.filter(\n",
                "       lambda x: quality_score(x['transliteration'], x['translation']) > 0.5\n",
                "   )\n",
                "   ```\n",
                "\n",
                "7. ARCHITECTURE MODIFICATIONS\n",
                "   ──────────────────────────\n",
                "   • Freeze encoder for first 5 epochs (faster convergence)\n",
                "   • Gradually unfreeze layers (discriminative fine-tuning)\n",
                "   \n",
                "   Implementation:\n",
                "   ```\n",
                "   # Freeze encoder initially\n",
                "   for param in model.encoder.parameters():\n",
                "       param.requires_grad = False\n",
                "   \n",
                "   # Train decoder only for 5 epochs\n",
                "   trainer.train(max_steps=...)\n",
                "   \n",
                "   # Unfreeze and continue\n",
                "   for param in model.encoder.parameters():\n",
                "       param.requires_grad = True\n",
                "   trainer.train()  # Continue training\n",
                "   ```\n",
                "\n",
                "SCORING TARGETS\n",
                "───────────────\n",
                "Current optimized config: ~28-32 geometric mean (expected baseline)\n",
                "With 1-2 techniques above: ~32-36 (competitive)\n",
                "With 3+ techniques above: 36+ (top quartile)\n",
                "\n",
                "RECOMMENDED PRIORITY ORDER\n",
                "─────────────────────────\n",
                "1. Try self-training augmentation first (biggest impact)\n",
                "2. Extend to 25-30 epochs with better LR schedule\n",
                "3. Ensemble with multiple seeds (stability boost)\n",
                "4. Post-processing with LM re-ranking (final polish)\n",
                "\n",
                "Remember: Geometric mean = √(BLEU × chrF++)\n",
                "- BLEU rewards exact matches (focus on common phrases)\n",
                "- chrF++ rewards character overlap (focus on morphology)\n",
                "- Balance both for optimal score\n",
                "\"\"\"\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"📚 ADVANCED STRATEGIES REFERENCE LOADED\")\n",
                "print(\"=\"*60)\n",
                "print(\"Implement these techniques to push scores from ~30 to 35+\")\n",
                "print(\"Priority: Data Augmentation → Extended Training → Ensemble\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [
                {
                    "databundleVersionId": 14976537,
                    "sourceId": 121150,
                    "sourceType": "competition"
                },
                {
                    "datasetId": 9082937,
                    "sourceId": 14236819,
                    "sourceType": "datasetVersion"
                }
            ],
            "dockerImageVersionId": 31234,
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        },
        "papermill": {
            "default_parameters": {},
            "duration": 6059.435801,
            "end_time": "2025-12-25T12:09:11.716859",
            "environment_variables": {},
            "exception": null,
            "input_path": "__notebook__.ipynb",
            "output_path": "__notebook__.ipynb",
            "parameters": {},
            "start_time": "2025-12-25T10:28:12.281058",
            "version": "2.6.0"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {
                    "1fe03852120a40cda6d5c85f4e29fcaf": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_b48bf215217541358d86be12f43b7f3c",
                            "placeholder": "​",
                            "style": "IPY_MODEL_fcaea7f32ee24eebaebfc0e32266d44a",
                            "tabbable": null,
                            "tooltip": null,
                            "value": " 8.15k/? [00:00&lt;00:00, 928kB/s]"
                        }
                    },
                    "226d44a1d8884129a1d1230b2642a786": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "FloatProgressModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "FloatProgressModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "ProgressView",
                            "bar_style": "success",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_b0c8da6c308b446cb01170a376a943c3",
                            "max": 77,
                            "min": 0,
                            "orientation": "horizontal",
                            "style": "IPY_MODEL_39da431ed6e041ef8c50061ff0cb2734",
                            "tabbable": null,
                            "tooltip": null,
                            "value": 77
                        }
                    },
                    "2bccb998d49d415580aaed49b33a1b22": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_791efc0e593d4599ba993239183a55e3",
                            "placeholder": "​",
                            "style": "IPY_MODEL_683b28591d2f4363aa74a0a1a44c7a1c",
                            "tabbable": null,
                            "tooltip": null,
                            "value": " 77/77 [00:00&lt;00:00, 388.82 examples/s]"
                        }
                    },
                    "2d21f5383a1548d98e116b290031fccd": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "39da431ed6e041ef8c50061ff0cb2734": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "ProgressStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "ProgressStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "bar_color": null,
                            "description_width": ""
                        }
                    },
                    "3b23edd7473a4b5cbeb0e7453bf9b6bc": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "427b3335b5374628a175042c3a3382e4": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_2d21f5383a1548d98e116b290031fccd",
                            "placeholder": "​",
                            "style": "IPY_MODEL_9aed8f291fff48a5a708caf8c20013d8",
                            "tabbable": null,
                            "tooltip": null,
                            "value": "Downloading builder script: "
                        }
                    },
                    "44b7b6316c1c4a4bbc5e3b84e174e79f": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "FloatProgressModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "FloatProgressModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "ProgressView",
                            "bar_style": "success",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_e6a4941ccd41469aa09a0ec5a804af6e",
                            "max": 1,
                            "min": 0,
                            "orientation": "horizontal",
                            "style": "IPY_MODEL_b76718f5d5b1422ab37b370c9cc527ff",
                            "tabbable": null,
                            "tooltip": null,
                            "value": 1
                        }
                    },
                    "45c5ff13e0804ab38df84c594f5cbfe4": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_c11ec6efe7cb4722883a279f3eb185ed",
                            "placeholder": "​",
                            "style": "IPY_MODEL_8846ab600ef243c39c4f1cb71a308f59",
                            "tabbable": null,
                            "tooltip": null,
                            "value": "Downloading builder script: "
                        }
                    },
                    "4a7eec71c5664bbd922f48d02fb439ac": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "4b48b9cb404748bdba5a1554f4687518": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HBoxModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HBoxModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HBoxView",
                            "box_style": "",
                            "children": [
                                "IPY_MODEL_e865f98ee6084edf9b9c63807b52da6b",
                                "IPY_MODEL_9f53690cf1d84b5aa3e3929a66799044",
                                "IPY_MODEL_f030f089b1124566ab437b62c99b2578"
                            ],
                            "layout": "IPY_MODEL_c2b4df78e626401c8a097c7890f1386e",
                            "tabbable": null,
                            "tooltip": null
                        }
                    },
                    "6297171298534da8a8ae461611777771": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "67bff04e3ed849f8aadf4b6b7df2b9e6": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HBoxModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HBoxModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HBoxView",
                            "box_style": "",
                            "children": [
                                "IPY_MODEL_427b3335b5374628a175042c3a3382e4",
                                "IPY_MODEL_44b7b6316c1c4a4bbc5e3b84e174e79f",
                                "IPY_MODEL_1fe03852120a40cda6d5c85f4e29fcaf"
                            ],
                            "layout": "IPY_MODEL_b8d2e1271f1e4afdb533651d811a2bb8",
                            "tabbable": null,
                            "tooltip": null
                        }
                    },
                    "683b28591d2f4363aa74a0a1a44c7a1c": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "6c90b63b47dc4de495594cfd1e6519b0": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "701dd03462844cf0af47a3ab8c452a04": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HBoxModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HBoxModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HBoxView",
                            "box_style": "",
                            "children": [
                                "IPY_MODEL_45c5ff13e0804ab38df84c594f5cbfe4",
                                "IPY_MODEL_bc4d79d1c6384c23a736957e90afd304",
                                "IPY_MODEL_9d58cd0d03464cd18c653fdec13d87e4"
                            ],
                            "layout": "IPY_MODEL_b0bcd130aa8341b1acef83b7cc545dce",
                            "tabbable": null,
                            "tooltip": null
                        }
                    },
                    "71affa5ac8894c80959fbb0448728c81": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_c324f021e0034d85bcf6459f17a7a752",
                            "placeholder": "​",
                            "style": "IPY_MODEL_6297171298534da8a8ae461611777771",
                            "tabbable": null,
                            "tooltip": null,
                            "value": "Map: 100%"
                        }
                    },
                    "791efc0e593d4599ba993239183a55e3": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "8846ab600ef243c39c4f1cb71a308f59": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "8f98493e151546f2baa629a4edf06a14": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "9369db00183844a5b53806dd64a214db": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "969e0b0841ff4999a85c76101abdcc41": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": "20px"
                        }
                    },
                    "9aed8f291fff48a5a708caf8c20013d8": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    },
                    "9d58cd0d03464cd18c653fdec13d87e4": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_b789e817126e4af1a184cfae19f2612f",
                            "placeholder": "​",
                            "style": "IPY_MODEL_3b23edd7473a4b5cbeb0e7453bf9b6bc",
                            "tabbable": null,
                            "tooltip": null,
                            "value": " 9.01k/? [00:00&lt;00:00, 914kB/s]"
                        }
                    },
                    "9f53690cf1d84b5aa3e3929a66799044": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "FloatProgressModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "FloatProgressModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "ProgressView",
                            "bar_style": "success",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_ab64a8242ff94bdfbfceeabac64877cf",
                            "max": 1452,
                            "min": 0,
                            "orientation": "horizontal",
                            "style": "IPY_MODEL_d0f4b1fc30b24fd1a84d58113003352a",
                            "tabbable": null,
                            "tooltip": null,
                            "value": 1452
                        }
                    },
                    "a1151781b3e84a738ae66c7ac283ba3b": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HBoxModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HBoxModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HBoxView",
                            "box_style": "",
                            "children": [
                                "IPY_MODEL_71affa5ac8894c80959fbb0448728c81",
                                "IPY_MODEL_226d44a1d8884129a1d1230b2642a786",
                                "IPY_MODEL_2bccb998d49d415580aaed49b33a1b22"
                            ],
                            "layout": "IPY_MODEL_4a7eec71c5664bbd922f48d02fb439ac",
                            "tabbable": null,
                            "tooltip": null
                        }
                    },
                    "a8e873b972004c38ae11c1d76cfe84ba": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "ProgressStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "ProgressStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "bar_color": null,
                            "description_width": ""
                        }
                    },
                    "ab64a8242ff94bdfbfceeabac64877cf": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "ae69eb96031c4d43bf4c52da5aedb198": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "b0bcd130aa8341b1acef83b7cc545dce": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "b0c8da6c308b446cb01170a376a943c3": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "b48bf215217541358d86be12f43b7f3c": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "b76718f5d5b1422ab37b370c9cc527ff": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "ProgressStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "ProgressStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "bar_color": null,
                            "description_width": ""
                        }
                    },
                    "b789e817126e4af1a184cfae19f2612f": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "b8d2e1271f1e4afdb533651d811a2bb8": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "bc4d79d1c6384c23a736957e90afd304": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "FloatProgressModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "FloatProgressModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "ProgressView",
                            "bar_style": "success",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_969e0b0841ff4999a85c76101abdcc41",
                            "max": 1,
                            "min": 0,
                            "orientation": "horizontal",
                            "style": "IPY_MODEL_a8e873b972004c38ae11c1d76cfe84ba",
                            "tabbable": null,
                            "tooltip": null,
                            "value": 1
                        }
                    },
                    "c11ec6efe7cb4722883a279f3eb185ed": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "c2b4df78e626401c8a097c7890f1386e": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "c324f021e0034d85bcf6459f17a7a752": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": null
                        }
                    },
                    "d0f4b1fc30b24fd1a84d58113003352a": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "ProgressStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "ProgressStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "bar_color": null,
                            "description_width": ""
                        }
                    },
                    "e6a4941ccd41469aa09a0ec5a804af6e": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "2.0.0",
                        "model_name": "LayoutModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/base",
                            "_model_module_version": "2.0.0",
                            "_model_name": "LayoutModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "LayoutView",
                            "align_content": null,
                            "align_items": null,
                            "align_self": null,
                            "border_bottom": null,
                            "border_left": null,
                            "border_right": null,
                            "border_top": null,
                            "bottom": null,
                            "display": null,
                            "flex": null,
                            "flex_flow": null,
                            "grid_area": null,
                            "grid_auto_columns": null,
                            "grid_auto_flow": null,
                            "grid_auto_rows": null,
                            "grid_column": null,
                            "grid_gap": null,
                            "grid_row": null,
                            "grid_template_areas": null,
                            "grid_template_columns": null,
                            "grid_template_rows": null,
                            "height": null,
                            "justify_content": null,
                            "justify_items": null,
                            "left": null,
                            "margin": null,
                            "max_height": null,
                            "max_width": null,
                            "min_height": null,
                            "min_width": null,
                            "object_fit": null,
                            "object_position": null,
                            "order": null,
                            "overflow": null,
                            "padding": null,
                            "right": null,
                            "top": null,
                            "visibility": null,
                            "width": "20px"
                        }
                    },
                    "e865f98ee6084edf9b9c63807b52da6b": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_ae69eb96031c4d43bf4c52da5aedb198",
                            "placeholder": "​",
                            "style": "IPY_MODEL_8f98493e151546f2baa629a4edf06a14",
                            "tabbable": null,
                            "tooltip": null,
                            "value": "Map: 100%"
                        }
                    },
                    "f030f089b1124566ab437b62c99b2578": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLModel",
                        "state": {
                            "_dom_classes": [],
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/controls",
                            "_view_module_version": "2.0.0",
                            "_view_name": "HTMLView",
                            "description": "",
                            "description_allow_html": false,
                            "layout": "IPY_MODEL_9369db00183844a5b53806dd64a214db",
                            "placeholder": "​",
                            "style": "IPY_MODEL_6c90b63b47dc4de495594cfd1e6519b0",
                            "tabbable": null,
                            "tooltip": null,
                            "value": " 1452/1452 [00:03&lt;00:00, 448.42 examples/s]"
                        }
                    },
                    "fcaea7f32ee24eebaebfc0e32266d44a": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "2.0.0",
                        "model_name": "HTMLStyleModel",
                        "state": {
                            "_model_module": "@jupyter-widgets/controls",
                            "_model_module_version": "2.0.0",
                            "_model_name": "HTMLStyleModel",
                            "_view_count": null,
                            "_view_module": "@jupyter-widgets/base",
                            "_view_module_version": "2.0.0",
                            "_view_name": "StyleView",
                            "background": null,
                            "description_width": "",
                            "font_size": null,
                            "text_color": null
                        }
                    }
                },
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
