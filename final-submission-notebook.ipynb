{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ==========================================\n",
    "# ðŸ¥£ MODEL SOUP CONFIGURATION\n",
    "# ==========================================\n",
    "# Paths to the OUTPUT DATASETS from your 3 notebooks\n",
    "# Ensure you add these datasets to this notebook via the \"Add Data\" button\n",
    "MODEL_DIRS = [\n",
    "    \"../input/notebook-a-output/byt5-base-saved\",        # From Notebook A\n",
    "    \"../input/notebook-b-output/byt5-greedy-saved\",      # From Notebook B\n",
    "    \"../input/notebook-c-output/byt5-specialist-saved\"   # From Notebook C\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "PREFIX = \"translate Akkadian to English: \"\n",
    "\n",
    "# ==========================================\n",
    "# 1. COOK THE SOUP (WEIGHT AVERAGING)\n",
    "# ==========================================\n",
    "print(f\"ðŸ¥£ Starting Model Soup with {len(MODEL_DIRS)} ingredients...\")\n",
    "\n",
    "# Load the first model (The Purist) as the base container\n",
    "print(f\"Loading Base: {MODEL_DIRS[0]}\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIRS[0])\n",
    "soup_state_dict = base_model.state_dict()\n",
    "\n",
    "# Add the other models (Greedy & Specialist)\n",
    "for model_path in MODEL_DIRS[1:]:\n",
    "    print(f\"Merging Ingredients from: {model_path}\")\n",
    "    participant_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    participant_state = participant_model.state_dict()\n",
    "    \n",
    "    for key in soup_state_dict:\n",
    "        # Sum the weights: W_total = W_a + W_b + W_c\n",
    "        soup_state_dict[key] += participant_state[key]\n",
    "\n",
    "# Divide by 3 to get the mathematical average\n",
    "print(\"Mixing (Averaging weights)...\")\n",
    "for key in soup_state_dict:\n",
    "    soup_state_dict[key] = soup_state_dict[key] / len(MODEL_DIRS)\n",
    "\n",
    "# Load the averaged weights back into the base model\n",
    "base_model.load_state_dict(soup_state_dict)\n",
    "print(\"âœ… Model Soup Ready! This model contains the knowledge of all three.\")\n",
    "\n",
    "# Move to GPU\n",
    "if torch.cuda.is_available():\n",
    "    base_model = base_model.cuda()\n",
    "base_model.eval()\n",
    "\n",
    "# Load Tokenizer (All ByT5 tokenizers are identical)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRS[0])\n",
    "\n",
    "# ==========================================\n",
    "# 2. GENERATE PREDICTIONS\n",
    "# ==========================================\n",
    "def predict(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    inputs = tokenizer(\n",
    "        PREFIX + text, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_length=MAX_LENGTH,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Load Test Data\n",
    "test_path = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "test_df = pd.read_csv(test_path)\n",
    "print(f\"Generating predictions for {len(test_df)} samples...\")\n",
    "\n",
    "# Run Inference\n",
    "submission_ids = []\n",
    "predictions = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    pred = predict(row['transliteration'])\n",
    "    submission_ids.append(row['id'])\n",
    "    predictions.append(pred)\n",
    "    if idx % 50 == 0: \n",
    "        print(f\"Processed {idx} samples...\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. SAVE SUBMISSION\n",
    "# ==========================================\n",
    "submission = pd.DataFrame({'id': submission_ids, 'translation': predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"ðŸŽ‰ Submission saved successfully! Ready to submit.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9084006,
     "sourceId": 14290093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084005,
     "sourceId": 14290098,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084224,
     "sourceId": 14290100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
