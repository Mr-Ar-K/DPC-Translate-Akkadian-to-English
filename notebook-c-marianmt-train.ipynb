{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:20:46.385959Z",
     "iopub.status.busy": "2025-12-20T17:20:46.385450Z",
     "iopub.status.idle": "2025-12-20T17:21:16.006482Z",
     "shell.execute_reply": "2025-12-20T17:21:16.005820Z",
     "shell.execute_reply.started": "2025-12-20T17:20:46.385925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-20 17:21:00.834252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766251261.017805      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766251261.070683      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766251261.512574      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766251261.512610      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766251261.512613      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766251261.512615      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path provided by you for Notebook C\n",
    "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/opus-mt-mul-en\"\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "OUTPUT_DIR = \"./marian-mt-saved\"\n",
    "\n",
    "# MarianMT is optimized for sentence-level translation.\n",
    "# 128 tokens is usually plenty for a single Akkadian sentence.\n",
    "MAX_LENGTH = 160 \n",
    "\n",
    "# MarianMT does not strictly require a prefix, but adding one can help alignment.\n",
    "# We will use an empty prefix here as the model is already aligned for mul->en.\n",
    "PREFIX = \"\"\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2.Data Loading & Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:21:16.008486Z",
     "iopub.status.busy": "2025-12-20T17:21:16.007900Z",
     "iopub.status.idle": "2025-12-20T17:21:16.179877Z",
     "shell.execute_reply": "2025-12-20T17:21:16.179101Z",
     "shell.execute_reply.started": "2025-12-20T17:21:16.008459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw documents: 1561\n",
      "Aligned training examples: 1561\n"
     ]
    }
   ],
   "source": [
    "def clean_translit(text):\n",
    "    \"\"\"Normalize transliteration by stripping scribal marks and gaps.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" <big_gap> \", text)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"[:]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_align_data(filepath):\n",
    "    \"\"\"\n",
    "    Aligns Akkadian transliterations to English translations.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    aligned_rows = []\n",
    "\n",
    "    print(f\"Raw documents: {len(df)}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        src = clean_translit(row.get(\"transliteration\", \"\"))\n",
    "        tgt = clean_translation(row.get(\"translation\", \"\"))\n",
    "\n",
    "        # Split source by newlines\n",
    "        src_lines = [s.strip() for s in src.split(\"\\n\") if len(s.strip()) > 1]\n",
    "        \n",
    "        # Split target by sentence punctuation\n",
    "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if len(t.strip()) > 1]\n",
    "\n",
    "        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n",
    "            for s, t in zip(src_lines, tgt_sents):\n",
    "                aligned_rows.append({\"src\": s, \"tgt\": t})\n",
    "        else:\n",
    "            aligned_rows.append({\"src\": src.replace(\"\\n\", \" \"), \"tgt\": tgt})\n",
    "\n",
    "    print(f\"Aligned training examples: {len(aligned_rows)}\")\n",
    "    return pd.DataFrame(aligned_rows)\n",
    "\n",
    "# Load and Split Data\n",
    "df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:21:16.181066Z",
     "iopub.status.busy": "2025-12-20T17:21:16.180818Z",
     "iopub.status.idle": "2025-12-20T17:21:20.448690Z",
     "shell.execute_reply": "2025-12-20T17:21:20.447949Z",
     "shell.execute_reply.started": "2025-12-20T17:21:16.181045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc7a90b9dd9492dbf0634a976c85e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa71162c0de046d08e360d4a2f7f6fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"src\"]]\n",
    "    targets = examples[\"tgt\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=MAX_LENGTH, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # Replace padding token id with -100\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply processing\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:21:20.450079Z",
     "iopub.status.busy": "2025-12-20T17:21:20.449697Z",
     "iopub.status.idle": "2025-12-20T17:21:21.904791Z",
     "shell.execute_reply": "2025-12-20T17:21:21.903974Z",
     "shell.execute_reply.started": "2025-12-20T17:21:20.450051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- CORRECTED C5. Training Configuration (Disk Space Safe) ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # --- DISK SPACE FIXES ---\n",
    "    save_strategy=\"no\",           # Do NOT save checkpoints during training\n",
    "    eval_strategy=\"epoch\",        # Evaluate every epoch\n",
    "    load_best_model_at_end=False, # Must be False if we aren't saving checkpoints\n",
    "    # ------------------------\n",
    "    \n",
    "    learning_rate=2e-5, \n",
    "    \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "\n",
    "    logging_steps=10, #added by me\n",
    "    \n",
    "    num_train_epochs=13, #increased to 13 from 7\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    fp16=True, # MarianMT is safe with fp16\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Accuracy-focused tweaks\n",
    "    label_smoothing_factor=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.04,\n",
    "    generation_max_length=180,\n",
    "    generation_num_beams=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C6. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-20T17:22:58.965Z",
     "iopub.execute_input": "2025-12-20T17:21:22.071350Z",
     "iopub.status.busy": "2025-12-20T17:21:22.071031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/148008205.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MarianMT Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='333' max='651' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [333/651 01:32 < 01:29, 3.57 it/s, Epoch 3.57/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.983993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.588560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.435226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting MarianMT Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-20T17:22:58.965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Notebook C (MarianMT) Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9082937,
     "sourceId": 14236819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
