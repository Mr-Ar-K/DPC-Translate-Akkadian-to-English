{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-25T12:14:36.175832Z",
     "iopub.status.busy": "2025-12-25T12:14:36.175605Z",
     "iopub.status.idle": "2025-12-25T12:15:40.422643Z",
     "shell.execute_reply": "2025-12-25T12:15:40.422026Z",
     "shell.execute_reply.started": "2025-12-25T12:14:36.175811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Deep Past Neural Ensemble Inference ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 12:14:51.095337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766664891.298313      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766664891.357816      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766664891.851037      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851082      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851085      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851088      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286a0586afeb435ea31d31b4777fc8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference byt5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453f6b3dd3304ef99924faf6e3a6a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference t5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9f6a77e75d47d4adc7de4402091a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference marian:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview:\n",
      "   id                                        translation\n",
      "0   0  K√†-ar-ma √∫ big_gap da-tim a√≠-ip-ri-ni Akkadian...\n",
      "1   1  -ni i-na n√©-m√¨-lim da-a√πr √∫-l√° e-WA ia-ra-t√≠-a...\n",
      "2   2  -it a-a√≠-im au-um-au ia-t√≠ a√©-bi‚Äû-l√°-nim Trans...\n",
      "3   3  √â-bi‚Äû-l√° K√ô. AN Translate Akkadian to English:...\n"
     ]
    }
   ],
   "source": [
    "# DEEP PAST CHALLENGE - ENSEMBLE SUBMISSION\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL1_PATH = os.getenv(\"MODEL1_PATH\", \"/kaggle/input/notebook-a-byt5/byt5-base-saved\")\n",
    "MODEL2_PATH = os.getenv(\"MODEL2_PATH\", \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\")\n",
    "MODEL3_PATH = os.getenv(\"MODEL3_PATH\", \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\")\n",
    "\n",
    "TEST_DATA_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENSEMBLE STRATEGY: AUTO-DETECT\n",
    "# -----------------------------------------------------------------------------\n",
    "# This notebook automatically chooses the best strategy:\n",
    "# 1. Weight averaging if all models have same architecture\n",
    "# 2. Voting ensemble if models have different architectures\n",
    "# 3. Best single model as fallback\n",
    "\n",
    "ENSEMBLE_MODE = os.getenv(\"ENSEMBLE_MODE\", \"auto\")  # \"auto\", \"voting\", or \"averaging\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODEL CONFIGURATIONS & WEIGHTS\n",
    "# -----------------------------------------------------------------------------\n",
    "# Adjust these weights based on validation scores from find-optimal-weights.ipynb\n",
    "# Or use equal weights as baseline: {\"weight\": 0.333}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"byt5\": {\n",
    "        \"path\": MODEL1_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.35,  # Adjust based on validation\n",
    "        \"num_beams\": 4\n",
    "    },\n",
    "    \"t5\": {\n",
    "        \"path\": MODEL2_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.40,  # Usually best performer\n",
    "        \"num_beams\": 4\n",
    "    },\n",
    "    \"marian\": {\n",
    "        \"path\": MODEL3_PATH,\n",
    "        \"prefix\": \">>eng<< \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.25,  # Adjust based on validation\n",
    "        \"num_beams\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Ensemble Mode: {ENSEMBLE_MODE}\")\n",
    "print(f\"\\nModel Weights:\")\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    exists = \"‚úì\" if os.path.exists(config[\"path\"]) else \"‚úó\"\n",
    "    print(f\"  {name:10s} {exists} weight={config['weight']:.2f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GAP REPLACEMENT FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def replace_gaps(text):\n",
    "    \"\"\"Replace various gap notations with standardized tokens\"\"\"\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    \n",
    "    # Complex gap patterns (order matters)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    # Simple gap patterns\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "    text = re.sub(r'‚Ä¶‚Ä¶', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "    text = re.sub(r'‚Ä¶', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD TEST DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "test_df['transliteration'] = test_df['transliteration'].apply(replace_gaps)\n",
    "test_inputs = test_df['transliteration'].astype(str).tolist()\n",
    "source_lengths = [len(t.split()) for t in test_inputs]\n",
    "\n",
    "print(f\"‚úì Loaded {len(test_df)} test samples\")\n",
    "print(f\"‚úì Average source length: {sum(source_lengths)/len(source_lengths):.1f} words\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# INFERENCE DATASET\n",
    "# -----------------------------------------------------------------------------\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length, prefix=\"\"):\n",
    "        self.texts = [prefix + str(t) for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STRATEGY 1: TRY WEIGHT AVERAGING\n",
    "# -----------------------------------------------------------------------------\n",
    "def try_weight_averaging():\n",
    "    \"\"\"Try to merge models by averaging weights\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ATTEMPTING WEIGHT AVERAGING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    available_models = {k: v for k, v in MODEL_CONFIGS.items() if os.path.exists(v[\"path\"])}\n",
    "    \n",
    "    if len(available_models) < 2:\n",
    "        print(\"‚ö†Ô∏è  Need at least 2 models for weight averaging\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load all models\n",
    "        models = {}\n",
    "        state_dicts = {}\n",
    "        \n",
    "        for name, config in available_models.items():\n",
    "            print(f\"Loading {name}...\")\n",
    "            models[name] = AutoModelForSeq2SeqLM.from_pretrained(config[\"path\"])\n",
    "            state_dicts[name] = models[name].state_dict()\n",
    "        \n",
    "        # Check architecture compatibility\n",
    "        keys_list = [set(sd.keys()) for sd in state_dicts.values()]\n",
    "        if not all(keys == keys_list[0] for keys in keys_list):\n",
    "            print(\"‚ö†Ô∏è  Models have incompatible architectures\")\n",
    "            # Try pairwise compatibility\n",
    "            names = list(state_dicts.keys())\n",
    "            if len(names) >= 2 and set(state_dicts[names[0]].keys()) == set(state_dicts[names[1]].keys()):\n",
    "                print(f\"‚úì Can merge {names[0]} and {names[1]}\")\n",
    "                # Merge first two compatible models\n",
    "                w_total = available_models[names[0]][\"weight\"] + available_models[names[1]][\"weight\"]\n",
    "                w1 = available_models[names[0]][\"weight\"] / w_total\n",
    "                w2 = available_models[names[1]][\"weight\"] / w_total\n",
    "                \n",
    "                merged_sd = {}\n",
    "                for key in state_dicts[names[0]].keys():\n",
    "                    merged_sd[key] = w1 * state_dicts[names[0]][key] + w2 * state_dicts[names[1]][key]\n",
    "                \n",
    "                model = models[names[1]]\n",
    "                model.load_state_dict(merged_sd)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(available_models[names[1]][\"path\"])\n",
    "                \n",
    "                # Cleanup\n",
    "                for m in models.values():\n",
    "                    if m is not model:\n",
    "                        del m\n",
    "                del state_dicts\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "                print(f\"‚úì Created 2-model ensemble ({names[0]} + {names[1]})\")\n",
    "                return model, tokenizer\n",
    "            else:\n",
    "                return None, None\n",
    "        \n",
    "        # All compatible - merge all models\n",
    "        print(\"‚úì All models compatible - merging all\")\n",
    "        weights = {k: v[\"weight\"] for k, v in available_models.items()}\n",
    "        w_total = sum(weights.values())\n",
    "        weights = {k: v/w_total for k, v in weights.items()}\n",
    "        \n",
    "        merged_sd = {}\n",
    "        first_name = list(state_dicts.keys())[0]\n",
    "        for key in state_dicts[first_name].keys():\n",
    "            merged_sd[key] = sum(weights[name] * state_dicts[name][key] for name in state_dicts.keys())\n",
    "        \n",
    "        model = models[first_name]\n",
    "        model.load_state_dict(merged_sd)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(available_models[first_name][\"path\"])\n",
    "        \n",
    "        # Cleanup\n",
    "        for m in models.values():\n",
    "            if m is not model:\n",
    "                del m\n",
    "        del state_dicts\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"‚úì Created {len(available_models)}-model ensemble\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Weight averaging failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STRATEGY 2: VOTING ENSEMBLE\n",
    "# -----------------------------------------------------------------------------\n",
    "def voting_ensemble():\n",
    "    \"\"\"Generate predictions from all models and combine via voting\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USING VOTING ENSEMBLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_predictions = {}\n",
    "    \n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        if not os.path.exists(config[\"path\"]):\n",
    "            print(f\"‚ö†Ô∏è  Skipping {model_name}: Model not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{model_name.upper()}]\")\n",
    "        print(f\"Loading from {config['path']}...\")\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config[\"path\"])\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(config[\"path\"])\n",
    "            model = model.to(DEVICE)\n",
    "            model.eval()\n",
    "            \n",
    "            dataset = InferenceDataset(test_inputs, tokenizer, config[\"max_length\"], config[\"prefix\"])\n",
    "            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(loader, desc=f\"{model_name}\"):\n",
    "                    outputs = model.generate(\n",
    "                        input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                        attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                        max_length=config[\"max_length\"],\n",
    "                        num_beams=config[\"num_beams\"],\n",
    "                        early_stopping=True,\n",
    "                    )\n",
    "                    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                    predictions.extend([d.strip() for d in decoded])\n",
    "            \n",
    "            all_predictions[model_name] = {\n",
    "                \"preds\": predictions,\n",
    "                \"weight\": config[\"weight\"]\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úì Generated {len(predictions)} predictions\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del model, tokenizer, dataset, loader\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error with {model_name}: {e}\")\n",
    "    \n",
    "    if not all_predictions:\n",
    "        raise RuntimeError(\"No models loaded successfully!\")\n",
    "    \n",
    "    # Combine predictions via weighted voting\n",
    "    print(f\"\\nCombining predictions from {len(all_predictions)} models...\")\n",
    "    final_predictions = []\n",
    "    \n",
    "    for i in range(len(test_inputs)):\n",
    "        best_pred = \"broken text\"\n",
    "        best_score = -999999\n",
    "        \n",
    "        for model_name, data in all_predictions.items():\n",
    "            pred = data[\"preds\"][i] if i < len(data[\"preds\"]) else \"\"\n",
    "            weight = data[\"weight\"]\n",
    "            \n",
    "            # Score prediction\n",
    "            if pred and len(pred) > 0:\n",
    "                pred_len = len(pred.split())\n",
    "                # Prefer reasonable length (0.8x to 2x source)\n",
    "                ratio = pred_len / max(1, source_lengths[i])\n",
    "                length_score = 0 if 0.5 < ratio < 3 else -10 * abs(ratio - 1.5)\n",
    "                score = weight * (pred_len + length_score)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_pred = pred\n",
    "        \n",
    "        final_predictions.append(best_pred)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN ENSEMBLE LOGIC\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_predictions = None\n",
    "\n",
    "if ENSEMBLE_MODE == \"voting\":\n",
    "    # Force voting ensemble\n",
    "    final_predictions = voting_ensemble()\n",
    "    \n",
    "elif ENSEMBLE_MODE == \"averaging\":\n",
    "    # Force weight averaging\n",
    "    merged_model, tokenizer = try_weight_averaging()\n",
    "    if merged_model is not None:\n",
    "        merged_model = merged_model.to(DEVICE).eval()\n",
    "        dataset = InferenceDataset(test_inputs, tokenizer, MAX_LENGTH, \"translate Akkadian to English: \")\n",
    "        loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Inference\"):\n",
    "                outputs = merged_model.generate(\n",
    "                    input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                    attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "                decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                predictions.extend([d.strip() for d in decoded])\n",
    "        \n",
    "        final_predictions = predictions\n",
    "    else:\n",
    "        print(\"Falling back to voting ensemble...\")\n",
    "        final_predictions = voting_ensemble()\n",
    "        \n",
    "else:  # auto mode\n",
    "    # Try weight averaging first, fall back to voting\n",
    "    merged_model, tokenizer = try_weight_averaging()\n",
    "    \n",
    "    if merged_model is not None:\n",
    "        print(\"\\n‚úì Using weight-averaged model\")\n",
    "        merged_model = merged_model.to(DEVICE).eval()\n",
    "        dataset = InferenceDataset(test_inputs, tokenizer, MAX_LENGTH, \"translate Akkadian to English: \")\n",
    "        loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Inference\"):\n",
    "                outputs = merged_model.generate(\n",
    "                    input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                    attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "                decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                predictions.extend([d.strip() for d in decoded])\n",
    "        \n",
    "        final_predictions = predictions\n",
    "    else:\n",
    "        print(\"\\n‚úì Using voting ensemble (models incompatible for averaging)\")\n",
    "        final_predictions = voting_ensemble()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CREATE SUBMISSION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"translation\": final_predictions\n",
    "})\n",
    "\n",
    "submission[\"translation\"] = submission[\"translation\"].apply(\n",
    "    lambda x: x if (x and len(x) > 0) else \"broken text\"\n",
    ")\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Submission saved to submission.csv\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Total: {len(submission)}\")\n",
    "print(f\"  Avg length: {submission['translation'].str.split().str.len().mean():.1f} words\")\n",
    "print(f\"  Empty/broken: {(submission['translation'] == 'broken text').sum()}\")\n",
    "\n",
    "print(f\"\\nüìã Sample predictions:\")\n",
    "print(submission.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SUBMISSION GUIDE\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Train all 3 models** using the training notebooks\n",
    "2. **Save each as a Kaggle dataset**\n",
    "3. **Add datasets as inputs** to this notebook\n",
    "4. **Adjust weights** in MODEL_CONFIGS (use find-optimal-weights.ipynb)\n",
    "5. **Run this notebook** ‚Üí generates submission.csv\n",
    "\n",
    "## Ensemble Modes\n",
    "\n",
    "Set `ENSEMBLE_MODE` in the first cell:\n",
    "- **\"auto\"** (default) - Tries weight averaging, falls back to voting\n",
    "- **\"voting\"** - Always use voting ensemble (slower but works with any models)\n",
    "- **\"averaging\"** - Only use weight averaging (faster but requires compatible models)\n",
    "\n",
    "## How to Find Optimal Weights\n",
    "\n",
    "Run `find-optimal-weights.ipynb` to automatically:\n",
    "1. Evaluate each model on validation set\n",
    "2. Calculate BLEU scores\n",
    "3. Grid search for best weight combination\n",
    "4. Output optimal weights\n",
    "\n",
    "Then update MODEL_CONFIGS with the results.\n",
    "\n",
    "## Default Weights\n",
    "\n",
    "Current configuration (adjust based on your validation):\n",
    "- **ByT5**: 0.35 (character-level, good for morphology)\n",
    "- **T5**: 0.40 (usually best overall)\n",
    "- **MarianMT**: 0.25 (translation-focused)\n",
    "\n",
    "## Model Paths\n",
    "\n",
    "Update these if your datasets have different names:\n",
    "- MODEL1_PATH: `/kaggle/input/notebook-a-byt5/byt5-base-saved`\n",
    "- MODEL2_PATH: `/kaggle/input/notebook-b-t5/t5-base-fine-tuned`\n",
    "- MODEL3_PATH: `/kaggle/input/notebook-c-marian-mt/marian-mt-saved`\n",
    "\n",
    "## Tips for Better Performance\n",
    "\n",
    "1. **Use validation scores** to set weights (don't guess!)\n",
    "2. **Increase num_beams** to 6 or 8 for better quality (slower)\n",
    "3. **Adjust max_length** per model based on typical output length\n",
    "4. **Add repetition_penalty=1.2** if outputs are too repetitive\n",
    "5. **Post-process** predictions for capitalization and punctuation\n",
    "\n",
    "## Expected Behavior\n",
    "\n",
    "- If all models have **same architecture** ‚Üí Uses weight averaging (faster)\n",
    "- If models have **different architectures** ‚Üí Uses voting ensemble (more robust)\n",
    "- If only **1 model** available ‚Üí Uses single model\n",
    "- Automatically handles missing models gracefully\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Out of Memory?**\n",
    "- Reduce BATCH_SIZE from 8 to 4 or 2\n",
    "- Use ENSEMBLE_MODE=\"averaging\" (loads only 1 model at a time)\n",
    "\n",
    "**Low scores?**\n",
    "- Run find-optimal-weights.ipynb to get better weights\n",
    "- Check that gap replacement is working (should see `<gap>` and `<big_gap>`)\n",
    "- Verify all models trained properly\n",
    "\n",
    "**Models won't merge?**\n",
    "- Normal! ByT5, T5, and MarianMT have different architectures\n",
    "- Notebook will automatically use voting ensemble instead\n",
    "- This is actually better for diverse models\n",
    "\n",
    "## Architecture Compatibility\n",
    "\n",
    "| Models | Compatible? | Strategy |\n",
    "|--------|-------------|----------|\n",
    "| ByT5 + T5 | Maybe ‚úì | Can try averaging |\n",
    "| ByT5 + MarianMT | No ‚úó | Use voting |\n",
    "| T5 + MarianMT | No ‚úó | Use voting |\n",
    "| All 3 | No ‚úó | Use voting |\n",
    "\n",
    "Voting ensemble is **more powerful** for different architectures anyway!\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9084006,
     "sourceId": 14290093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084005,
     "sourceId": 14290098,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084224,
     "sourceId": 14290100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
