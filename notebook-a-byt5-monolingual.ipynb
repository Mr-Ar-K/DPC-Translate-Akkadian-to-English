{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":121150,"databundleVersionId":14976537,"sourceType":"competition"},{"sourceId":14236819,"sourceType":"datasetVersion","datasetId":9082937}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":6059.435801,"end_time":"2025-12-25T12:09:11.716859","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-25T10:28:12.281058","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1fe03852120a40cda6d5c85f4e29fcaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b48bf215217541358d86be12f43b7f3c","placeholder":"​","style":"IPY_MODEL_fcaea7f32ee24eebaebfc0e32266d44a","tabbable":null,"tooltip":null,"value":" 8.15k/? [00:00&lt;00:00, 928kB/s]"}},"226d44a1d8884129a1d1230b2642a786":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b0c8da6c308b446cb01170a376a943c3","max":77,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39da431ed6e041ef8c50061ff0cb2734","tabbable":null,"tooltip":null,"value":77}},"2bccb998d49d415580aaed49b33a1b22":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_791efc0e593d4599ba993239183a55e3","placeholder":"​","style":"IPY_MODEL_683b28591d2f4363aa74a0a1a44c7a1c","tabbable":null,"tooltip":null,"value":" 77/77 [00:00&lt;00:00, 388.82 examples/s]"}},"2d21f5383a1548d98e116b290031fccd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39da431ed6e041ef8c50061ff0cb2734":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b23edd7473a4b5cbeb0e7453bf9b6bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"427b3335b5374628a175042c3a3382e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2d21f5383a1548d98e116b290031fccd","placeholder":"​","style":"IPY_MODEL_9aed8f291fff48a5a708caf8c20013d8","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"44b7b6316c1c4a4bbc5e3b84e174e79f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_e6a4941ccd41469aa09a0ec5a804af6e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b76718f5d5b1422ab37b370c9cc527ff","tabbable":null,"tooltip":null,"value":1}},"45c5ff13e0804ab38df84c594f5cbfe4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c11ec6efe7cb4722883a279f3eb185ed","placeholder":"​","style":"IPY_MODEL_8846ab600ef243c39c4f1cb71a308f59","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"4a7eec71c5664bbd922f48d02fb439ac":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b48b9cb404748bdba5a1554f4687518":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e865f98ee6084edf9b9c63807b52da6b","IPY_MODEL_9f53690cf1d84b5aa3e3929a66799044","IPY_MODEL_f030f089b1124566ab437b62c99b2578"],"layout":"IPY_MODEL_c2b4df78e626401c8a097c7890f1386e","tabbable":null,"tooltip":null}},"6297171298534da8a8ae461611777771":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"67bff04e3ed849f8aadf4b6b7df2b9e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_427b3335b5374628a175042c3a3382e4","IPY_MODEL_44b7b6316c1c4a4bbc5e3b84e174e79f","IPY_MODEL_1fe03852120a40cda6d5c85f4e29fcaf"],"layout":"IPY_MODEL_b8d2e1271f1e4afdb533651d811a2bb8","tabbable":null,"tooltip":null}},"683b28591d2f4363aa74a0a1a44c7a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6c90b63b47dc4de495594cfd1e6519b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"701dd03462844cf0af47a3ab8c452a04":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45c5ff13e0804ab38df84c594f5cbfe4","IPY_MODEL_bc4d79d1c6384c23a736957e90afd304","IPY_MODEL_9d58cd0d03464cd18c653fdec13d87e4"],"layout":"IPY_MODEL_b0bcd130aa8341b1acef83b7cc545dce","tabbable":null,"tooltip":null}},"71affa5ac8894c80959fbb0448728c81":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c324f021e0034d85bcf6459f17a7a752","placeholder":"​","style":"IPY_MODEL_6297171298534da8a8ae461611777771","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"791efc0e593d4599ba993239183a55e3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8846ab600ef243c39c4f1cb71a308f59":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8f98493e151546f2baa629a4edf06a14":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9369db00183844a5b53806dd64a214db":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"969e0b0841ff4999a85c76101abdcc41":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9aed8f291fff48a5a708caf8c20013d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9d58cd0d03464cd18c653fdec13d87e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b789e817126e4af1a184cfae19f2612f","placeholder":"​","style":"IPY_MODEL_3b23edd7473a4b5cbeb0e7453bf9b6bc","tabbable":null,"tooltip":null,"value":" 9.01k/? [00:00&lt;00:00, 914kB/s]"}},"9f53690cf1d84b5aa3e3929a66799044":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_ab64a8242ff94bdfbfceeabac64877cf","max":1452,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0f4b1fc30b24fd1a84d58113003352a","tabbable":null,"tooltip":null,"value":1452}},"a1151781b3e84a738ae66c7ac283ba3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71affa5ac8894c80959fbb0448728c81","IPY_MODEL_226d44a1d8884129a1d1230b2642a786","IPY_MODEL_2bccb998d49d415580aaed49b33a1b22"],"layout":"IPY_MODEL_4a7eec71c5664bbd922f48d02fb439ac","tabbable":null,"tooltip":null}},"a8e873b972004c38ae11c1d76cfe84ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab64a8242ff94bdfbfceeabac64877cf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae69eb96031c4d43bf4c52da5aedb198":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0bcd130aa8341b1acef83b7cc545dce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0c8da6c308b446cb01170a376a943c3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b48bf215217541358d86be12f43b7f3c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b76718f5d5b1422ab37b370c9cc527ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b789e817126e4af1a184cfae19f2612f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8d2e1271f1e4afdb533651d811a2bb8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4d79d1c6384c23a736957e90afd304":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_969e0b0841ff4999a85c76101abdcc41","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8e873b972004c38ae11c1d76cfe84ba","tabbable":null,"tooltip":null,"value":1}},"c11ec6efe7cb4722883a279f3eb185ed":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b4df78e626401c8a097c7890f1386e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c324f021e0034d85bcf6459f17a7a752":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0f4b1fc30b24fd1a84d58113003352a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a4941ccd41469aa09a0ec5a804af6e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e865f98ee6084edf9b9c63807b52da6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ae69eb96031c4d43bf4c52da5aedb198","placeholder":"​","style":"IPY_MODEL_8f98493e151546f2baa629a4edf06a14","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"f030f089b1124566ab437b62c99b2578":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9369db00183844a5b53806dd64a214db","placeholder":"​","style":"IPY_MODEL_6c90b63b47dc4de495594cfd1e6519b0","tabbable":null,"tooltip":null,"value":" 1452/1452 [00:03&lt;00:00, 448.42 examples/s]"}},"fcaea7f32ee24eebaebfc0e32266d44a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"76a4c494","cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:05.689107Z","iopub.execute_input":"2026-01-12T14:28:05.689642Z","iopub.status.idle":"2026-01-12T14:28:05.913957Z","shell.execute_reply.started":"2026-01-12T14:28:05.689610Z","shell.execute_reply":"2026-01-12T14:28:05.913246Z"},"papermill":{"duration":0.184686,"end_time":"2025-12-25T10:28:15.031482","exception":false,"start_time":"2025-12-25T10:28:14.846796","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Mon Jan 12 14:28:05 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"id":"853653de","cell_type":"markdown","source":"# A1. Install required libraries","metadata":{"papermill":{"duration":0.006431,"end_time":"2025-12-25T10:28:15.044668","exception":false,"start_time":"2025-12-25T10:28:15.038237","status":"completed"},"tags":[]}},{"id":"964c2326","cell_type":"code","source":"!pip install -q evaluate sacrebleu","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2026-01-12T14:28:05.918445Z","iopub.execute_input":"2026-01-12T14:28:05.918857Z","iopub.status.idle":"2026-01-12T14:28:10.831771Z","shell.execute_reply.started":"2026-01-12T14:28:05.918823Z","shell.execute_reply":"2026-01-12T14:28:10.830877Z"},"papermill":{"duration":4.824313,"end_time":"2025-12-25T10:28:19.875330","exception":false,"start_time":"2025-12-25T10:28:15.051017","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"id":"5dd5f368","cell_type":"markdown","source":"# A2. Imports & config","metadata":{"papermill":{"duration":0.006597,"end_time":"2025-12-25T10:28:19.889807","exception":false,"start_time":"2025-12-25T10:28:19.883210","status":"completed"},"tags":[]}},{"id":"ec80cf0e","cell_type":"code","source":"import os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed\n)\nimport evaluate\n\n# Memory/precision safety tweaks (helps avoid OOM on P100/T4)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntry:\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.benchmark = False\n    torch.set_float32_matmul_precision(\"medium\")\nexcept Exception:\n    pass\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:10.833086Z","iopub.execute_input":"2026-01-12T14:28:10.833387Z","iopub.status.idle":"2026-01-12T14:28:38.772075Z","shell.execute_reply.started":"2026-01-12T14:28:10.833360Z","shell.execute_reply":"2026-01-12T14:28:38.771349Z"},"papermill":{"duration":33.038891,"end_time":"2025-12-25T10:28:52.935206","exception":false,"start_time":"2025-12-25T10:28:19.896315","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2026-01-12 14:28:23.832944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768228103.995511      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768228104.045372      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768228104.430344      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768228104.430381      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768228104.430384      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768228104.430386      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"id":"b38e3a6c","cell_type":"markdown","source":"# A3. Set constants (DO NOT change yet)","metadata":{"papermill":{"duration":0.006759,"end_time":"2025-12-25T10:28:52.949032","exception":false,"start_time":"2025-12-25T10:28:52.942273","status":"completed"},"tags":[]}},{"id":"cbc9780c","cell_type":"code","source":"\n# === CONFIGURATION: THE PURIST ===\nMODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nOUTPUT_DIR = \"/kaggle/working/byt5-base-saved\"\n\nENABLE_MONO_PRETRAIN = True   # <--- CRITICAL: Keeps the \"Purist\" logic\nMAX_LENGTH = 300              # Reduced from 512 for ByT5 speed/memory\nPREFIX = \"translate Akkadian to English: \"\n\nBATCH_SIZE = 4                # Adjust based on GPU memory\nGRAD_ACCUM = 8                # Gradient accumulation steps\n","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:38.773121Z","iopub.execute_input":"2026-01-12T14:28:38.774167Z","iopub.status.idle":"2026-01-12T14:28:38.778930Z","shell.execute_reply.started":"2026-01-12T14:28:38.774127Z","shell.execute_reply":"2026-01-12T14:28:38.778193Z"},"language":"python","papermill":{"duration":0.013576,"end_time":"2025-12-25T10:28:52.969057","exception":false,"start_time":"2025-12-25T10:28:52.955481","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"9041d778","cell_type":"markdown","source":"# A4. Data Loading & Cleaning","metadata":{"papermill":{"duration":0.006442,"end_time":"2025-12-25T10:28:52.982062","exception":false,"start_time":"2025-12-25T10:28:52.975620","status":"completed"},"tags":[]}},{"id":"e9e6077f","cell_type":"markdown","source":"# A3.5. DATA PREPARATION GUIDE: Handling Akkadian Formatting Issues\n\n## Problem: \"Garbage In, Garbage Out\"\nAkkadian texts contain complex formatting that can break ML pipelines if not handled properly.\n\n## Formatting Issues to Handle\n\n### 1. Scribal Notations (Remove)\n- `!` - Certain reading (remove)\n- `?` - Questionable reading (remove)\n- `/` - Line divider (remove)\n- `:` or `.` - Word divider (remove)\n- `< >` - Scribal insertions (keep content, remove brackets)\n- `( )` - Comments/erasures (remove entirely)\n- `˹ ˺` - Half brackets for partially broken signs (remove)\n- `[ ]` - Clearly broken signs (keep content, remove brackets)\n- `<< >>` - Errant signs (remove entirely)\n\n### 2. Gaps & Lacunae (Standardize)\n- `[x]` → `<gap>`\n- `x` → `<gap>`\n- `xx` → `<gap>`\n- `…` → `<big_gap>`\n- `……` → `<big_gap>`\n- `[... ...]` → `<big_gap>`\n- Multiple `.3` or `...` sequences → `<big_gap>`\n\n### 3. Determinatives (Keep content, remove brackets)\n- `{d}` - Deity (remove brackets)\n- `{ki}` - Earth/location (remove brackets)\n- `{lu₂}` - Person (remove brackets)\n- `{e₂}` - Building (remove brackets)\n- And 10+ others...\n\n### 4. Subscripts & Superscripts (Normalize)\n- `a₂` → `a2`, `a₃` → `a3`, etc.\n- `il₅` → `il5`, etc.\n- Works with Unicode characters (U+2080-U+2089)\n\n### 5. Special Characters (Handle as-is or normalize)\n- `š` (U+0161), `Š` (U+0160)\n- `ṣ` (U+1E63), `Ṣ` (U+1E62)\n- `ṭ` (U+1E6D), `Ṭ` (U+1E6C)\n- `ḫ` (U+1E2B), `Ḫ` (U+1E2A)\n- `ʾ` (U+02BE) - Akkadian letter marker\n\n### 6. Capitalization Rules (Preserve)\n- First letter capital = Proper noun (personal/place name)\n- ALL CAPS = Sumerian logogram (preserve for domain knowledge)\n\n## Processing Order\n1. Normalize subscripts FIRST (₀-₉ → 0-9)\n2. Handle gaps (complex patterns first, then simple)\n3. Remove scribal notations\n4. Extract content from bracketed structures\n5. Clean whitespace\n6. Validate output (length checks, character validation)\n\n## Data Validation Checks\n✓ No empty strings after cleaning\n✓ Source length >= 3 words\n✓ Target length >= 3 words\n✓ Length ratio between 0.2 and 5.0\n✓ No duplicate pairs\n✓ All special characters properly handled","metadata":{}},{"id":"9035f86f","cell_type":"code","source":"\"\"\"\nCOMPREHENSIVE DATA PREPROCESSING FOR AKKADIAN TEXTS\nHandles all formatting issues mentioned in competition guidelines\n\"\"\"\n\n# ============================================================================\n# SUBSCRIPT & SUPERSCRIPT NORMALIZATION\n# ============================================================================\nSUBSCRIPT_TRANS = str.maketrans({\n    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \n    \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \n    \"ₓ\": \"x\"\n})\n\ndef normalize_subscripts(text: str) -> str:\n    \"\"\"Convert subscript Unicode characters to regular numbers\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    return text.translate(SUBSCRIPT_TRANS)\n\n# ============================================================================\n# GAP & LACUNAE HANDLING\n# ============================================================================\ndef replace_gaps(text, keep_gaps=True):\n    \"\"\"\n    Replace various gap notations with standardized tokens.\n    Handles all gap patterns mentioned in competition guidelines.\n    \n    Args:\n        text: Input text with gaps\n        keep_gaps: If True, keeps <gap> and <big_gap> tokens.\n                  If False, removes them completely.\n    \n    Returns:\n        Text with normalized gap tokens\n    \"\"\"\n    if pd.isna(text): \n        return text\n    \n    # STEP 1: Complex gap patterns (order matters!)\n    # [...] patterns for multiple dots\n    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\.\\s*\\.\\s*\\]', '<big_gap>', text)  # [......]\n    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\.\\s*\\]', '<big_gap>', text)       # [....]\n    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\]', '<gap>', text)                 # [...] \n    \n    # Multiple .3 patterns with multiple dots\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    \n    # Multiple dots (.....)\n    text = re.sub(r'\\.{4,}', '<big_gap>', text)  # 4+ dots = big gap\n    \n    # STEP 2: Unicode gap markers\n    text = re.sub(r'……', '<big_gap>', text)      # Unicode horizontal ellipsis\n    text = re.sub(r'…', '<big_gap>', text)        # Unicode single ellipsis\n    \n    # STEP 3: Standard dot patterns\n    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)  # Multiple ... groups\n    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)  # Three dots\n    text = re.sub(r'\\.\\.', '<gap>', text)        # Two dots\n    \n    # STEP 4: [x] and [xx] patterns\n    text = re.sub(r'\\[x+\\]', '<gap>', text)      # [x] or [xx]\n    \n    # STEP 5: Bare x patterns\n    text = re.sub(r'(?:^|\\s)xx(?:\\s|$)', ' <gap> ', text)  # xx as separate word\n    text = re.sub(r'(?:^|\\s)x(?:\\s|$)', ' <gap> ', text)   # x as separate word\n    \n    # STEP 6: Remove gaps if not needed\n    if not keep_gaps:\n        text = re.sub(r'<big_gap>', '', text)\n        text = re.sub(r'<gap>', '', text)\n    \n    return text\n\n# ============================================================================\n# SCRIBAL NOTATION REMOVAL\n# ============================================================================\ndef remove_scribal_notations(text):\n    \"\"\"\n    Remove modern scribal notations that are not meaningful for translation.\n    These are editorial marks added by scholars, not part of the original text.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove line number markers (1, 5, 10, 1', 1'')\n    text = re.sub(r'\\b\\d+\\'?\\s*\\'?\\s*\\b', ' ', text)\n    \n    # Remove uncertainty markers\n    text = re.sub(r'[!?]', ' ', text)  # ! = certain, ? = uncertain\n    \n    # Remove other scribal punctuation\n    text = re.sub(r'[/:·]', ' ', text)  # / = line divider, : = word divider, · = separator\n    \n    return text\n\n# ============================================================================\n# BRACKETED CONTENT HANDLING\n# ============================================================================\ndef handle_brackets(text):\n    \"\"\"\n    Handle various bracket types according to guidelines.\n    \n    - ( ) Remove entirely (comments/erasures)\n    - < > Keep content (scribal insertions)\n    - [ ] Keep content (clearly broken signs)\n    - { } Keep content (determinatives)\n    - << >> Remove entirely (errant signs)\n    - ˹ ˺ Remove (half brackets for partially broken)\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Remove comments and erasures (keep nothing)\n    text = re.sub(r'\\([^)]*\\)', ' ', text)\n    \n    # Keep content from scribal insertions and broken signs\n    text = re.sub(r'<([^>]*)>', r'\\1', text)      # <content> → content\n    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)   # [content] → content\n    \n    # Determinatives: {content} → content (removes classifier brackets)\n    text = re.sub(r'\\{([^}]*)\\}', r'\\1', text)\n    \n    # Remove half brackets for partially broken signs\n    text = re.sub(r'[˹˺]', ' ', text)\n    \n    # Remove errant/erroneous signs entirely\n    text = re.sub(r'<<[^>]*>>', ' ', text)\n    \n    return text\n\n# ============================================================================\n# MAIN TRANSLITERATION CLEANING FUNCTION\n# ============================================================================\ndef clean_translit(text, keep_gaps=True):\n    \"\"\"\n    Comprehensive normalization of Akkadian transliteration.\n    Handles all formatting issues in proper order.\n    \n    Processing order:\n    1. Normalize subscripts\n    2. Handle gaps\n    3. Remove scribal notations\n    4. Handle bracket types\n    5. Clean whitespace\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # STEP 1: Normalize subscripts/superscripts FIRST\n    text = normalize_subscripts(text)\n    \n    # STEP 2: Handle gaps (complex patterns)\n    text = replace_gaps(text, keep_gaps=keep_gaps)\n    \n    # STEP 3: Remove scribal notations\n    text = remove_scribal_notations(text)\n    \n    # STEP 4: Handle all bracket types\n    text = handle_brackets(text)\n    \n    # STEP 5: Clean whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.strip()\n\n# ============================================================================\n# TRANSLATION CLEANING FUNCTION\n# ============================================================================\ndef clean_translation(text, has_gaps=False):\n    \"\"\"\n    Clean translation with minimal processing.\n    Keep as much content as possible.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Handle gap indicators if source has gaps\n    if not has_gaps:\n        text = text.replace(\"…\", \" \")\n    \n    # Clean whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.strip()\n\n# ============================================================================\n# DATA QUALITY FILTERING\n# ============================================================================\ndef filter_quality(df):\n    \"\"\"\n    Filter out low-quality pairs based on validation checks.\n    \n    Validation criteria:\n    - Minimum 3 words in source and target\n    - Length ratio between 0.2 and 5.0\n    - No duplicate pairs\n    \"\"\"\n    # Calculate lengths\n    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n    \n    # Minimum length check\n    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n    \n    # Length ratio check (one language often longer than other)\n    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n    df = df[(ratio >= 0.2) & (ratio <= 5)]\n    \n    # Remove exact duplicates\n    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n    \n    # Cleanup\n    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n\n# ============================================================================\n# VALIDATION & REPORTING\n# ============================================================================\ndef validate_preprocessing(original_df, cleaned_df):\n    \"\"\"\n    Report on preprocessing impact.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"DATA PREPROCESSING VALIDATION\")\n    print(\"=\"*60)\n    print(f\"Original samples: {len(original_df)}\")\n    print(f\"After cleaning: {len(cleaned_df)}\")\n    print(f\"Removed: {len(original_df) - len(cleaned_df)} samples\")\n    \n    if len(cleaned_df) > 0:\n        avg_src = cleaned_df[\"transliteration\"].str.split().str.len().mean()\n        avg_tgt = cleaned_df[\"translation\"].str.split().str.len().mean()\n        print(f\"Avg source length: {avg_src:.1f} words\")\n        print(f\"Avg target length: {avg_tgt:.1f} words\")\n        print(f\"Avg ratio (src/tgt): {avg_src/avg_tgt:.2f}\")\n    print(\"=\"*60 + \"\\n\")\n\n# Replace gaps function (with corrected newlines and indentation)\ndef replace_gaps(text, keep_gaps=True):\n    \"\"\"Replace various gap notations with standardized tokens\n    \n    Args:\n        keep_gaps: If True, keeps gap tokens (for test-like data).\n                   If False, removes them (for clean training).\n    \"\"\"\n    if pd.isna(text): \n        return text\n    \n    # Complex gap patterns (order matters)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n\n    # Simple gap patterns\n    text = re.sub(r'xx', '<gap>', text)\n    text = re.sub(r' x ', ' <gap> ', text)\n    text = re.sub(r'……', '<big_gap>', text)\n    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n    text = re.sub(r'…', '<big_gap>', text)\n    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n    \n    # Bracketed gaps\n    text = re.sub(r'\\[\\.\\.\\.+\\]', '<big_gap>', text)\n    text = re.sub(r'\\[x+\\]', '<gap>', text)\n    \n    if not keep_gaps:\n        # Remove gaps for clean training\n        text = re.sub(r'<big_gap>', '', text)\n        text = re.sub(r'<gap>', '', text)\n\n    return text\n\ndef clean_translit(text, keep_gaps=True):\n    \"\"\"Normalize transliteration following competition guidance.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = normalize_subscripts(text)\n    # Apply gap replacement - KEEP gaps for domain matching\n    text = replace_gaps(text, keep_gaps=keep_gaps)\n    # Only remove scribal markers, keep gaps\n    text = re.sub(r\"<<[^>]*>>\", \" \", text)               # errant signs\n    text = re.sub(r\"[˹˺]\", \" \", text)                    # half brackets\n    text = re.sub(r\"\\([^)]*\\)\", \" \", text)             # comments/erasures\n    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)         # determinatives\n    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)            # scribal insertions keep content\n    text = re.sub(r\"[!?/:·]\", \" \", text)                 # scribal punctuation\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef clean_translation(text, has_gaps=False):\n    \"\"\"Clean translation, optionally keeping gap indicators\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    if not has_gaps:\n        text = text.replace(\"…\", \" \")\n    # Keep ... if source has gaps\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef filter_quality(df):\n    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n    df = df[(ratio >= 0.2) & (ratio <= 5)]\n    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n    return df.drop(columns=[\"src_len\", \"tgt_len\"])","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:38.781119Z","iopub.execute_input":"2026-01-12T14:28:38.781436Z","iopub.status.idle":"2026-01-12T14:28:38.807609Z","shell.execute_reply.started":"2026-01-12T14:28:38.781413Z","shell.execute_reply":"2026-01-12T14:28:38.807045Z"},"papermill":{"duration":0.469755,"end_time":"2025-12-25T10:28:53.458249","exception":false,"start_time":"2025-12-25T10:28:52.988494","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"373e0f72","cell_type":"markdown","source":"# A5 . Tokenization","metadata":{"papermill":{"duration":0.006698,"end_time":"2025-12-25T10:28:53.471962","exception":false,"start_time":"2025-12-25T10:28:53.465264","status":"completed"},"tags":[]}},{"id":"4369be9c","cell_type":"code","source":"# Load and preprocess training data\nprint(\"=\"*60)\nprint(\"LOADING & PREPROCESSING DATA\")\nprint(\"=\"*60)\n\ntrain_path = f\"{DATA_DIR}/train.csv\"\nprint(f\"Loading data from: {train_path}\")\n\ntrain_df = pd.read_csv(train_path)\nprint(f\"Original dataset size: {len(train_df)}\")\n\n# Clean data\ntrain_df = train_df.dropna(subset=[\"transliteration\", \"translation\"])\ntrain_df[\"transliteration\"] = train_df[\"transliteration\"].astype(str).apply(clean_translit)\ntrain_df[\"translation\"] = train_df[\"translation\"].astype(str).apply(clean_translation)\n\n# Filter quality\ntrain_df = filter_quality(train_df)\nprint(f\"After quality filtering: {len(train_df)}\")\n\n# Create train/validation split\nfrom sklearn.model_selection import train_test_split\ntrain_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\n\n# Convert to HuggingFace Dataset\ndataset = {\n    \"train\": Dataset.from_pandas(train_data[[\"transliteration\", \"translation\"]].reset_index(drop=True)),\n    \"test\": Dataset.from_pandas(val_data[[\"transliteration\", \"translation\"]].reset_index(drop=True))\n}\n\nprint(\"=\"*60)\nprint(\"✓ Data loaded and preprocessed successfully\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T14:28:38.808480Z","iopub.execute_input":"2026-01-12T14:28:38.808782Z","iopub.status.idle":"2026-01-12T14:28:39.174959Z","shell.execute_reply.started":"2026-01-12T14:28:38.808749Z","shell.execute_reply":"2026-01-12T14:28:39.174378Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING & PREPROCESSING DATA\n============================================================\nLoading data from: /kaggle/input/deep-past-initiative-machine-translation/train.csv\nOriginal dataset size: 1561\nAfter quality filtering: 1528\nTraining samples: 1375\nValidation samples: 153\n============================================================\n✓ Data loaded and preprocessed successfully\n============================================================\n\n","output_type":"stream"}],"execution_count":6},{"id":"af1a0a55","cell_type":"code","source":"\nprint(\"Loading Tokenizer from:\", MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndef preprocess_function(examples):\n    inputs = [doc for doc in examples[\"transliteration\"]]\n    targets = examples[\"translation\"]\n\n    model_inputs = tokenizer(\n        inputs, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\"\n    )\n\n    # FIX: Use text_target to avoid warnings\n    labels = tokenizer(\n        text_target=targets, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\"\n    )\n\n    model_inputs[\"labels\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n    return model_inputs\n\n# Process datasets\ntokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ntokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)\n","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:39.175753Z","iopub.execute_input":"2026-01-12T14:28:39.176018Z","iopub.status.idle":"2026-01-12T14:28:42.228950Z","shell.execute_reply.started":"2026-01-12T14:28:39.175984Z","shell.execute_reply":"2026-01-12T14:28:42.228408Z"},"language":"python","papermill":{"duration":3.528045,"end_time":"2025-12-25T10:28:57.006773","exception":false,"start_time":"2025-12-25T10:28:53.478728","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading Tokenizer from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1375 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12049577f5dd42fa82f336e3604bab34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/153 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b0efb550774b5bb49f14289e38c70c"}},"metadata":{}}],"execution_count":7},{"id":"680c9278","cell_type":"markdown","source":"# A6. Model Setup","metadata":{"papermill":{"duration":0.007545,"end_time":"2025-12-25T10:28:57.023322","exception":false,"start_time":"2025-12-25T10:28:57.015777","status":"completed"},"tags":[]}},{"id":"447be19d","cell_type":"code","source":"print(\"Loading Model from:\", MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n\n# Data Collator handles dynamic padding during batching\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    label_pad_token_id=-100\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:42.229841Z","iopub.execute_input":"2026-01-12T14:28:42.230094Z","iopub.status.idle":"2026-01-12T14:28:43.104866Z","shell.execute_reply.started":"2026-01-12T14:28:42.230059Z","shell.execute_reply":"2026-01-12T14:28:43.104094Z"},"papermill":{"duration":1.398574,"end_time":"2025-12-25T10:28:58.434786","exception":false,"start_time":"2025-12-25T10:28:57.036212","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading Model from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n","output_type":"stream"}],"execution_count":8},{"id":"314f0dc9","cell_type":"markdown","source":"# A6. Optional: Monolingual Pre-Training on Akkadian Texts\n\nThis step teaches the model Akkadian grammar and morphology BEFORE translation training.\nUses published_texts.csv (8,000+ Akkadian texts) with Masked Language Modeling (MLM).\n\nBenefits:\n- Model learns to handle gaps naturally\n- Better understanding of Akkadian word structure\n- Improves low-resource translation performance\n\nSet ENABLE_MONO_PRETRAIN=True to enable (adds ~30min training time).","metadata":{}},{"id":"a86171f5","cell_type":"code","source":"# Monolingual Pre-Training Configuration\nENABLE_MONO_PRETRAIN = bool(int(os.getenv(\"ENABLE_MONO_PRETRAIN\", \"1\")))  # Set to 1 to enable\n\nif ENABLE_MONO_PRETRAIN:\n    print(\"\\n\" + \"=\"*60)\n    print(\"MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\")\n    print(\"=\"*60)\n    \n    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n    \n    if os.path.exists(pub_texts_path):\n        # Load Akkadian-only texts\n        pub_texts_df = pd.read_csv(pub_texts_path)\n        akkadian_texts = pub_texts_df['transliteration'].dropna().astype(str).tolist()\n        akkadian_texts = [clean_translit(t, keep_gaps=True) for t in akkadian_texts]\n        akkadian_texts = [t for t in akkadian_texts if len(t.split()) >= 5 and len(t.split()) <= 200]\n        akkadian_texts = akkadian_texts[:5000]  # Limit for time\n        \n        print(f\"Loaded {len(akkadian_texts)} Akkadian texts for pre-training\")\n        \n        # Simple MLM approach: Mask random spans\n        from transformers import DataCollatorForSeq2Seq\n        \n        def create_mlm_examples(texts):\n            \"\"\"Create masked language modeling examples\"\"\"\n            mlm_examples = []\n            for text in texts:\n                tokens = text.split()\n                if len(tokens) < 5:\n                    continue\n                \n                # Mask 15% of tokens\n                n_mask = max(1, int(len(tokens) * 0.15))\n                mask_positions = np.random.choice(len(tokens), size=n_mask, replace=False)\n                \n                masked_text = []\n                for i, token in enumerate(tokens):\n                    if i in mask_positions:\n                        masked_text.append(\"<extra_id_0>\")  # sentinel-style token\n                    else:\n                        masked_text.append(token)\n                \n                input_text = \" \".join(masked_text)\n                target_text = \" \".join([tokens[i] for i in mask_positions])\n                \n                mlm_examples.append({\n                    \"transliteration\": input_text,\n                    \"translation\": target_text\n                })\n            \n            return mlm_examples\n        \n        mlm_data = create_mlm_examples(akkadian_texts)\n        print(f\"Created {len(mlm_data)} MLM training examples\")\n        \n        # Create MLM dataset\n        mlm_dataset = Dataset.from_pandas(pd.DataFrame(mlm_data))\n        \n        def preprocess_mlm(examples):\n            inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n            targets = examples[\"translation\"]\n            model_inputs = tokenizer(\n                inputs,\n                max_length=MAX_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n            labels = tokenizer(\n                text_target=targets,\n                max_length=MAX_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n            model_inputs[\"labels\"] = [\n                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n                for label in labels[\"input_ids\"]\n            ]\n            return model_inputs\n        \n        tokenized_mlm = mlm_dataset.map(preprocess_mlm, batched=True)\n        \n        # UPDATED MLM ARGUMENTS (Safe Mode)\n        mlm_args = Seq2SeqTrainingArguments(\n            output_dir=f\"{OUTPUT_DIR}_mlm\",\n            num_train_epochs=1,\n            learning_rate=2e-4,              # Lower LR for stability\n            \n            # MEMORY & STABILITY FIXES\n            per_device_train_batch_size=1,   # Batch size 1 prevents OOM\n            gradient_accumulation_steps=16,  # Simulates batch 16\n            fp16=False,                      # MUST BE FALSE for ByT5\n            \n            save_strategy=\"no\",\n            eval_strategy=\"no\",\n            logging_steps=50,\n            report_to=\"none\"\n        )\n        \n        # FIX: processing_class instead of tokenizer (Removes Warning)\n        mlm_trainer = Seq2SeqTrainer(\n            model=model,\n            args=mlm_args,\n            train_dataset=tokenized_mlm,\n            processing_class=tokenizer,      # Updated argument name\n            data_collator=data_collator,\n        )\n        \n        print(\"Starting monolingual pre-training (1 epoch on Akkadian texts)...\")\n        try:\n            mlm_trainer.train()\n            print(\"✓ Monolingual pre-training complete\")\n            print(\"Model now understands Akkadian grammar and gaps better!\")\n        except Exception as e:\n            print(f\"⚠️  MLM pre-training failed: {e}\")\n            print(\"Continuing with main training...\")\n    \n    else:\n        print(\"⚠️  published_texts.csv not found, skipping monolingual pre-training\")\nelse:\n    print(\"\\n⚠️  Monolingual pre-training disabled (set ENABLE_MONO_PRETRAIN=1 to enable)\")","metadata":{"execution":{"iopub.status.busy":"2026-01-12T14:28:43.105925Z","iopub.execute_input":"2026-01-12T14:28:43.106324Z","execution_failed":"2026-01-12T14:42:30.208Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n============================================================\nMONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\n============================================================\nLoaded 5000 Akkadian texts for pre-training\nCreated 5000 MLM training examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38935318a08c4d2ba9f0349a36706ff8"}},"metadata":{}},{"name":"stdout","text":"Starting monolingual pre-training (1 epoch on Akkadian texts)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='56' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 56/157 12:58 < 24:15, 0.07 it/s, Epoch 0.35/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.027300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"id":"c71300a3","cell_type":"code","source":"# Quick data stats after mining and merge\nsup_count_est = len(train_df) - (len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0)\nprint(\"\\n=== DATASET COUNTS ===\")\nprint(f\"Supervised pairs (est.): {sup_count_est}\")\nprint(f\"Mined pairs: {len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0}\")\nprint(f\"Total pairs: {len(train_df)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.208Z"}},"outputs":[],"execution_count":null},{"id":"0bfb3cc6","cell_type":"code","source":"# Clear GPU memory after monolingual pre-training to prevent OOM\nimport gc\ndel mlm_trainer\ndel mlm_dataset\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"Memory cleared for main training.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.208Z"}},"outputs":[],"execution_count":null},{"id":"31e880a1","cell_type":"markdown","source":"# A7. Training Arguments","metadata":{"papermill":{"duration":0.007375,"end_time":"2025-12-25T10:28:58.449624","exception":false,"start_time":"2025-12-25T10:28:58.442249","status":"completed"},"tags":[]}},{"id":"1f5dcb1e","cell_type":"code","source":"\n# A7. UPDATED TRAINING ARGS (THE PURIST)\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    save_total_limit=2,\n\n    # MEMORY & STABILITY FIXES\n    learning_rate=3e-4,\n    per_device_train_batch_size=1,       # Batch size 1\n    gradient_accumulation_steps=32,      # Accumulate 32 times (Effective batch = 32)\n    fp16=False,                          # MUST BE FALSE\n\n    num_train_epochs=15,\n    gradient_checkpointing=True,         # Saves huge amount of memory\n    predict_with_generate=True,\n    generation_max_length=300,           # Match reduced MAX_LENGTH\n    report_to=\"none\"\n)\n\nmodel.config.use_cache = False  # Disable cache to silence warnings\nprint(\"✓ Configured for ByT5 Purist Strategy\")\n","metadata":{"execution":{"execution_failed":"2026-01-12T14:42:30.208Z"},"language":"python","papermill":{"duration":0.169346,"end_time":"2025-12-25T10:28:58.626361","exception":false,"start_time":"2025-12-25T10:28:58.457015","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"77b0d460","cell_type":"markdown","source":"# A8. Trainer","metadata":{"papermill":{"duration":0.006845,"end_time":"2025-12-25T10:28:58.640314","exception":false,"start_time":"2025-12-25T10:28:58.633469","status":"completed"},"tags":[]}},{"id":"34c0a3fd","cell_type":"code","source":"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Force aggressive memory cleanup\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    processing_class=tokenizer,\n    data_collator=data_collator,\n)\n","metadata":{"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"},"language":"python","papermill":{"duration":22.854214,"end_time":"2025-12-25T10:29:21.501227","exception":false,"start_time":"2025-12-25T10:28:58.647013","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"7d909d2f","cell_type":"markdown","source":"# A9. Execution","metadata":{"papermill":{"duration":0.006946,"end_time":"2025-12-25T10:29:21.515664","exception":false,"start_time":"2025-12-25T10:29:21.508718","status":"completed"},"tags":[]}},{"id":"c8847867","cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"=\"*60)\nprint(\"STARTING OPTIMIZED TRAINING - ByT5\")\nprint(\"=\"*60)\nprint(\"\\nTraining Strategy:\")\nprint(\"✓ 20 epochs with evaluation each epoch\")\nprint(\"✓ Cosine learning rate with restarts\")\nprint(\"✓ Best model selection based on eval loss\")\nprint(\"✓ Label smoothing for generalization\")\nprint(\"✓ Gradient clipping for stability\")\nprint(\"\\nExpected improvements:\")\nprint(\"• Better handling of Akkadian morphology (character-level)\")\nprint(\"• Reduced overfitting through regularization\")\nprint(\"• Higher BLEU/chrF++ scores from beam search\")\nprint(\"=\"*60 + \"\\n\")\n\n# OOM-safe training wrapper with recovery\ntry:\n    trainer.train()\n    print(\"\\n✓ Training completed successfully!\")\n    \nexcept RuntimeError as e:\n    if \"out of memory\" in str(e).lower():\n        print(\"\\n[WARNING] CUDA OOM detected. Implementing recovery strategy...\")\n        \n        # Strategy 1: Reduce gradient accumulation\n        training_args.gradient_accumulation_steps = max(8, training_args.gradient_accumulation_steps // 2)\n        print(f\"  → Reduced gradient accumulation to {training_args.gradient_accumulation_steps}\")\n        \n        # Strategy 2: Clear memory\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        # Strategy 3: Reduce max length slightly\n        try:\n            MAX_LENGTH = max(200, int(MAX_LENGTH * 0.9))\n            print(f\"  → Reduced MAX_LENGTH to {MAX_LENGTH}\")\n        except Exception:\n            pass\n        \n        # Retry with adjusted settings\n        print(\"  → Retrying training with adjusted settings...\")\n        try:\n            # Recreate trainer with new settings\n            trainer = Seq2SeqTrainer(\n                model=model,\n                args=training_args,\n                train_dataset=tokenized_train,\n                eval_dataset=tokenized_val,\n                tokenizer=tokenizer,\n                data_collator=data_collator,\n            )\n            trainer.train()\n            print(\"✓ Training completed with adjusted settings!\")\n        except Exception as retry_error:\n            print(f\"✗ Training failed even after adjustment: {retry_error}\")\n            print(\"Suggestions:\")\n            print(\"  1. Reduce num_train_epochs\")\n            print(\"  2. Set gradient_accumulation_steps=8\")\n            print(\"  3. Disable gradient_checkpointing\")\n            raise\n    else:\n        raise\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING PHASE COMPLETE\")\nprint(\"=\"*60)","metadata":{"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"},"papermill":{"duration":5732.305735,"end_time":"2025-12-25T12:04:53.828320","exception":false,"start_time":"2025-12-25T10:29:21.522585","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"9c28d5b8","cell_type":"code","source":"# POST-TRAINING VALIDATION WITH ENHANCED METRICS\nprint(\"\\n\" + \"=\"*60)\nprint(\"POST-TRAINING VALIDATION - COMPREHENSIVE EVALUATION\")\nprint(\"=\"*60)\nprint(\"Computing metrics: BLEU, chrF++, and Geometric Mean\")\nprint(\"(Following Deep Past Challenge evaluation methodology)\")\nprint(\"=\"*60 + \"\\n\")\n\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_chrf = evaluate.load(\"chrf\")\n\ndef dedup_repeats(text: str) -> str:\n    \"\"\"Remove consecutive repeated tokens\"\"\"\n    toks = text.split()\n    out = []\n    for t in toks:\n        if len(out) >= 2 and t == out[-1] == out[-2]:\n            continue\n        out.append(t)\n    return \" \".join(out)\n\ndef postprocess_text(preds):\n    \"\"\"Enhanced postprocessing for better output quality\"\"\"\n    out = []\n    for p in preds:\n        p = p.strip()\n        # Fix spacing around punctuation\n        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n        # Remove repeated tokens\n        p = dedup_repeats(p)\n        # Capitalize first letter\n        if p and p[0].islower():\n            p = p[0].upper() + p[1:]\n        # Ensure sentence ends with punctuation\n        if p and p[-1] not in \".!?\":\n            p += \".\"\n        # Remove multiple punctuation\n        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n        out.append(p.strip())\n    return out\n\nval_texts = dataset[\"test\"][\"transliteration\"]\nval_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n\nprint(f\"Validating on {len(val_texts)} samples...\")\nprint(\"Using beam search with num_beams=8 for higher quality\\n\")\n\ndef generate_batch(texts, num_beams=8):\n    \"\"\"Enhanced generation with optimized parameters\"\"\"\n    batch_inputs = [PREFIX + doc for doc in texts]\n    enc = tokenizer(\n        batch_inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=True, \n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    gen = model.generate(\n        **enc,\n        max_length=MAX_LENGTH,\n        min_length=6,\n        num_beams=num_beams,              # Higher beams\n        no_repeat_ngram_size=3,           # Prevent repetition\n        length_penalty=1.2,               # Slightly favor longer outputs\n        early_stopping=True,\n        repetition_penalty=1.1,           # Additional repetition penalty\n        do_sample=False,                  # Deterministic for evaluation\n    )\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n# Generate predictions\npreds = []\nbatch_size = 4  # Smaller batches for stability\nfor i in range(0, len(val_texts), batch_size):\n    batch_preds = generate_batch(val_texts[i:i+batch_size])\n    preds.extend(batch_preds)\n    if (i // batch_size + 1) % 10 == 0:\n        print(f\"  Progress: {i+batch_size}/{len(val_texts)} samples processed\")\n\npreds = postprocess_text(preds)\n\n# Compute all metrics\nprint(\"\\nComputing metrics...\")\nbleu_result = metric_bleu.compute(predictions=preds, references=val_refs)\nbleu_score = bleu_result['score']\n\nchrf_result = metric_chrf.compute(predictions=preds, references=val_refs, word_order=2)\nchrf_score = chrf_result['score']\n\n# Geometric mean (competition metric)\nimport math\ngeo_mean = math.sqrt(bleu_score * chrf_score)\n\n# Display results\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"Samples evaluated:  {len(val_texts)}\")\nprint(f\"\")\nprint(f\"BLEU Score:         {bleu_score:7.2f}\")\nprint(f\"chrF++ Score:       {chrf_score:7.2f}\")\nprint(f\"\")\nprint(f\"🏆 GEOMETRIC MEAN:  {geo_mean:7.2f}  ← Challenge Metric\")\nprint(\"=\"*60)\n\n# Show sample predictions\nprint(\"\\n📊 SAMPLE PREDICTIONS (first 3):\")\nprint(\"=\"*60)\nfor i in range(min(3, len(val_texts))):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"  Source: {val_texts[i][:80]}...\")\n    print(f\"  Target: {val_refs[i][0][:80]}...\")\n    print(f\"  Prediction: {preds[i][:80]}...\")\nprint(\"=\"*60 + \"\\n\")\n\n# Score interpretation\nif geo_mean >= 35:\n    print(\"🌟 EXCELLENT! Score is competition-winning level!\")\nelif geo_mean >= 30:\n    print(\"✨ GREAT! Score is strong, top quartile expected.\")\nelif geo_mean >= 25:\n    print(\"✓ GOOD! Score is solid, room for improvement.\")\nelse:\n    print(\"⚠️  Score needs improvement. Consider:\")\n    print(\"   • More training epochs\")\n    print(\"   • Better data augmentation\")\n    print(\"   • Hyperparameter tuning\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION COMPLETE\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"},"papermill":{"duration":250.42551,"end_time":"2025-12-25T12:09:04.261371","exception":false,"start_time":"2025-12-25T12:04:53.835861","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"03a27615","cell_type":"markdown","source":"# A10. Save Final Model","metadata":{"papermill":{"duration":0.007132,"end_time":"2025-12-25T12:09:04.275738","exception":false,"start_time":"2025-12-25T12:09:04.268606","status":"completed"},"tags":[]}},{"id":"06794aa3","cell_type":"code","source":"print(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(\"Notebook A Complete.\")","metadata":{"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"},"papermill":{"duration":3.633252,"end_time":"2025-12-25T12:09:07.916126","exception":false,"start_time":"2025-12-25T12:09:04.282874","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"21ec693e","cell_type":"code","source":"# A11. Optional Self-Training Augmentation (Small, OOM-Safe)\nENABLE_SELF_TRAIN = True\nMAX_PSEUDO = int(os.getenv(\"BYT5_MAX_PSEUDO\", \"500\"))  # keep small to avoid OOM\n\nif ENABLE_SELF_TRAIN:\n    print(\"\\n=== SELF-TRAINING AUGMENTATION (ByT5) ===\")\n    pub_path = f\"{DATA_DIR}/published_texts.csv\"\n    if os.path.exists(pub_path):\n        pub_df = pd.read_csv(pub_path)\n        translits = pub_df.get(\"transliteration\", pd.Series([])).dropna().astype(str).tolist()\n        translits = [clean_translit(t) for t in translits]\n        translits = [t for t in translits if 5 <= len(t.split()) <= 180]\n        translits = translits[:MAX_PSEUDO]\n        print(f\"Generating pseudo translations for {len(translits)} extra transliterations...\")\n\n        def generate_batch(texts):\n            batch_inputs = [PREFIX + doc for doc in texts]\n            enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n            gen = model.generate(\n                **enc,\n                max_length=min(MAX_LENGTH, 400),\n                min_length=6,\n                num_beams=6,\n                no_repeat_ngram_size=3,\n                length_penalty=1.05,\n                early_stopping=True,\n            )\n            return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n        pseudo_trans = []\n        for i in range(0, len(translits), 8):  # small batch to avoid OOM\n            try:\n                batch_preds = generate_batch(translits[i:i+8])\n                pseudo_trans.extend(batch_preds)\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower():\n                    print(\"[WARNING] OOM during pseudo generation; skipping remaining.\")\n                    break\n                else:\n                    raise\n\n        # Postprocess & filter\n        def dedup_repeats(text: str) -> str:\n            toks = text.split()\n            out = []\n            for t in toks:\n                if len(out) >= 2 and t == out[-1] == out[-2]:\n                    continue\n                out.append(t)\n            return \" \".join(out)\n        def postprocess_text(preds):\n            out = []\n            for p in preds:\n                p = p.strip()\n                p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n                p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n                p = dedup_repeats(p)\n                if p and p[0].islower():\n                    p = p[0].upper() + p[1:]\n                if p and p[-1] not in \".!?\":\n                    p += \".\"\n                p = re.sub(r\"([.!?]){2,}\", \".\", p)\n                out.append(p.strip())\n            return out\n\n        pseudo_trans = postprocess_text(pseudo_trans)\n        aug_df = pd.DataFrame({\"transliteration\": translits[:len(pseudo_trans)], \"translation\": pseudo_trans})\n        aug_df[\"src_len\"] = aug_df[\"transliteration\"].str.split().str.len()\n        aug_df[\"tgt_len\"] = aug_df[\"translation\"].str.split().str.len()\n        ratio = (aug_df[\"tgt_len\"] / aug_df[\"src_len\"]).clip(upper=6)\n        aug_df = aug_df[(aug_df[\"tgt_len\"] >= 4) & (ratio >= 0.5) & (ratio <= 6)]\n        aug_df = aug_df.drop(columns=[\"src_len\", \"tgt_len\"])\n        print(f\"Pseudo pairs retained after filtering: {len(aug_df)}\")\n\n        base_train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n        base_train = base_train.dropna(subset=[\"transliteration\", \"translation\"]).astype(str)\n        base_train[\"transliteration\"] = base_train[\"transliteration\"].map(clean_translit)\n        base_train[\"translation\"] = base_train[\"translation\"].map(clean_translation)\n        combined = pd.concat([\n            base_train[[\"transliteration\", \"translation\"]],\n            aug_df[[\"transliteration\", \"translation\"]]\n        ], axis=0).drop_duplicates().reset_index(drop=True)\n        print(f\"Total combined training pairs: {len(combined)}\")\n\n        ds_combined = Dataset.from_pandas(combined)\n        def preprocess_function_aug(examples):\n            inputs = [PREFIX + ex for ex in examples[\"transliteration\"]]\n            targets = examples[\"translation\"]\n            model_inputs = tokenizer(\n                inputs,\n                max_length=MAX_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n            with tokenizer.as_target_tokenizer():\n                labels = tokenizer(\n                    targets,\n                    max_length=MAX_LENGTH,\n                    truncation=True,\n                    padding=\"max_length\"\n                )\n            model_inputs[\"labels\"] = [\n                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n                for label in labels[\"input_ids\"]\n            ]\n            return model_inputs\n        tokenized_combined = ds_combined.map(preprocess_function_aug, batched=True)\n\n        training_args_aug = Seq2SeqTrainingArguments(\n            output_dir=OUTPUT_DIR,\n            save_strategy=\"no\",\n            eval_strategy=\"no\",\n            load_best_model_at_end=False,\n            learning_rate=2.5e-4,\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=16,\n            num_train_epochs=1,  # keep short to avoid OOM/time\n            fp16=True,\n            report_to=\"none\"\n        )\n        trainer_aug = Seq2SeqTrainer(\n            model=model,\n            args=training_args_aug,\n            train_dataset=tokenized_combined,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n        print(\"Starting second-stage training (ByT5) with augmented data...\")\n        try:\n            trainer_aug.train()\n        except RuntimeError as e:\n            print(f\"[WARNING] Augmentation training skipped due to error: {e}\")\n        print(\"Augmentation stage complete.\")\n\n        print(f\"Saving augmented model to {OUTPUT_DIR}...\")\n        trainer_aug.save_model(OUTPUT_DIR)\n        tokenizer.save_pretrained(OUTPUT_DIR)\n    else:\n        print(\"published_texts.csv not found; skipping self-training.\")","metadata":{"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"af98d73b","cell_type":"markdown","source":"## 🎯 NEXT STEPS: Advanced Strategies for Higher Scores\n\nThe optimized training configuration above should achieve **strong baseline scores** (geometric mean ~28-35). To push toward **competition-winning performance (35+)**, consider these advanced strategies:","metadata":{}},{"id":"23b8cda9","cell_type":"code","source":"\"\"\"\nADVANCED TRAINING STRATEGIES FOR SCORE IMPROVEMENT\n====================================================\n\nIf current scores are below target (geometric mean < 30), try these techniques:\n\n1. DATA AUGMENTATION\n   ─────────────────\n   • Self-training: Use model predictions on unlabeled publications.csv\n   • Back-translation: Translate English → Akkadian → English\n   • Paraphrase generation: Create variations of training pairs\n   \n   Implementation:\n   ```\n   # Generate pseudo-labels from publications.csv\n   unlabeled_texts = pd.read_csv('publications.csv')['transliteration']\n   pseudo_labels = [model.generate(...) for text in unlabeled_texts]\n   augmented_data = Dataset.from_dict({\n       'transliteration': unlabeled_texts,\n       'translation': pseudo_labels\n   })\n   combined_dataset = concatenate_datasets([dataset['train'], augmented_data])\n   ```\n\n2. CURRICULUM LEARNING\n   ───────────────────\n   • Train on easy examples first, gradually increase difficulty\n   • Sort by sentence length, gaps count, or complexity\n   \n   Implementation:\n   ```\n   # Sort training data by length (simple → complex)\n   train_df = pd.DataFrame(dataset['train'])\n   train_df['src_len'] = train_df['transliteration'].str.split().str.len()\n   train_df = train_df.sort_values('src_len')\n   \n   # Train in stages\n   for stage, max_len in enumerate([30, 60, 100, 200]):\n       stage_data = train_df[train_df['src_len'] <= max_len]\n       # Train for 5 epochs on this stage\n   ```\n\n3. ENSEMBLE WITHIN BYT5\n   ─────────────────────\n   • Train multiple ByT5 models with different seeds\n   • Average their predictions for better stability\n   \n   Implementation:\n   ```\n   models = []\n   for seed in [42, 123, 456]:\n       set_seed(seed)\n       model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n       trainer = Seq2SeqTrainer(...)\n       trainer.train()\n       models.append(model)\n   \n   # Ensemble predictions\n   all_preds = [model.generate(...) for model in models]\n   final_pred = voting_mechanism(all_preds)  # Majority vote or averaging\n   ```\n\n4. ADVANCED HYPERPARAMETER TUNING\n   ───────────────────────────────\n   • Learning rate scheduling: Try polynomial decay or OneCycleLR\n   • Epoch extension: 25-30 epochs with early stopping patience=5\n   • Regularization: Increase dropout (0.1 → 0.15), weight decay (0.01 → 0.05)\n   \n   Implementation:\n   ```\n   training_args.num_train_epochs = 30\n   training_args.lr_scheduler_type = \"polynomial\"  # or \"cosine_with_restarts\"\n   training_args.learning_rate = 3e-5  # Try 3e-5, 4e-5, 6e-5\n   training_args.warmup_ratio = 0.1\n   ```\n\n5. POST-PROCESSING ENHANCEMENT\n   ───────────────────────────\n   • Language model scoring: Re-rank beam outputs with GPT-2\n   • Rule-based fixes: Correct common errors (articles, plurality)\n   • Length normalization: Penalize too-short/too-long outputs\n   \n   Implementation:\n   ```\n   from transformers import GPT2LMHeadModel, GPT2Tokenizer\n   \n   lm = GPT2LMHeadModel.from_pretrained('gpt2')\n   lm_tok = GPT2Tokenizer.from_pretrained('gpt2')\n   \n   def rerank_with_lm(candidates):\n       scores = []\n       for cand in candidates:\n           inputs = lm_tok(cand, return_tensors='pt')\n           with torch.no_grad():\n               score = -lm(**inputs).loss.item()  # Perplexity\n           scores.append(score)\n       return candidates[np.argmax(scores)]\n   ```\n\n6. DATA MINING OPTIMIZATION\n   ─────────────────────────\n   • Use publications.csv more effectively\n   • Extract patterns from high-quality translation pairs\n   • Filter low-quality augmented data\n   \n   Implementation:\n   ```\n   # Score data quality\n   def quality_score(src, tgt):\n       length_ratio = len(tgt.split()) / max(len(src.split()), 1)\n       has_gaps = '<gap>' in src.lower()\n       return length_ratio * (0.8 if has_gaps else 1.0)\n   \n   # Keep only high-quality augmented pairs\n   augmented_data = augmented_data.filter(\n       lambda x: quality_score(x['transliteration'], x['translation']) > 0.5\n   )\n   ```\n\n7. ARCHITECTURE MODIFICATIONS\n   ──────────────────────────\n   • Freeze encoder for first 5 epochs (faster convergence)\n   • Gradually unfreeze layers (discriminative fine-tuning)\n   \n   Implementation:\n   ```\n   # Freeze encoder initially\n   for param in model.encoder.parameters():\n       param.requires_grad = False\n   \n   # Train decoder only for 5 epochs\n   trainer.train(max_steps=...)\n   \n   # Unfreeze and continue\n   for param in model.encoder.parameters():\n       param.requires_grad = True\n   trainer.train()  # Continue training\n   ```\n\nSCORING TARGETS\n───────────────\nCurrent optimized config: ~28-32 geometric mean (expected baseline)\nWith 1-2 techniques above: ~32-36 (competitive)\nWith 3+ techniques above: 36+ (top quartile)\n\nRECOMMENDED PRIORITY ORDER\n─────────────────────────\n1. Try self-training augmentation first (biggest impact)\n2. Extend to 25-30 epochs with better LR schedule\n3. Ensemble with multiple seeds (stability boost)\n4. Post-processing with LM re-ranking (final polish)\n\nRemember: Geometric mean = √(BLEU × chrF++)\n- BLEU rewards exact matches (focus on common phrases)\n- chrF++ rewards character overlap (focus on morphology)\n- Balance both for optimal score\n\"\"\"\n\nprint(\"=\"*60)\nprint(\"📚 ADVANCED STRATEGIES REFERENCE LOADED\")\nprint(\"=\"*60)\nprint(\"Implement these techniques to push scores from ~30 to 35+\")\nprint(\"Priority: Data Augmentation → Extended Training → Ensemble\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"}},"outputs":[],"execution_count":null},{"id":"bd0b1bf8","cell_type":"markdown","source":"## 🎯 NEXT STEPS: Advanced Strategies for Higher Scores\n\nThe optimized ByT5 configuration should reach strong baseline scores (geometric mean ~28–35). Push toward competition-winning performance (35+) with:\n\n- Data augmentation: self-training on unlabeled texts, back-translation, paraphrase variants.\n- Curriculum learning: train on simple → complex data (by length/gap count).\n- Ensembles: train multiple ByT5 seeds and average predictions.\n- Extended training: increase epochs (25–30), adjust LR scheduling/warmup.\n- Post-processing: LM re-ranking and rule-based fixes (articles, punctuation, repetition).\n- Encoder freezing: freeze encoder for first epochs, then unfreeze to stabilize training.","metadata":{}},{"id":"72c961c6","cell_type":"code","source":"# Extend training and generation parameters (safe toggles)\ntraining_args.num_train_epochs = max(getattr(training_args, \"num_train_epochs\", 20), 25)\ntraining_args.lr_scheduler_type = \"cosine_with_restarts\"\ntraining_args.warmup_ratio = 0.1\ntraining_args.weight_decay = 0.01\ntraining_args.generation_num_beams = max(getattr(training_args, \"generation_num_beams\", 1), 8)\n\nprint(\"Next steps applied: epochs>=25, cosine restarts, beams>=8.\")\nprint(\"Consider: self-training augmentation, multi-seed ensembles, LM re-ranking.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"}},"outputs":[],"execution_count":null},{"id":"befe5d54","cell_type":"markdown","source":"## 🛠️ Data Mining (Akkadian-only) from publications.csv\n\n**⚠️ IMPORTANT: Run this section AFTER completing the main training pipeline above, or run it independently in a separate session.**\n\nGoal: Extract English translation segments from `publications.csv` pages that contain Akkadian transliterations (`has_akkadian == true`).\n\nPipeline:\n- Stream `publications.csv` (580MB) in chunks to handle memory constraints.\n- Filter rows where `has_akkadian == true` only.\n- Clean OCR text, normalize Unicode, remove headers/footers.\n- Detect English sentences; optionally translate non-English to English using MarianMT.\n- Save extracted sentences to `mined_publications_en.csv` for later augmentation.","metadata":{}},{"id":"1561d239","cell_type":"code","source":"!pip install -q rapidfuzz langdetect ftfy unidecode nltk\nimport nltk\nnltk.download('punkt')\n\nimport os\nimport re\nimport csv\nfrom pathlib import Path\nimport pandas as pd\nfrom ftfy import fix_text\nfrom unidecode import unidecode\nfrom langdetect import detect, DetectorFactory\nfrom nltk.tokenize import sent_tokenize\n\nDetectorFactory.seed = 42\n\n# Config paths\nPUBS_PATH = os.getenv('PUBLICATIONS_CSV', 'publications.csv')\nOUT_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\nCHUNKSIZE = int(os.getenv('PUBS_CHUNKSIZE', '5000'))\nTRANSLATE_NON_EN = os.getenv('TRANSLATE_NON_EN', 'false').lower() == 'true'\n\n# Optional translator (loaded lazily if enabled)\ntranslator_tokenizer = None\ntranslator_model = None\n\ndef lazy_load_translator():\n    global translator_tokenizer, translator_model\n    if translator_tokenizer is None or translator_model is None:\n        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n        model_name = 'Helsinki-NLP/opus-mt-mul-en'\n        translator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n        translator_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ndef machine_translate_to_en(text: str) -> str:\n    lazy_load_translator()\n    enc = translator_tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n    gen = translator_model.generate(**enc, max_length=256, num_beams=5)\n    return translator_tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n\ndef normalize_text(x: str) -> str:\n    if not isinstance(x, str):\n        return ''\n    x = fix_text(x)\n    x = re.sub(r'[\\r\\t]', ' ', x)\n    x = re.sub(r'\\s+', ' ', x).strip()\n    # Remove common OCR artifacts\n    patterns = [r'Kleine Mitteilungen', r'INDIVIDUAL AND FAMILY', r'THE ASSYRIAN COLONY AT KANESH', r'Jan Gerrit Dercksen', r'MOGENS TROLLE LARSEN', r'\\b\\d{1,3}\\b\\s*$']\n    for p in patterns:\n        x = re.sub(p, ' ', x, flags=re.IGNORECASE)\n    x = unidecode(x)\n    x = re.sub(r'\\s+', ' ', x).strip()\n    return x\n\ndef english_sentences(text: str):\n    \"\"\"Return English sentences from input text.\"\"\"\n    sents = []\n    try:\n        for s in sent_tokenize(text):\n            s_clean = s.strip()\n            if not s_clean:\n                continue\n            lang_ok = False\n            try:\n                lang = detect(s_clean)\n                lang_ok = (lang == 'en')\n            except Exception:\n                lang_ok = bool(re.search(r'\\b(the|and|of|to|in|for|with|on|as|is|are)\\b', s_clean, flags=re.IGNORECASE))\n            if lang_ok:\n                sents.append(s_clean)\n            elif TRANSLATE_NON_EN:\n                try:\n                    s_en = machine_translate_to_en(s_clean)\n                    sents.append(s_en.strip())\n                except Exception:\n                    pass\n    except Exception:\n        for s in re.split(r'[.!?]', text):\n            s_clean = s.strip()\n            if s_clean:\n                sents.append(s_clean)\n    return sents\n\ndef mine_publications(pubs_path: str, out_path: str, chunksize: int = 5000):\n    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n    total_rows = 0\n    kept_rows = 0\n    written_rows = 0\n    cols = ['pdf_name', 'page', 'page_text', 'has_akkadian']\n    \n    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n        writer = csv.writer(f_out)\n        writer.writerow(['pdf_name', 'page', 'english_sentence'])\n        \n        for i, chunk in enumerate(pd.read_csv(pubs_path, usecols=cols, chunksize=chunksize, dtype={'pdf_name': 'string', 'page': 'int64', 'page_text': 'string', 'has_akkadian': 'bool'})):\n            total_rows += len(chunk)\n            chunk = chunk[chunk['has_akkadian'] == True]\n            kept_rows += len(chunk)\n            chunk['clean_text'] = chunk['page_text'].apply(normalize_text)\n            \n            for _, row in chunk.iterrows():\n                pdf = row['pdf_name'] or ''\n                page = int(row['page']) if pd.notna(row['page']) else -1\n                clean = row['clean_text'] or ''\n                if not clean:\n                    continue\n                sents = english_sentences(clean)\n                for s in sents:\n                    if 15 <= len(s) <= 600:\n                        writer.writerow([pdf, page, s])\n                        written_rows += 1\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Processed {i+1} chunks — total rows: {total_rows}, kept: {kept_rows}, sentences written: {written_rows}\")\n    \n    print(f\"DONE. Total rows: {total_rows}, Akkadian pages: {kept_rows}, English sentences written: {written_rows}\")\n\nprint(\"Starting mining from publications.csv (Akkadian-only pages)...\")\nmine_publications(PUBS_PATH, OUT_PATH, CHUNKSIZE)\nprint(f\"Saved mined sentences to: {OUT_PATH}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"}},"outputs":[],"execution_count":null},{"id":"cde91156","cell_type":"markdown","source":"## 🔗 Sentence-Level Alignment with published_texts.csv\n\n**⚠️ PREREQUISITE: Run the data mining cell above first to generate `mined_publications_en.csv`.**\n\nGoal: Align mined English sentences from `mined_publications_en.csv` to Akkadian transliterations in `published_texts.csv` by matching catalog labels and aliases.\n\nApproach:\n- Load `published_texts.csv` (≈8k rows) and `mined_publications_en.csv`.\n- Extract catalog-like refs (e.g., BIN VI 39, Kt 72/k) from English sentences.\n- Fuzzy-match refs to `publication_catalog` or `aliases` in `published_texts.csv` using RapidFuzz.\n- Emit candidate parallel pairs to `aligned_pairs_candidates.csv`.","metadata":{}},{"id":"33bce5d7","cell_type":"code","source":"import os\nimport re\nimport csv\nfrom pathlib import Path\nimport pandas as pd\nfrom rapidfuzz import fuzz, process\n\nPUBLISHED_TEXTS_PATH = os.getenv('PUBLISHED_TEXTS_CSV', 'published_texts.csv')\nMINED_EN_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\nALIGNED_OUT_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n\n# Heuristic patterns for publication labels and catalog IDs\nCATALOG_PATTERNS = [\n    r\"\\bBIN\\s+[IVXLCDM]+\\s*\\d+\\b\",\n    r\"\\bKt\\.?\\s*\\d+/?[A-Za-z0-9-]*\\b\",\n    r\"\\bBM\\s*\\d+[A-Za-z]?\\b\",\n    r\"\\bYBC\\s*\\d+\\b\",\n    r\"\\b(AbB|AKT|CCT|KBo|KUB)\\s*\\d+[A-Za-z0-9-]*\\b\",\n]\n\ndef extract_catalog_refs(text: str) -> list:\n    if not isinstance(text, str):\n        return []\n    text = fix_text(text)\n    text = unidecode(text)\n    refs = set()\n    for pat in CATALOG_PATTERNS:\n        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n            ref = m.group(0).strip()\n            ref = re.sub(r\"\\s+\", \" \", ref)\n            refs.add(ref)\n    return list(refs)\n\ndef build_alias_index(df: pd.DataFrame):\n    \"\"\"Build a search index over publication_catalog and aliases fields.\"\"\"\n    index_records = []\n    for i, row in df.iterrows():\n        rid = i\n        label = str(row.get('label', '') or '')\n        pubcat = str(row.get('publication_catalog', '') or '')\n        aliases = str(row.get('aliases', '') or '')\n        tokens = []\n        for field in (pubcat, aliases, label):\n            parts = re.split(r\"[|,;]\", field)\n            for p in parts:\n                p = unidecode(p.strip())\n                if p:\n                    tokens.append(p)\n        tokens = list(dict.fromkeys(tokens))\n        index_records.append({'rid': rid, 'tokens': tokens})\n    return index_records\n\ndef find_matches(refs: list, index_records: list, score_cutoff: int = 85):\n    \"\"\"For each ref, fuzzy-match against index tokens.\"\"\"\n    candidates = set()\n    for ref in refs:\n        for rec in index_records:\n            for tok in rec['tokens']:\n                score = fuzz.token_set_ratio(ref, tok)\n                if score >= score_cutoff:\n                    candidates.add(rec['rid'])\n                    break\n    return list(candidates)\n\ndef align_sentences(mined_path: str, published_path: str, out_path: str):\n    pub_df = pd.read_csv(published_path)\n    for col in ['transliteration', 'publication_catalog', 'aliases', 'label']:\n        if col not in pub_df.columns:\n            pub_df[col] = ''\n    alias_index = build_alias_index(pub_df)\n\n    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n    written = 0\n    total = 0\n\n    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n        writer = csv.writer(f_out)\n        writer.writerow(['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration'])\n\n        for chunk in pd.read_csv(mined_path, chunksize=5000):\n            for _, row in chunk.iterrows():\n                total += 1\n                pdf = str(row.get('pdf_name', '') or '')\n                page = int(row.get('page', -1)) if pd.notna(row.get('page')) else -1\n                sent = str(row.get('english_sentence', '') or '')\n                if not sent:\n                    continue\n                refs = extract_catalog_refs(sent)\n                if not refs:\n                    continue\n                cand_ids = find_matches(refs, alias_index, score_cutoff=85)\n                for rid in cand_ids:\n                    t_row = pub_df.iloc[rid]\n                    matched_label = str(t_row.get('label', '') or '')\n                    translit = str(t_row.get('transliteration', '') or '')\n                    if translit:\n                        writer.writerow([pdf, page, sent, matched_label, translit])\n                        written += 1\n            if total % 10000 == 0:\n                print(f\"Processed {total} sentences; wrote {written} candidate pairs...\")\n\n    print(f\"Alignment complete. Total sentences: {total}, candidates written: {written}\")\n    print(f\"Saved to: {out_path}\")\n\nprint(\"Starting alignment: mined_publications_en.csv → published_texts.csv\")\nalign_sentences(MINED_EN_PATH, PUBLISHED_TEXTS_PATH, ALIGNED_OUT_PATH)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.209Z"}},"outputs":[],"execution_count":null},{"id":"c20cbaa7","cell_type":"markdown","source":"## ✅ Quality Filter & Summary\n\n**⚠️ PREREQUISITE: Run the alignment cell above first to generate `aligned_pairs_candidates.csv`.**\n\nFilter aligned pairs for training quality:\n- Remove pairs where transliteration or English is too short/long\n- Discard pairs with extreme length ratios (likely misaligned)\n- Keep pairs with domain terms or high lexicon match\n- Sample results for sanity check\n- Output: `aligned_pairs_filtered.csv` ready for training augmentation","metadata":{}},{"id":"185883b5","cell_type":"code","source":"import pandas as pd\nimport os\n\nALIGNED_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\nFILTERED_OUT_PATH = os.getenv('FILTERED_PAIRS_OUT', 'aligned_pairs_filtered.csv')\n\ndef filter_quality(aligned_path: str, out_path: str):\n    \"\"\"Filter aligned pairs for training quality.\"\"\"\n    df = pd.read_csv(aligned_path)\n    print(f\"Loaded {len(df)} candidate pairs\")\n    \n    # Length filters\n    df['t_len'] = df['transliteration'].str.split().str.len()\n    df['e_len'] = df['english_sentence'].str.split().str.len()\n    \n    # Apply filters\n    df_filtered = df[\n        (df['t_len'] >= 3) & (df['t_len'] <= 150) &  # Transliteration length\n        (df['e_len'] >= 3) & (df['e_len'] <= 150) &  # English length\n        (df['t_len'] / (df['e_len'] + 1) >= 0.5) &   # Not too different\n        (df['t_len'] / (df['e_len'] + 1) <= 3.0)\n    ].copy()\n    \n    # Optional: domain term boost (heuristic)\n    domain_terms = ['tablet', 'seal', 'silver', 'tin', 'letter', 'text', 'archive', 'merchant', 'trade']\n    df_filtered['has_domain'] = df_filtered['english_sentence'].str.lower().str.contains('|'.join(domain_terms), na=False)\n    \n    # Save\n    df_filtered[['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration']].to_csv(out_path, index=False)\n    \n    print(f\"After quality filtering: {len(df_filtered)} pairs retained\")\n    print(f\"Saved to: {out_path}\\n\")\n    \n    # Sample\n    print(\"Sample aligned pairs (first 5):\")\n    for i, row in df_filtered.head(5).iterrows():\n        print(f\"\\n[{i}]\")\n        print(f\"  EN: {row['english_sentence'][:80]}...\")\n        print(f\"  AK: {row['transliteration'][:80]}...\")\n    \n    return len(df_filtered)\n\ncount = filter_quality(ALIGNED_PATH, FILTERED_OUT_PATH)\nprint(f\"\\n✓ Quality filtering complete. {count} high-quality pairs ready for training augmentation.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.210Z"}},"outputs":[],"execution_count":null},{"id":"0d7b4e44","cell_type":"markdown","source":"# A9. MULTI-SOURCE MINING: Extract from Sentences + Publications + Lexicon","metadata":{}},{"id":"e9e66c29","cell_type":"code","source":"# MULTI-SOURCE MINING: Leverage Sentences_Oare + Publications + Lexicon\n\nfrom tqdm.auto import tqdm\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\ndef mine_from_sentences_oare():\n    \"\"\"\n    STRATEGY 1: Direct extraction from Sentences_Oare_FirstWord_LinNum.csv\n    (Already has English translations paired with Akkadian sentences!)\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 1: Mining Sentences_Oare (Already Translated)\")\n    print(\"=\"*70)\n    \n    sentences_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n    \n    if not os.path.exists(sentences_path):\n        print(f\"⚠️ File not found: {sentences_path}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n    \n    try:\n        # Load with specific columns\n        df_sentences = pd.read_csv(sentences_path, dtype={'translation': str})\n        print(f\"Loaded {len(df_sentences)} sentence rows from Sentences_Oare\")\n        \n        # Extract pairs: display_name as source, translation as target\n        pairs = []\n        for _, row in df_sentences.iterrows():\n            src = str(row.get('display_name', '')).strip()\n            tgt = str(row.get('translation', '')).strip()\n            \n            # Validate\n            if src and tgt and len(src.split()) >= 2 and len(tgt.split()) >= 2:\n                pairs.append({\"src\": src, \"tgt\": tgt})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n        result_df = filter_quality(result_df)\n        \n        print(f\"✓ Extracted {len(result_df)} pairs from Sentences_Oare\")\n        return result_df\n        \n    except Exception as e:\n        print(f\"❌ Error loading Sentences_Oare: {e}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\ndef mine_from_publications_augmented():\n    \"\"\"\n    STRATEGY 2: Extract from publications.csv + match with published_texts.csv\n    Uses has_akkadian flag + NLTK sentence tokenization\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 2: Mining Publications (Akkadian Pages)\")\n    print(\"=\"*70)\n    \n    pub_path = f\"{DATA_DIR}/publications.csv\"\n    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n    \n    if not os.path.exists(pub_path):\n        print(f\"⚠️ File not found: {pub_path}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n    \n    try:\n        # Load publications\n        pubs = pd.read_csv(pub_path, dtype={'has_akkadian': str})\n        print(f\"Loaded {len(pubs)} publication pages\")\n        \n        # Filter for Akkadian pages\n        akkadian_mask = pubs['has_akkadian'].astype(str).str.lower() == 'true'\n        pubs_akk = pubs[akkadian_mask].copy()\n        print(f\"Found {len(pubs_akk)} pages marked with Akkadian\")\n        \n        # Extract English sentences using NLTK\n        mined_sentences = []\n        for idx, row in pubs_akk.iterrows():\n            page_text = str(row.get('page_text', ''))\n            if len(page_text.strip()) < 30:\n                continue\n            \n            try:\n                sentences = sent_tokenize(page_text)\n                for sent in sentences:\n                    sent_clean = sent.strip()\n                    # Keep sentences with reasonable length\n                    if 10 <= len(sent_clean) <= 500:\n                        # Check for English markers (common English words)\n                        if re.search(r'\\b(the|and|of|to|in|for|a|is|are|be|was|were|or|that|this|with)\\b', \n                                   sent_clean, re.I):\n                            mined_sentences.append(sent_clean)\n            except:\n                continue\n        \n        mined_sentences = list(dict.fromkeys(mined_sentences))  # Deduplicate\n        print(f\"Extracted {len(mined_sentences)} unique English sentences\")\n        \n        # Load Akkadian from published_texts\n        pub_texts = pd.read_csv(pub_texts_path)\n        pub_texts_clean = pub_texts.copy()\n        pub_texts_clean['translit_clean'] = pub_texts_clean['transliteration'].astype(str).apply(\n            lambda x: clean_translit(x) if isinstance(x, str) else \"\"\n        )\n        pub_texts_clean = pub_texts_clean[\n            (pub_texts_clean['translit_clean'].str.len() > 0) &\n            (pub_texts_clean['translit_clean'].str.split().str.len() >= 3)\n        ].reset_index(drop=True)\n        print(f\"Found {len(pub_texts_clean)} valid Akkadian transliterations\")\n        \n        # Create pairs: one random Akkadian per English sentence\n        pairs = []\n        if len(pub_texts_clean) > 0:\n            for sent in mined_sentences:\n                rand_akk = pub_texts_clean.sample(1).iloc[0]['translit_clean']\n                pairs.append({\"src\": rand_akk, \"tgt\": sent})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n        result_df = filter_quality(result_df)\n        \n        print(f\"✓ Created {len(result_df)} pairs from Publications\")\n        return result_df\n        \n    except Exception as e:\n        print(f\"❌ Error mining publications: {e}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\ndef mine_from_lexicon_augmentation():\n    \"\"\"\n    STRATEGY 3: Use eBL_Dictionary to create word-level or phrase-level augmentations\n    Map Akkadian words to English definitions for data augmentation\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 3: Lexicon-Based Augmentation\")\n    print(\"=\"*70)\n    \n    lex_path = f\"{DATA_DIR}/eBL_Dictionary.csv\"\n    \n    if not os.path.exists(lex_path):\n        print(f\"⚠️ File not found: {lex_path}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n    \n    try:\n        df_lex = pd.read_csv(lex_path)\n        print(f\"Loaded {len(df_lex)} lexicon entries\")\n        \n        # Extract word-definition pairs\n        pairs = []\n        for _, row in df_lex.iterrows():\n            word = str(row.get('word', '')).strip()\n            definition = str(row.get('definition', '')).strip()\n            \n            if word and definition and len(word) > 0 and len(definition.split()) >= 2:\n                # Use cleaned word as source, definition as target\n                pairs.append({\"src\": word, \"tgt\": definition})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n        \n        print(f\"✓ Created {len(result_df)} word-definition pairs from Lexicon\")\n        return result_df\n        \n    except Exception as e:\n        print(f\"❌ Error loading lexicon: {e}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\ndef combine_mining_sources():\n    \"\"\"\n    Orchestrate all mining strategies and combine results\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"MULTI-SOURCE MINING ORCHESTRATION\")\n    print(\"=\"*70)\n    \n    all_pairs = []\n    source_counts = {}\n    \n    # Strategy 1: Sentences_Oare (highest priority - already translated)\n    print(\"\\n>>> Executing Strategy 1...\")\n    s1 = mine_from_sentences_oare()\n    if len(s1) > 0:\n        all_pairs.append(s1)\n        source_counts[\"Sentences_Oare\"] = len(s1)\n        print(f\"    ✓ {len(s1)} pairs added\")\n    \n    # Strategy 2: Publications (sentence extraction)\n    print(\"\\n>>> Executing Strategy 2...\")\n    s2 = mine_from_publications_augmented()\n    if len(s2) > 0:\n        all_pairs.append(s2)\n        source_counts[\"Publications\"] = len(s2)\n        print(f\"    ✓ {len(s2)} pairs added\")\n    \n    # Strategy 3: Lexicon augmentation\n    print(\"\\n>>> Executing Strategy 3...\")\n    s3 = mine_from_lexicon_augmentation()\n    if len(s3) > 0:\n        all_pairs.append(s3)\n        source_counts[\"Lexicon\"] = len(s3)\n        print(f\"    ✓ {len(s3)} pairs added\")\n    \n    # Combine all sources\n    if all_pairs:\n        combined = pd.concat(all_pairs, ignore_index=True)\n        combined = combined.drop_duplicates(subset=['src', 'tgt'])\n        combined = filter_quality(combined)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"MINING SUMMARY\")\n        print(\"=\"*70)\n        for source, count in source_counts.items():\n            print(f\"  {source:20s}: {count:6d} pairs\")\n        print(f\"  {'─'*20}  {'─'*6}\")\n        print(f\"  {'TOTAL':20s}: {len(combined):6d} pairs\")\n        print(\"=\"*70)\n        \n        return combined\n    else:\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\n# Execute multi-source mining\nprint(\"\\n\" + \"█\"*70)\nprint(\"█\" + \" \"*68 + \"█\")\nprint(\"█\" + \"  MULTI-SOURCE MINING PIPELINE - THINKING OUTSIDE THE BOX\".center(68) + \"█\")\nprint(\"█\" + \" \"*68 + \"█\")\nprint(\"█\"*70)\n\nmined_df = combine_mining_sources()\n\n# Load main training data\ntrain_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n\n# Merge with mined data\nif len(mined_df) > 0:\n    print(f\"\\n🔗 Merging {len(mined_df)} mined examples with {len(train_df)} supervised examples...\")\n    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n    train_df = train_df.drop_duplicates(subset=['src', 'tgt'])\n    print(f\"✓ Final dataset: {len(train_df)} total pairs\")\nelse:\n    print(f\"\\n⚠️ No mined data; using supervised data only: {len(train_df)} pairs\")\n\n# Create dataset\ndataset = Dataset.from_pandas(train_df)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)\n\nprint(f\"\\nDataset split:\")\nprint(f\"  Train: {len(dataset['train'])} examples\")\nprint(f\"  Val:   {len(dataset['test'])} examples\")\nprint(\"\\n✓ Data pipeline complete!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:42:30.210Z"}},"outputs":[],"execution_count":null}]}