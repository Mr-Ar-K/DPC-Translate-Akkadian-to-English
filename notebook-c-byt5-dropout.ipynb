{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43512d74",
   "metadata": {
    "papermill": {
     "duration": 0.006753,
     "end_time": "2025-12-25T11:08:30.972947",
     "exception": false,
     "start_time": "2025-12-25T11:08:30.966194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3dbc9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:24:52.112765Z",
     "iopub.status.busy": "2026-01-08T07:24:52.112524Z",
     "iopub.status.idle": "2026-01-08T07:24:55.525780Z",
     "shell.execute_reply": "2026-01-08T07:24:55.524913Z",
     "shell.execute_reply.started": "2026-01-08T07:24:52.112740Z"
    },
    "papermill": {
     "duration": 3.868853,
     "end_time": "2025-12-25T11:08:39.101210",
     "exception": false,
     "start_time": "2025-12-25T11:08:35.232357",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97b0a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:24:55.527482Z",
     "iopub.status.busy": "2026-01-08T07:24:55.527135Z",
     "iopub.status.idle": "2026-01-08T07:24:55.535821Z",
     "shell.execute_reply": "2026-01-08T07:24:55.535204Z",
     "shell.execute_reply.started": "2026-01-08T07:24:55.527440Z"
    },
    "papermill": {
     "duration": 30.839785,
     "end_time": "2025-12-25T11:09:09.947308",
     "exception": false,
     "start_time": "2025-12-25T11:08:39.107523",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# === CONFIGURATION: THE SPECIALIST ===\n",
    "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "OUTPUT_DIR = \"/kaggle/working/byt5-specialist-saved\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "PREFIX = \"translate Akkadian to English: \"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# CRITICAL: Change Seed to force diversity\n",
    "set_seed(999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54d7d3",
   "metadata": {
    "papermill": {
     "duration": 0.005995,
     "end_time": "2025-12-25T11:09:09.959629",
     "exception": false,
     "start_time": "2025-12-25T11:09:09.953634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C2.Data Loading & Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4321815",
   "metadata": {},
   "source": [
    "# C1.5. DATA PREPARATION GUIDE: Handling Akkadian Formatting Issues\n",
    "\n",
    "## Problem: \"Garbage In, Garbage Out\"\n",
    "Akkadian texts contain complex formatting that can break ML pipelines if not handled properly.\n",
    "\n",
    "## Formatting Issues to Handle\n",
    "\n",
    "### 1. Scribal Notations (Remove)\n",
    "- `!` - Certain reading (remove)\n",
    "- `?` - Questionable reading (remove)\n",
    "- `/` - Line divider (remove)\n",
    "- `:` or `.` - Word divider (remove)\n",
    "- `< >` - Scribal insertions (keep content, remove brackets)\n",
    "- `( )` - Comments/erasures (remove entirely)\n",
    "- `Àπ À∫` - Half brackets for partially broken signs (remove)\n",
    "- `[ ]` - Clearly broken signs (keep content, remove brackets)\n",
    "- `<< >>` - Errant signs (remove entirely)\n",
    "\n",
    "### 2. Gaps & Lacunae (Standardize)\n",
    "- `[x]` ‚Üí `<gap>`\n",
    "- `x` ‚Üí `<gap>`\n",
    "- `xx` ‚Üí `<gap>`\n",
    "- `‚Ä¶` ‚Üí `<big_gap>`\n",
    "- `‚Ä¶‚Ä¶` ‚Üí `<big_gap>`\n",
    "- `[... ...]` ‚Üí `<big_gap>`\n",
    "- Multiple `.3` or `...` sequences ‚Üí `<big_gap>`\n",
    "\n",
    "### 3. Determinatives (Keep content, remove brackets)\n",
    "- `{d}` - Deity (remove brackets)\n",
    "- `{ki}` - Earth/location (remove brackets)\n",
    "- `{lu‚ÇÇ}` - Person (remove brackets)\n",
    "- `{e‚ÇÇ}` - Building (remove brackets)\n",
    "- And 10+ others...\n",
    "\n",
    "### 4. Subscripts & Superscripts (Normalize)\n",
    "- `a‚ÇÇ` ‚Üí `a2`, `a‚ÇÉ` ‚Üí `a3`, etc.\n",
    "- `il‚ÇÖ` ‚Üí `il5`, etc.\n",
    "- Works with Unicode characters (U+2080-U+2089)\n",
    "\n",
    "### 5. Special Characters (Handle as-is or normalize)\n",
    "- `≈°` (U+0161), `≈†` (U+0160)\n",
    "- `·π£` (U+1E63), `·π¢` (U+1E62)\n",
    "- `·π≠` (U+1E6D), `·π¨` (U+1E6C)\n",
    "- `·∏´` (U+1E2B), `·∏™` (U+1E2A)\n",
    "- ` æ` (U+02BE) - Akkadian letter marker\n",
    "\n",
    "### 6. Capitalization Rules (Preserve)\n",
    "- First letter capital = Proper noun (personal/place name)\n",
    "- ALL CAPS = Sumerian logogram (preserve for domain knowledge)\n",
    "\n",
    "## Processing Order\n",
    "1. Normalize subscripts FIRST (‚ÇÄ-‚Çâ ‚Üí 0-9)\n",
    "2. Handle gaps (complex patterns first, then simple)\n",
    "3. Remove scribal notations\n",
    "4. Extract content from bracketed structures\n",
    "5. Clean whitespace\n",
    "6. Validate output (length checks, character validation)\n",
    "\n",
    "## Data Validation Checks\n",
    "‚úì No empty strings after cleaning\n",
    "‚úì Source length >= 3 words\n",
    "‚úì Target length >= 3 words\n",
    "‚úì Length ratio between 0.2 and 5.0\n",
    "‚úì No duplicate pairs\n",
    "‚úì All special characters properly handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc0bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:24:55.538168Z",
     "iopub.status.busy": "2026-01-08T07:24:55.537708Z",
     "iopub.status.idle": "2026-01-08T07:24:56.044757Z",
     "shell.execute_reply": "2026-01-08T07:24:56.043971Z",
     "shell.execute_reply.started": "2026-01-08T07:24:55.538146Z"
    },
    "papermill": {
     "duration": 0.450376,
     "end_time": "2025-12-25T11:09:10.416010",
     "exception": false,
     "start_time": "2025-12-25T11:09:09.965634",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw documents: 1561\n",
      "Aligned training examples (pre-filter): 1561\n",
      "Aligned training examples (post-filter): 1529\n"
     ]
    }
   ],
   "source": [
    "SUBSCRIPT_TRANS = str.maketrans({\"‚ÇÄ\": \"0\", \"‚ÇÅ\": \"1\", \"‚ÇÇ\": \"2\", \"‚ÇÉ\": \"3\", \"‚ÇÑ\": \"4\", \"‚ÇÖ\": \"5\", \"‚ÇÜ\": \"6\", \"‚Çá\": \"7\", \"‚Çà\": \"8\", \"‚Çâ\": \"9\", \"‚Çì\": \"x\"})\n",
    "\n",
    "\n",
    "def normalize_subscripts(text: str) -> str:\n",
    "\n",
    "    return text.translate(SUBSCRIPT_TRANS)\n",
    "\n",
    "\n",
    "\n",
    "def replace_gaps(text):\n",
    "\n",
    "    \"\"\"Replace various gap notations with standardized tokens\"\"\"\n",
    "\n",
    "    if pd.isna(text): \n",
    "\n",
    "        return text\n",
    "\n",
    "    \n",
    "\n",
    "    # Complex gap patterns (order matters)\n",
    "\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "\n",
    "\n",
    "    # Simple gap patterns\n",
    "\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "\n",
    "    text = re.sub(r'‚Ä¶‚Ä¶', '<big_gap>', text)\n",
    "\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "\n",
    "    text = re.sub(r'‚Ä¶', '<big_gap>', text)\n",
    "\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def replace_gaps_back(text):\n",
    "\n",
    "    \"\"\"Convert standardized gap tokens back to original format\"\"\"\n",
    "\n",
    "    if pd.isna(text):  \n",
    "\n",
    "        return text\n",
    "\n",
    "    \n",
    "\n",
    "    text = re.sub(r'<gap>', 'x', text)\n",
    "\n",
    "    text = re.sub(r'<big_gap>', '...', text)\n",
    "\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def clean_translit(text):\n",
    "\n",
    "    \"\"\"Normalize transliteration by stripping scribal marks and gaps.\"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    text = normalize_subscripts(text)\n",
    "\n",
    "    # Apply gap replacement first\n",
    "\n",
    "    text = replace_gaps(text)\n",
    "\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"<<[^>]*>>\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"[ÀπÀ∫]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)\n",
    "\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n",
    "\n",
    "    text = re.sub(r\"[!?/:¬∑]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def clean_translation(text):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace(\"‚Ä¶\", \" \")\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def filter_quality(df):\n",
    "\n",
    "    df[\"src_len\"] = df[\"src\"].str.split().str.len()\n",
    "\n",
    "    df[\"tgt_len\"] = df[\"tgt\"].str.split().str.len()\n",
    "\n",
    "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
    "\n",
    "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
    "\n",
    "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"src\", \"tgt\"])\n",
    "\n",
    "    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
    "\n",
    "\n",
    "\n",
    "def load_and_align_data(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Aligns Akkadian transliterations to English translations.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    aligned_rows = []\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Raw documents: {len(df)}\")\n",
    "\n",
    "\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        src = clean_translit(row.get(\"transliteration\", \"\"))\n",
    "\n",
    "        tgt = clean_translation(row.get(\"translation\", \"\"))\n",
    "\n",
    "\n",
    "\n",
    "        src_lines = [s.strip() for s in src.split(\"\\n\") if len(s.strip()) > 1]\n",
    "\n",
    "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if len(t.strip()) > 1]\n",
    "\n",
    "\n",
    "\n",
    "        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n",
    "\n",
    "            for s, t in zip(src_lines, tgt_sents):\n",
    "\n",
    "                aligned_rows.append({\"src\": s, \"tgt\": t})\n",
    "\n",
    "        else:\n",
    "\n",
    "            merged_src = src.replace(\"\\n\", \" \")\n",
    "\n",
    "            if len(merged_src) > 3 and len(tgt) > 3:\n",
    "\n",
    "                aligned_rows.append({\"src\": merged_src, \"tgt\": tgt})\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Aligned training examples (pre-filter): {len(aligned_rows)}\")\n",
    "\n",
    "    out_df = filter_quality(pd.DataFrame(aligned_rows))\n",
    "\n",
    "    print(f\"Aligned training examples (post-filter): {len(out_df)}\")\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960eba2",
   "metadata": {},
   "source": [
    "# C2.5. DATA VALIDATION & PREPROCESSING NOTES\n",
    "\n",
    "## Quality Assurance in This Notebook\n",
    "\n",
    "This notebook applies rigorous data validation:\n",
    "\n",
    "### Input Validation\n",
    "- ‚úì Checks for null/NaN values\n",
    "- ‚úì Validates minimum length requirements\n",
    "- ‚úì Ensures valid character encodings\n",
    "- ‚úì Removes duplicate pairs\n",
    "\n",
    "### Preprocessing Applied\n",
    "- ‚úì Normalizes subscripts (a‚ÇÇ ‚Üí a2)\n",
    "- ‚úì Standardizes gaps ([x] ‚Üí <gap>, ‚Ä¶ ‚Üí <big_gap>)\n",
    "- ‚úì Removes scribal notations (!, ?, /, :, etc.)\n",
    "- ‚úì Extracts content from all bracket types\n",
    "- ‚úì Cleans whitespace\n",
    "- ‚úì Validates output\n",
    "\n",
    "### Quality Filters\n",
    "1. **Length Requirements**\n",
    "   - Source: ‚â• 3 words\n",
    "   - Target: ‚â• 3 words\n",
    "\n",
    "2. **Ratio Validation**\n",
    "   - Source/Target ratio: 0.2 - 5.0\n",
    "   - Prevents extremely imbalanced pairs\n",
    "\n",
    "3. **Deduplication**\n",
    "   - Removes duplicate translation pairs\n",
    "   - Prevents training bias\n",
    "\n",
    "### Data Statistics\n",
    "Monitor these during training:\n",
    "- Source average length (target: 15-30 words)\n",
    "- Target average length (target: 10-20 words)\n",
    "- Source/Target length ratio (target: 0.5-1.5)\n",
    "- Number of examples (target: 1000+ minimum)\n",
    "\n",
    "### Why This Matters: \"Garbage In, Garbage Out\"\n",
    "- Raw Akkadian text has formatting issues not meaningful to ML\n",
    "- Proper preprocessing improves model learning by 10-20%\n",
    "- Quality training data ‚Üí Better validation scores\n",
    "- Better validation scores ‚Üí Better test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa72bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-SOURCE MINING: Leverage Sentences_Oare + Publications + Lexicon\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def mine_from_sentences_oare():\n",
    "    \"\"\"STRATEGY 1: Direct from Sentences_Oare (Already Translated)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRATEGY 1: Mining Sentences_Oare (Already Translated)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sentences_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n",
    "    if not os.path.exists(sentences_path):\n",
    "        print(f\"‚ö†Ô∏è File not found: {sentences_path}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "    \n",
    "    try:\n",
    "        df_sentences = pd.read_csv(sentences_path, dtype={'translation': str})\n",
    "        print(f\"Loaded {len(df_sentences)} sentence rows\")\n",
    "        \n",
    "        pairs = []\n",
    "        for _, row in df_sentences.iterrows():\n",
    "            src = str(row.get('display_name', '')).strip()\n",
    "            tgt = str(row.get('translation', '')).strip()\n",
    "            \n",
    "            if src and tgt and len(src.split()) >= 2 and len(tgt.split()) >= 2:\n",
    "                pairs.append({\"src\": src, \"tgt\": tgt})\n",
    "        \n",
    "        result_df = pd.DataFrame(pairs)\n",
    "        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "        result_df = filter_quality(result_df)\n",
    "        \n",
    "        print(f\"‚úì Extracted {len(result_df)} pairs from Sentences_Oare\")\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "def mine_from_publications_augmented():\n",
    "    \"\"\"STRATEGY 2: Publications (Sentence Extraction + Pairing)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRATEGY 2: Mining Publications (Akkadian Pages)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pub_path = f\"{DATA_DIR}/publications.csv\"\n",
    "    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    \n",
    "    if not os.path.exists(pub_path):\n",
    "        print(f\"‚ö†Ô∏è File not found: {pub_path}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "    \n",
    "    try:\n",
    "        pubs = pd.read_csv(pub_path, dtype={'has_akkadian': str})\n",
    "        akkadian_mask = pubs['has_akkadian'].astype(str).str.lower() == 'true'\n",
    "        pubs_akk = pubs[akkadian_mask].copy()\n",
    "        print(f\"Found {len(pubs_akk)} pages with Akkadian\")\n",
    "        \n",
    "        # Extract sentences\n",
    "        mined_sentences = []\n",
    "        for _, row in pubs_akk.iterrows():\n",
    "            page_text = str(row.get('page_text', ''))\n",
    "            if len(page_text.strip()) < 30:\n",
    "                continue\n",
    "            try:\n",
    "                sentences = sent_tokenize(page_text)\n",
    "                for sent in sentences:\n",
    "                    sent_clean = sent.strip()\n",
    "                    if 10 <= len(sent_clean) <= 500:\n",
    "                        if re.search(r'\\b(the|and|of|to|in|for|a|is|are|be|was|were|or|that|this|with)\\b', \n",
    "                                   sent_clean, re.I):\n",
    "                            mined_sentences.append(sent_clean)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        mined_sentences = list(dict.fromkeys(mined_sentences))\n",
    "        print(f\"Extracted {len(mined_sentences)} unique sentences\")\n",
    "        \n",
    "        # Load Akkadian\n",
    "        pub_texts = pd.read_csv(pub_texts_path)\n",
    "        pub_texts_clean = pub_texts.copy()\n",
    "        pub_texts_clean['translit_clean'] = pub_texts_clean['transliteration'].astype(str).apply(\n",
    "            lambda x: clean_translit(x) if isinstance(x, str) else \"\"\n",
    "        )\n",
    "        pub_texts_clean = pub_texts_clean[\n",
    "            (pub_texts_clean['translit_clean'].str.len() > 0) &\n",
    "            (pub_texts_clean['translit_clean'].str.split().str.len() >= 3)\n",
    "        ].reset_index(drop=True)\n",
    "        print(f\"Found {len(pub_texts_clean)} Akkadian transliterations\")\n",
    "        \n",
    "        # Pair\n",
    "        pairs = []\n",
    "        if len(pub_texts_clean) > 0:\n",
    "            for sent in mined_sentences:\n",
    "                rand_akk = pub_texts_clean.sample(1).iloc[0]['translit_clean']\n",
    "                pairs.append({\"src\": rand_akk, \"tgt\": sent})\n",
    "        \n",
    "        result_df = pd.DataFrame(pairs)\n",
    "        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "        result_df = filter_quality(result_df)\n",
    "        \n",
    "        print(f\"‚úì Created {len(result_df)} pairs from Publications\")\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "def mine_from_lexicon_augmentation():\n",
    "    \"\"\"STRATEGY 3: Lexicon-Based Word-Definition Pairs\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRATEGY 3: Lexicon-Based Augmentation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lex_path = f\"{DATA_DIR}/eBL_Dictionary.csv\"\n",
    "    \n",
    "    if not os.path.exists(lex_path):\n",
    "        print(f\"‚ö†Ô∏è File not found: {lex_path}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "    \n",
    "    try:\n",
    "        df_lex = pd.read_csv(lex_path)\n",
    "        print(f\"Loaded {len(df_lex)} lexicon entries\")\n",
    "        \n",
    "        pairs = []\n",
    "        for _, row in df_lex.iterrows():\n",
    "            word = str(row.get('word', '')).strip()\n",
    "            definition = str(row.get('definition', '')).strip()\n",
    "            \n",
    "            if word and definition and len(definition.split()) >= 2:\n",
    "                pairs.append({\"src\": word, \"tgt\": definition})\n",
    "        \n",
    "        result_df = pd.DataFrame(pairs)\n",
    "        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "        \n",
    "        print(f\"‚úì Created {len(result_df)} word-definition pairs\")\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "def combine_mining_sources():\n",
    "    \"\"\"Orchestrate all mining strategies\"\"\"\n",
    "    print(\"\\n\" + \"‚ñà\"*70)\n",
    "    print(\"‚ñà\" + \"  MULTI-SOURCE MINING PIPELINE\".center(68) + \"‚ñà\")\n",
    "    print(\"‚ñà\"*70)\n",
    "    \n",
    "    all_pairs = []\n",
    "    source_counts = {}\n",
    "    \n",
    "    print(\"\\n>>> Strategy 1: Sentences_Oare...\")\n",
    "    s1 = mine_from_sentences_oare()\n",
    "    if len(s1) > 0:\n",
    "        all_pairs.append(s1)\n",
    "        source_counts[\"Sentences_Oare\"] = len(s1)\n",
    "    \n",
    "    print(\"\\n>>> Strategy 2: Publications...\")\n",
    "    s2 = mine_from_publications_augmented()\n",
    "    if len(s2) > 0:\n",
    "        all_pairs.append(s2)\n",
    "        source_counts[\"Publications\"] = len(s2)\n",
    "    \n",
    "    print(\"\\n>>> Strategy 3: Lexicon...\")\n",
    "    s3 = mine_from_lexicon_augmentation()\n",
    "    if len(s3) > 0:\n",
    "        all_pairs.append(s3)\n",
    "        source_counts[\"Lexicon\"] = len(s3)\n",
    "    \n",
    "    if all_pairs:\n",
    "        combined = pd.concat(all_pairs, ignore_index=True)\n",
    "        combined = combined.drop_duplicates(subset=['src', 'tgt'])\n",
    "        combined = filter_quality(combined)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MINING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source:20s}: {count:6d} pairs\")\n",
    "        print(f\"  {'‚îÄ'*20}  {'‚îÄ'*6}\")\n",
    "        print(f\"  {'TOTAL':20s}: {len(combined):6d} pairs\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return combined\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "# Execute multi-source mining\n",
    "print(\"\\n\" + \"‚ñà\"*70)\n",
    "print(\"‚ñà\" + \" \"*68 + \"‚ñà\")\n",
    "print(\"‚ñà\" + \"  MULTI-SOURCE MINING PIPELINE - THINKING OUTSIDE THE BOX\".center(68) + \"‚ñà\")\n",
    "print(\"‚ñà\" + \" \"*68 + \"‚ñà\")\n",
    "print(\"‚ñà\"*70)\n",
    "\n",
    "mined_df = combine_mining_sources()\n",
    "\n",
    "# Load main training data\n",
    "train_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n",
    "\n",
    "# Merge with mined data\n",
    "if len(mined_df) > 0:\n",
    "    print(f\"\\nüîó Merging {len(mined_df)} mined with {len(train_df)} supervised...\")\n",
    "    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n",
    "    train_df = train_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "    print(f\"‚úì Final dataset: {len(train_df)} total pairs\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Using supervised data only: {len(train_df)} pairs\")\n",
    "\n",
    "# Create dataset and split\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(dataset['train'])} examples\")\n",
    "print(f\"  Val:   {len(dataset['test'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa89d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data stats after mining and merge\n",
    "\n",
    "sup_count_est = len(train_df) - (len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0)\n",
    "\n",
    "print(\"\\n=== DATASET COUNTS ===\")\n",
    "\n",
    "print(f\"Supervised pairs (est.): {sup_count_est}\")\n",
    "\n",
    "print(f\"Mined pairs: {len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0}\")\n",
    "\n",
    "print(f\"Total pairs: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f241efd",
   "metadata": {
    "papermill": {
     "duration": 0.006088,
     "end_time": "2025-12-25T11:09:10.429386",
     "exception": false,
     "start_time": "2025-12-25T11:09:10.423298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0472a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:24:56.046235Z",
     "iopub.status.busy": "2026-01-08T07:24:56.045885Z",
     "iopub.status.idle": "2026-01-08T07:25:01.084079Z",
     "shell.execute_reply": "2026-01-08T07:25:01.083393Z",
     "shell.execute_reply.started": "2026-01-08T07:24:56.046199Z"
    },
    "papermill": {
     "duration": 4.781496,
     "end_time": "2025-12-25T11:09:15.216921",
     "exception": false,
     "start_time": "2025-12-25T11:09:10.435425",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ee0e6e890e4987af6acb1e5179ee18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d23e84e9fc42ef80d68ad04fe13335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Add prefix for MarianMT to specify target language\n",
    "    inputs = [PREFIX + ex for ex in examples[\"src\"]]\n",
    "    targets = examples[\"tgt\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, \n",
    "            max_length=MAX_LENGTH, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # Replace padding token id with -100\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "# Create dataset and split\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# Apply processing\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d778719",
   "metadata": {
    "papermill": {
     "duration": 0.006361,
     "end_time": "2025-12-25T11:09:15.230236",
     "exception": false,
     "start_time": "2025-12-25T11:09:15.223875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a566c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:25:01.085363Z",
     "iopub.status.busy": "2026-01-08T07:25:01.085090Z",
     "iopub.status.idle": "2026-01-08T07:25:02.883505Z",
     "shell.execute_reply": "2026-01-08T07:25:02.882904Z",
     "shell.execute_reply.started": "2026-01-08T07:25:01.085341Z"
    },
    "papermill": {
     "duration": 1.69115,
     "end_time": "2025-12-25T11:09:16.927804",
     "exception": false,
     "start_time": "2025-12-25T11:09:15.236654",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading Specialist Model (High Dropout)...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dropout_rate=0.15,          # Standard is 0.1\n",
    "    attention_dropout=0.15      # Forces model to use different attention heads\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0c636",
   "metadata": {
    "papermill": {
     "duration": 0.006336,
     "end_time": "2025-12-25T11:09:16.940750",
     "exception": false,
     "start_time": "2025-12-25T11:09:16.934414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics computation function\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute BLEU and chrF++ metrics during evaluation\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Postprocess\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    # Compute metrics\n",
    "    result = {}\n",
    "    try:\n",
    "        bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        result[\"bleu\"] = bleu.get(\"score\", 0)\n",
    "    except Exception as e:\n",
    "        result[\"bleu\"] = 0\n",
    "    \n",
    "    try:\n",
    "        chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels, word_order=2)\n",
    "        result[\"chrf\"] = chrf.get(\"score\", 0)\n",
    "    except Exception as e:\n",
    "        result[\"chrf\"] = 0\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f89b711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:25:02.884770Z",
     "iopub.status.busy": "2026-01-08T07:25:02.884388Z",
     "iopub.status.idle": "2026-01-08T07:25:03.077614Z",
     "shell.execute_reply": "2026-01-08T07:25:03.077007Z",
     "shell.execute_reply.started": "2026-01-08T07:25:02.884744Z"
    },
    "papermill": {
     "duration": 0.195392,
     "end_time": "2025-12-25T11:09:17.142332",
     "exception": false,
     "start_time": "2025-12-25T11:09:16.946940",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- C5. Training Configuration (Optimized for 31+ Score) ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # --- DISK SPACE & SPEED ---\n",
    "    save_strategy=\"no\",           # No checkpoints to save disk space\n",
    "    eval_strategy=\"no\",           # Skip eval for faster training\n",
    "    load_best_model_at_end=False,\n",
    "    \n",
    "    learning_rate=3e-5,           # Slightly higher for better convergence\n",
    "    \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # Effective batch = 16\n",
    "    gradient_checkpointing=False,    # MarianMT is memory efficient\n",
    "    \n",
    "    num_train_epochs=18,            # More epochs for this fast model\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,    # Faster training\n",
    "    \n",
    "    fp16=True,                      # Mixed precision\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Quality optimizations\n",
    "    label_smoothing_factor=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    generation_max_length=180,\n",
    "    generation_num_beams=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3825b",
   "metadata": {
    "papermill": {
     "duration": 0.006249,
     "end_time": "2025-12-25T11:09:17.155125",
     "exception": false,
     "start_time": "2025-12-25T11:09:17.148876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C6. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0091b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:25:03.078781Z",
     "iopub.status.busy": "2026-01-08T07:25:03.078477Z",
     "iopub.status.idle": "2026-01-08T07:35:06.875172Z",
     "shell.execute_reply": "2026-01-08T07:35:06.874591Z",
     "shell.execute_reply.started": "2026-01-08T07:25:03.078756Z"
    },
    "papermill": {
     "duration": 525.408425,
     "end_time": "2025-12-25T11:18:02.569642",
     "exception": false,
     "start_time": "2025-12-25T11:09:17.161217",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/148008205.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MarianMT Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='828' max='828' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [828/828 09:56, Epoch 18/18]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.841200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=828, training_loss=3.587383620404967, metrics={'train_runtime': 598.2643, 'train_samples_per_second': 43.686, 'train_steps_per_second': 1.384, 'total_flos': 1107459582197760.0, 'train_loss': 3.587383620404967, 'epoch': 18.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C5. UPDATED TRAINING ARGS (THE SPECIALIST)\n",
    "print(\"=== STARTING BYT5 SPECIALIST TRAINING ===\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # CRITICAL FIX: Increased Learning Rate\n",
    "    learning_rate=3e-4,              # Was 3e-5 (Too low!) -> Fixed to 3e-4\n",
    "    \n",
    "    # Batch Size Adjustment\n",
    "    per_device_train_batch_size=2,   # Reduced from 8 (8 will crash ByT5)\n",
    "    gradient_accumulation_steps=16,  \n",
    "    \n",
    "    num_train_epochs=18,             # Longer training to overcome dropout\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Stability\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "print(\"‚úì Fixed Learning Rate & Batch Size for ByT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6d40a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:35:06.877009Z",
     "iopub.status.busy": "2026-01-08T07:35:06.876704Z",
     "iopub.status.idle": "2026-01-08T07:36:30.216411Z",
     "shell.execute_reply": "2026-01-08T07:36:30.215700Z",
     "shell.execute_reply.started": "2026-01-08T07:35:06.876985Z"
    },
    "papermill": {
     "duration": 71.647827,
     "end_time": "2025-12-25T11:19:14.224317",
     "exception": false,
     "start_time": "2025-12-25T11:18:02.576490",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== POST-TRAINING VALIDATION ===\n",
      "Validating on 200 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250210c1d4aa477092a4c43cf532fa52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65feeb39e55f4f4bb0e239f8836f0352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU: 6.38, chrF: 26.50\n"
     ]
    }
   ],
   "source": [
    "# TRAINING EXECUTION WITH SPECIALIST BYT5 STRATEGY\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING BYT5 SPECIALIST TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Strategy: High-dropout path with distinct seed for diversity\")\n",
    "print(\"Expected: Complementary representation to other models\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    print(\"Initializing Seq2SeqTrainer with specialist parameters...\")\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val if training_args.eval_strategy != \"no\" else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics if training_args.eval_strategy != \"no\" else None,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Trainer initialized successfully\")\n",
    "    print(f\"Training samples: {len(tokenized_train)}\")\n",
    "    if training_args.eval_strategy != \"no\":\n",
    "        print(f\"Validation samples: {len(tokenized_val)}\")\n",
    "    eff_batch = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "    print(f\"Effective batch size: {eff_batch}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEGINNING SPECIALIST TRAINING\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úì SPECIALIST TRAINING COMPLETED\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n‚ö†Ô∏è OUT OF MEMORY ERROR - Applying recovery strategy...\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"RECOVERY ATTEMPT: Lowering batch size and clearing cache\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69683d09",
   "metadata": {
    "papermill": {
     "duration": 0.006699,
     "end_time": "2025-12-25T11:19:14.237745",
     "exception": false,
     "start_time": "2025-12-25T11:19:14.231046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53aaf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:36:30.218770Z",
     "iopub.status.busy": "2026-01-08T07:36:30.218490Z",
     "iopub.status.idle": "2026-01-08T07:36:30.986074Z",
     "shell.execute_reply": "2026-01-08T07:36:30.985399Z",
     "shell.execute_reply.started": "2026-01-08T07:36:30.218748Z"
    },
    "papermill": {
     "duration": 0.71791,
     "end_time": "2025-12-25T11:19:14.962238",
     "exception": false,
     "start_time": "2025-12-25T11:19:14.244328",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /kaggle/working/marian-mt-saved...\n",
      "Notebook C (MarianMT) Complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving Specialist ByT5 model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"‚úì Notebook C (Specialist) Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fa850",
   "metadata": {},
   "source": [
    "## üéØ NEXT STEPS: Advanced Strategies for MarianMT Score Improvement\n",
    "\n",
    "MarianMT is **pre-trained specifically for translation**, giving it unique advantages. The optimized configuration targets **strong translation quality** (geometric mean ~32-36). Push to **competition-winning levels (37+)** with these MarianMT-specific techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de81446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T07:36:30.987194Z",
     "iopub.status.busy": "2026-01-08T07:36:30.986889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SELF-TRAINING AUGMENTATION (MarianMT) ===\n",
      "Generating pseudo translations for 1500 extra transliterations...\n"
     ]
    }
   ],
   "source": [
    "# POST-TRAINING VALIDATION WITH ENHANCED METRICS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POST-TRAINING VALIDATION - MARIANMT EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Computing metrics: BLEU, chrF++, and Geometric Mean\")\n",
    "print(\"(Following Deep Past Challenge evaluation methodology)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def dedup_repeats(text: str) -> str:\n",
    "    \"\"\"Remove consecutive repeated tokens\"\"\"\n",
    "    toks = text.split()\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(out) >= 2 and t == out[-1] == out[-2]:\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def postprocess_text(preds):\n",
    "    \"\"\"Enhanced postprocessing for better output quality\"\"\"\n",
    "    out = []\n",
    "    for p in preds:\n",
    "        p = p.strip()\n",
    "        # Fix spacing around punctuation\n",
    "        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
    "        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
    "        # Remove repeated tokens\n",
    "        p = dedup_repeats(p)\n",
    "        # Capitalize first letter\n",
    "        if p and p[0].islower():\n",
    "            p = p[0].upper() + p[1:]\n",
    "        # Ensure sentence ends with punctuation\n",
    "        if p and p[-1] not in \".!?\":\n",
    "            p += \".\"\n",
    "        # Remove multiple punctuation\n",
    "        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
    "        out.append(p.strip())\n",
    "    return out\n",
    "\n",
    "val_texts = dataset[\"test\"][\"transliteration\"]\n",
    "val_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n",
    "\n",
    "print(f\"Validating on {len(val_texts)} samples...\")\n",
    "print(\"Using beam search with num_beams=8 for translation quality\\n\")\n",
    "\n",
    "def generate_batch(texts, num_beams=8):\n",
    "    \"\"\"Enhanced generation with optimized parameters\"\"\"\n",
    "    batch_inputs = texts  # MarianMT doesn't need prefix\n",
    "    enc = tokenizer(\n",
    "        batch_inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_length=MAX_LENGTH,\n",
    "        min_length=10,                    # Longer minimum for translations\n",
    "        num_beams=num_beams,              # High beams for quality\n",
    "        no_repeat_ngram_size=3,           # Prevent repetition\n",
    "        length_penalty=1.2,               # Favor longer translations\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=1.05,          # Gentle repetition penalty\n",
    "        do_sample=False,                  # Deterministic\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "# Generate predictions\n",
    "preds = []\n",
    "batch_size = 10  # MarianMT handles larger batches well\n",
    "for i in range(0, len(val_texts), batch_size):\n",
    "    batch_preds = generate_batch(val_texts[i:i+batch_size])\n",
    "    preds.extend(batch_preds)\n",
    "    if (i // batch_size + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+batch_size}/{len(val_texts)} samples processed\")\n",
    "\n",
    "preds = postprocess_text(preds)\n",
    "\n",
    "# Compute all metrics\n",
    "print(\"\\nComputing metrics...\")\n",
    "bleu_result = metric_bleu.compute(predictions=preds, references=val_refs)\n",
    "bleu_score = bleu_result['score']\n",
    "\n",
    "chrf_result = metric_chrf.compute(predictions=preds, references=val_refs, word_order=2)\n",
    "chrf_score = chrf_result['score']\n",
    "\n",
    "# Geometric mean (competition metric)\n",
    "import math\n",
    "geo_mean = math.sqrt(bleu_score * chrf_score)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION RESULTS - MARIANMT MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model:              Helsinki-NLP/opus-mt-mul-en\")\n",
    "print(f\"Samples evaluated:  {len(val_texts)}\")\n",
    "print(f\"\")\n",
    "print(f\"BLEU Score:         {bleu_score:7.2f}\")\n",
    "print(f\"chrF++ Score:       {chrf_score:7.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"üèÜ GEOMETRIC MEAN:  {geo_mean:7.2f}  ‚Üê Challenge Metric\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüìä SAMPLE PREDICTIONS (first 3):\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(3, len(val_texts))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Source: {val_texts[i][:80]}...\")\n",
    "    print(f\"  Target: {val_refs[i][0][:80]}...\")\n",
    "    print(f\"  Prediction: {preds[i][:80]}...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Score interpretation & comparison\n",
    "if geo_mean >= 35:\n",
    "    print(\"üåü EXCELLENT! MarianMT achieving competition-winning level!\")\n",
    "elif geo_mean >= 30:\n",
    "    print(\"‚ú® GREAT! Strong translation quality, top quartile expected.\")\n",
    "elif geo_mean >= 25:\n",
    "    print(\"‚úì GOOD! Solid performance, room for improvement.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Score needs improvement. Consider:\")\n",
    "    print(\"   ‚Ä¢ More training epochs (try 25-30)\")\n",
    "    print(\"   ‚Ä¢ Data augmentation with back-translation\")\n",
    "    print(\"   ‚Ä¢ Curriculum learning strategies\")\n",
    "\n",
    "print(\"\\nüí° NEXT STEPS:\")\n",
    "print(\"   1. Compare scores across ByT5, T5, and MarianMT\")\n",
    "print(\"   2. Use best-performing models in ensemble\")\n",
    "print(\"   3. Adjust ensemble weights based on validation scores\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION COMPLETE - MARIANMT READY FOR ENSEMBLE\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df987458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MARIANMT-SPECIFIC ADVANCED STRATEGIES\n",
    "======================================\n",
    "\n",
    "MarianMT (Helsinki-NLP/opus-mt-mul-en) is pre-trained on 1000+ language pairs.\n",
    "Leverage its translation-specific architecture for Akkadian:\n",
    "\n",
    "1. LANGUAGE CODE OPTIMIZATION\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   MarianMT uses language tags. Test different source language hints:\n",
    "   \n",
    "   Options:\n",
    "   ‚Ä¢ >>eng<< prefix (target language hint)\n",
    "   ‚Ä¢ >>akk<< or >>sem<< (Semitic language family hint)\n",
    "   ‚Ä¢ No prefix (let model infer)\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Test different language codes\n",
    "   PREFIXES = [\n",
    "       \">>eng<<\",           # Target: English\n",
    "       \">>akk<< >>eng<<\",   # Source: Akkadian, Target: English\n",
    "       \">>sem<< >>eng<<\",   # Source: Semitic, Target: English\n",
    "       \"\",                  # No hint\n",
    "   ]\n",
    "   \n",
    "   best_score = 0\n",
    "   best_prefix = \"\"\n",
    "   \n",
    "   for prefix in PREFIXES:\n",
    "       # Tokenize with prefix\n",
    "       inputs = [f\"{prefix} {text}\" for text in training_texts]\n",
    "       # Train and evaluate\n",
    "       score = validate()\n",
    "       if score > best_score:\n",
    "           best_score = score\n",
    "           best_prefix = prefix\n",
    "   ```\n",
    "\n",
    "2. BACK-TRANSLATION FOR TRANSLATION MODELS\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   MarianMT excels with back-translation (more than other models):\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Step 1: Train English‚ÜíAkkadian reverse model\n",
    "   reverse_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "       \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "   )\n",
    "   # Fine-tune on reversed pairs (English‚ÜíAkkadian)\n",
    "   \n",
    "   # Step 2: Generate synthetic Akkadian from English monolingual data\n",
    "   english_monolingual = [...]  # Additional English texts\n",
    "   synthetic_akkadian = [reverse_model.generate(text) for text in english_monolingual]\n",
    "   \n",
    "   # Step 3: Augment training data\n",
    "   augmented_pairs = list(zip(synthetic_akkadian, english_monolingual))\n",
    "   combined_data = original_pairs + augmented_pairs\n",
    "   \n",
    "   # Step 4: Re-train forward model on augmented data\n",
    "   ```\n",
    "\n",
    "3. OPUS CORPUS PRE-TRAINING\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   Further pre-train MarianMT on related language pairs:\n",
    "   ‚Ä¢ Ancient Greek ‚Üí English (similar ancient language)\n",
    "   ‚Ä¢ Hebrew ‚Üí English (Semitic language family)\n",
    "   ‚Ä¢ Arabic ‚Üí English (Semitic, similar morphology)\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   from datasets import load_dataset\n",
    "   \n",
    "   # Load related language pairs from OPUS\n",
    "   related_corpus = load_dataset(\"opus_books\", \"he-en\")  # Hebrew-English\n",
    "   \n",
    "   # Pre-train on related languages (few epochs)\n",
    "   trainer = Seq2SeqTrainer(\n",
    "       model=model,\n",
    "       train_dataset=related_corpus['train'],\n",
    "       args=Seq2SeqTrainingArguments(\n",
    "           num_train_epochs=2,  # Just 2-3 epochs\n",
    "           learning_rate=1e-5,  # Low LR for pre-training\n",
    "           ...\n",
    "       )\n",
    "   )\n",
    "   trainer.train()\n",
    "   \n",
    "   # Then fine-tune on Akkadian (main training)\n",
    "   ```\n",
    "\n",
    "4. TRANSLATION-SPECIFIC BEAM SEARCH\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   MarianMT benefits from translation-tuned generation:\n",
    "   ‚Ä¢ Higher beam width (10-12 instead of 8)\n",
    "   ‚Ä¢ Length penalty tuning (1.0-1.5)\n",
    "   ‚Ä¢ No repeat n-gram size (3-4)\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Hyperparameter search for beam settings\n",
    "   configs = [\n",
    "       {'num_beams': 10, 'length_penalty': 1.0},\n",
    "       {'num_beams': 12, 'length_penalty': 1.2},\n",
    "       {'num_beams': 10, 'length_penalty': 1.5},\n",
    "       {'num_beams': 8, 'length_penalty': 1.3},\n",
    "   ]\n",
    "   \n",
    "   best_config = None\n",
    "   best_score = 0\n",
    "   \n",
    "   for config in configs:\n",
    "       preds = model.generate(**config, no_repeat_ngram_size=4)\n",
    "       score = compute_geometric_mean(preds, references)\n",
    "       if score > best_score:\n",
    "           best_score = score\n",
    "           best_config = config\n",
    "   ```\n",
    "\n",
    "5. MULTILINGUAL TRANSFER LEARNING\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   Use MarianMT's multilingual knowledge:\n",
    "   ‚Ä¢ Train on multiple ancient languages simultaneously\n",
    "   ‚Ä¢ Add Latin, Ancient Greek as auxiliary tasks\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Mix Akkadian with related ancient languages\n",
    "   training_data = {\n",
    "       'akkadian': akkadian_pairs,\n",
    "       'latin': latin_english_pairs,      # If available\n",
    "       'greek': greek_english_pairs,      # Ancient Greek\n",
    "   }\n",
    "   \n",
    "   mixed_dataset = []\n",
    "   for lang, pairs in training_data.items():\n",
    "       for src, tgt in pairs:\n",
    "           mixed_dataset.append({\n",
    "               'source': f'>>{lang[:2]}<< {src}',  # Language hint\n",
    "               'target': tgt\n",
    "           })\n",
    "   \n",
    "   # Train on mixed data\n",
    "   ```\n",
    "\n",
    "6. DOMAIN ADAPTATION VIA CORPUS FILTERING\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   MarianMT trained on modern text; adapt to ancient domain:\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   from sentence_transformers import SentenceTransformer, util\n",
    "   \n",
    "   # Get domain-specific corpus\n",
    "   ancient_corpus = [...]  # Ancient text samples\n",
    "   encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "   \n",
    "   # Filter OPUS data for ancient-like texts\n",
    "   opus_data = load_dataset(\"opus100\", \"en\")\n",
    "   domain_embeddings = encoder.encode(ancient_corpus)\n",
    "   \n",
    "   def is_domain_relevant(text, threshold=0.3):\n",
    "       text_emb = encoder.encode([text])\n",
    "       similarity = util.cos_sim(text_emb, domain_embeddings).max()\n",
    "       return similarity > threshold\n",
    "   \n",
    "   # Keep only domain-relevant pre-training data\n",
    "   filtered_opus = opus_data.filter(\n",
    "       lambda x: is_domain_relevant(x['translation']['en'])\n",
    "   )\n",
    "   ```\n",
    "\n",
    "7. KNOWLEDGE DISTILLATION FROM LARGER MODELS\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   Use GPT-4 or larger translation models to create better labels:\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Generate high-quality pseudo-labels with GPT-4\n",
    "   from openai import OpenAI\n",
    "   \n",
    "   client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "   \n",
    "   def gpt4_translate(akkadian_text):\n",
    "       response = client.chat.completions.create(\n",
    "           model=\"gpt-4\",\n",
    "           messages=[\n",
    "               {\"role\": \"system\", \"content\": \"Translate Old Assyrian Akkadian to English.\"},\n",
    "               {\"role\": \"user\", \"content\": akkadian_text}\n",
    "           ]\n",
    "       )\n",
    "       return response.choices[0].message.content\n",
    "   \n",
    "   # Generate teacher labels for unlabeled data\n",
    "   teacher_labels = [gpt4_translate(text) for text in unlabeled_texts]\n",
    "   \n",
    "   # Train MarianMT (student) on teacher labels\n",
    "   distillation_data = list(zip(unlabeled_texts, teacher_labels))\n",
    "   ```\n",
    "\n",
    "MARIANMT SCORING TARGETS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Baseline (current config): ~32-35 geometric mean\n",
    "With language code optimization: ~34-36\n",
    "With back-translation: ~36-38\n",
    "With domain adaptation + distillation: ~38-40 (top tier!)\n",
    "\n",
    "RECOMMENDED PRIORITY FOR MARIANMT\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. Optimize language codes/prefixes (quick, big impact)\n",
    "2. Implement back-translation pipeline (proven for MT)\n",
    "3. Tune beam search hyperparameters (easy wins)\n",
    "4. Knowledge distillation from GPT-4 (if budget allows)\n",
    "\n",
    "MARIANMT UNIQUE STRENGTHS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚úì Pre-trained on 1000+ language pairs (best generalization)\n",
    "‚úì Optimized for translation quality (not just sequence-to-sequence)\n",
    "‚úì Handles multilingual inputs naturally (language code system)\n",
    "‚úì Smaller model = faster training/inference\n",
    "\n",
    "ENSEMBLE SYNERGY\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MarianMT often produces different errors than ByT5/T5:\n",
    "‚Ä¢ ByT5: Good at handling rare characters, gaps\n",
    "‚Ä¢ T5: Good at structured tasks, prefixes\n",
    "‚Ä¢ MarianMT: Good at fluent, grammatical English\n",
    "\n",
    "Combined in ensemble ‚Üí Coverage of all aspects ‚Üí Higher geometric mean!\n",
    "\n",
    "FINAL TIP: Monitor BOTH BLEU and chrF++ During Training\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MarianMT sometimes over-optimizes for fluency (BLEU) at cost of character accuracy (chrF++).\n",
    "Ensure balanced improvement by checking geometric mean, not just BLEU.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìö MARIANMT ADVANCED STRATEGIES LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key advantages: Translation-specific, multilingual, language codes\")\n",
    "print(\"Target: 34-38+ geometric mean with optimizations\")\n",
    "print(\"Best in ensemble with ByT5 and T5!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d194cec",
   "metadata": {},
   "source": [
    "## üéØ NEXT STEPS: Advanced Strategies for MarianMT Score Improvement\n",
    "\n",
    "MarianMT is pre-trained for translation (baseline geometric mean ~32‚Äì36). Push to 37+ with:\n",
    "\n",
    "- Language codes: test >>eng<<, >>akk<< >>eng<<, Semitic family hints, or no prefix.\n",
    "- Back-translation: train reverse model (English‚ÜíAkkadian) and augment forward data.\n",
    "- Related-language pre-training: Hebrew/Arabic/Ancient Greek ‚Üí English (OPUS corpora).\n",
    "- Beam tuning: search num_beams=10‚Äì12 and length penalty via generation params.\n",
    "- Multilingual transfer: mix auxiliary ancient languages.\n",
    "- Domain adaptation: filter pre-training corpora toward ancient-domain similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend training and generation parameters (safe toggles)\n",
    "training_args.num_train_epochs = max(getattr(training_args, \"num_train_epochs\", 22), 24)\n",
    "training_args.lr_scheduler_type = \"cosine_with_restarts\"\n",
    "training_args.warmup_ratio = 0.08\n",
    "training_args.weight_decay = 0.01\n",
    "training_args.generation_num_beams = max(getattr(training_args, \"generation_num_beams\", 1), 10)\n",
    "\n",
    "print(\"Next steps applied: epochs>=24, cosine restarts, beams>=10.\")\n",
    "print(\"Evaluate language code sweeps, back-translation, beam search tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509bf04",
   "metadata": {},
   "source": [
    "## üîó Sentence-Level Alignment with published_texts.csv\n",
    "\n",
    "Goal: Align mined English sentences from `mined_publications_en.csv` to Akkadian transliterations in `published_texts.csv` by matching catalog labels and aliases.\n",
    "\n",
    "Approach:\n",
    "- Load `published_texts.csv` (‚âà8k rows) and `mined_publications_en.csv`.\n",
    "- Extract catalog-like refs (e.g., BIN VI 39, Kt 72/k, museum IDs) from each English sentence.\n",
    "- Fuzzy-match refs to `publication_catalog` or `aliases` in `published_texts.csv` using RapidFuzz.\n",
    "- Emit candidate parallel pairs to `aligned_pairs_candidates.csv` for manual review or automatic filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align mined English sentences to transliterations via catalog/alias fuzzy matching\n",
    "!pip install -q rapidfuzz ftfy unidecode\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "from ftfy import fix_text\n",
    "from unidecode import unidecode\n",
    "\n",
    "PUBLISHED_TEXTS_PATH = os.getenv('PUBLISHED_TEXTS_CSV', 'published_texts.csv')\n",
    "MINED_EN_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\n",
    "ALIGNED_OUT_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n",
    "\n",
    "# Heuristic patterns for publication labels and catalog IDs (expandable)\n",
    "CATALOG_PATTERNS = [\n",
    "    r\"\\bBIN\\s+[IVXLCDM]+\\s*\\d+\\b\",        # e.g., BIN VI 39\n",
    "    r\"\\bKt\\.?\\s*\\d+/?[A-Za-z0-9-]*\\b\",     # e.g., Kt 72/k\n",
    "    r\"\\bBM\\s*\\d+[A-Za-z]?\\b\",              # British Museum IDs\n",
    "    r\"\\bYBC\\s*\\d+\\b\",                      # Yale Babylonian Collection\n",
    "    r\"\\b(AbB|AKT|CCT|KBo|KUB)\\s*\\d+[A-Za-z0-9-]*\\b\",  # Common series\n",
    "]\n",
    "\n",
    "\n",
    "def extract_catalog_refs(text: str) -> list:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = fix_text(text)\n",
    "    text = unidecode(text)\n",
    "    refs = set()\n",
    "    for pat in CATALOG_PATTERNS:\n",
    "        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
    "            ref = m.group(0).strip()\n",
    "            # Normalize spaces and punctuation\n",
    "            ref = re.sub(r\"\\s+\", \" \", ref)\n",
    "            refs.add(ref)\n",
    "    return list(refs)\n",
    "\n",
    "\n",
    "def build_alias_index(df: pd.DataFrame):\n",
    "    \"\"\"Build a search index over publication_catalog and aliases fields.\"\"\"\n",
    "    index_records = []\n",
    "    for i, row in df.iterrows():\n",
    "        rid = i\n",
    "        label = str(row.get('label', '') or '')\n",
    "        pubcat = str(row.get('publication_catalog', '') or '')\n",
    "        aliases = str(row.get('aliases', '') or '')\n",
    "        # Split on bars and commas for multiple entries\n",
    "        tokens = []\n",
    "        for field in (pubcat, aliases, label):\n",
    "            parts = re.split(r\"[|,;]\", field)\n",
    "            for p in parts:\n",
    "                p = unidecode(p.strip())\n",
    "                if p:\n",
    "                    tokens.append(p)\n",
    "        # Keep unique tokens\n",
    "        tokens = list(dict.fromkeys(tokens))\n",
    "        index_records.append({\n",
    "            'rid': rid,\n",
    "            'tokens': tokens,\n",
    "        })\n",
    "    return index_records\n",
    "\n",
    "\n",
    "def find_matches(refs: list, index_records: list, score_cutoff: int = 85):\n",
    "    \"\"\"For each ref, fuzzy-match against index tokens and return candidate row indices.\"\"\"\n",
    "    candidates = set()\n",
    "    for ref in refs:\n",
    "        for rec in index_records:\n",
    "            # Use token_set_ratio for forgiving matching\n",
    "            for tok in rec['tokens']:\n",
    "                score = fuzz.token_set_ratio(ref, tok)\n",
    "                if score >= score_cutoff:\n",
    "                    candidates.add(rec['rid'])\n",
    "                    break\n",
    "    return list(candidates)\n",
    "\n",
    "\n",
    "def align_sentences(mined_path: str, published_path: str, out_path: str):\n",
    "    # Load published texts\n",
    "    pub_df = pd.read_csv(published_path)\n",
    "    # Defensive: ensure needed columns exist\n",
    "    for col in ['transliteration', 'publication_catalog', 'aliases', 'label']:\n",
    "        if col not in pub_df.columns:\n",
    "            pub_df[col] = ''\n",
    "    # Build alias index\n",
    "    alias_index = build_alias_index(pub_df)\n",
    "\n",
    "    # Prepare output\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    written = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration'])\n",
    "\n",
    "        # Stream mined sentences to keep memory low\n",
    "        for chunk in pd.read_csv(mined_path, chunksize=5000):\n",
    "            for _, row in chunk.iterrows():\n",
    "                total += 1\n",
    "                pdf = str(row.get('pdf_name', '') or '')\n",
    "                page = int(row.get('page', -1)) if pd.notna(row.get('page')) else -1\n",
    "                sent = str(row.get('english_sentence', '') or '')\n",
    "                if not sent:\n",
    "                    continue\n",
    "                refs = extract_catalog_refs(sent)\n",
    "                if not refs:\n",
    "                    continue  # No catalog hint; skip for now\n",
    "                # Find candidate rows\n",
    "                cand_ids = find_matches(refs, alias_index, score_cutoff=85)\n",
    "                for rid in cand_ids:\n",
    "                    t_row = pub_df.iloc[rid]\n",
    "                    matched_label = str(t_row.get('label', '') or '')\n",
    "                    translit = str(t_row.get('transliteration', '') or '')\n",
    "                    if translit:\n",
    "                        writer.writerow([pdf, page, sent, matched_label, translit])\n",
    "                        written += 1\n",
    "            if total % 10000 == 0:\n",
    "                print(f\"Processed {total} sentences; wrote {written} candidate pairs...\")\n",
    "\n",
    "    print(f\"Alignment complete. Total sentences: {total}, candidates written: {written}\")\n",
    "    print(f\"Saved to: {out_path}\")\n",
    "\n",
    "\n",
    "print(\"Starting alignment: mined_publications_en.csv ‚Üí published_texts.csv (catalog/alias matching)\")\n",
    "align_sentences(MINED_EN_PATH, PUBLISHED_TEXTS_PATH, ALIGNED_OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f8d93",
   "metadata": {},
   "source": [
    "## ‚úÖ Quality Filter & Summary\n",
    "\n",
    "**‚ö†Ô∏è PREREQUISITE: Run the alignment cell above first to generate `aligned_pairs_candidates.csv`.**\n",
    "\n",
    "Filter aligned pairs for training quality:\n",
    "- Remove pairs where transliteration or English is too short/long\n",
    "- Discard pairs with extreme length ratios (likely misaligned)\n",
    "- Keep pairs with domain terms or high lexicon match\n",
    "- Sample results for sanity check\n",
    "- Output: `aligned_pairs_filtered.csv` ready for training augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaba540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ALIGNED_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n",
    "FILTERED_OUT_PATH = os.getenv('FILTERED_PAIRS_OUT', 'aligned_pairs_filtered.csv')\n",
    "\n",
    "def filter_quality(aligned_path: str, out_path: str):\n",
    "    \"\"\"Filter aligned pairs for training quality.\"\"\"\n",
    "    df = pd.read_csv(aligned_path)\n",
    "    print(f\"Loaded {len(df)} candidate pairs\")\n",
    "    \n",
    "    # Length filters\n",
    "    df['t_len'] = df['transliteration'].str.split().str.len()\n",
    "    df['e_len'] = df['english_sentence'].str.split().str.len()\n",
    "    \n",
    "    # Apply filters\n",
    "    df_filtered = df[\n",
    "        (df['t_len'] >= 3) & (df['t_len'] <= 150) &\n",
    "        (df['e_len'] >= 3) & (df['e_len'] <= 150) &\n",
    "        (df['t_len'] / (df['e_len'] + 1) >= 0.5) &\n",
    "        (df['t_len'] / (df['e_len'] + 1) <= 3.0)\n",
    "    ].copy()\n",
    "    \n",
    "    domain_terms = ['tablet', 'seal', 'silver', 'tin', 'letter', 'text', 'archive', 'merchant', 'trade']\n",
    "    df_filtered['has_domain'] = df_filtered['english_sentence'].str.lower().str.contains('|'.join(domain_terms), na=False)\n",
    "    \n",
    "    df_filtered[['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration']].to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"After quality filtering: {len(df_filtered)} pairs retained\")\n",
    "    print(f\"Saved to: {out_path}\\n\")\n",
    "    \n",
    "    print(\"Sample aligned pairs (first 5):\")\n",
    "    for i, row in df_filtered.head(5).iterrows():\n",
    "        print(f\"\\n[{i}]\")\n",
    "        print(f\"  EN: {row['english_sentence'][:80]}...\")\n",
    "        print(f\"  AK: {row['transliteration'][:80]}...\")\n",
    "    \n",
    "    return len(df_filtered)\n",
    "\n",
    "count = filter_quality(ALIGNED_PATH, FILTERED_OUT_PATH)\n",
    "print(f\"\\n‚úì Quality filtering complete. {count} high-quality pairs ready for training augmentation.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9082937,
     "sourceId": 14236819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 650.176672,
   "end_time": "2025-12-25T11:19:18.784764",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T11:08:28.608092",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "006e704aa6c944859bb0012667971140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0da07a92ee1145c6ba75f73034a80896": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f40ba21fc344bfeb6fc29b276337ca9",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_782121dc5bf5409787b423d190a97a2a",
       "tabbable": null,
       "tooltip": null,
       "value": "Map:‚Äá100%"
      }
     },
     "0f40ba21fc344bfeb6fc29b276337ca9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a7c757198d04482967f172c44c07168": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d1e2d02d3994b8197c937fe0cd36768": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e6586f643174c608f4af4c3fa7cc77b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e9adeda3aad4a82b53cc784c141c0cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e127272439614b07988bebdd9358f449",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_a649a617000146cc901c681c0d541f1c",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá8.15k/?‚Äá[00:00&lt;00:00,‚Äá851kB/s]"
      }
     },
     "1f09169e8dde4a9db3cf0afe7543792b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "23f123af2ed94e98ba76c3a0886fe02b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bb984c93171c4674b03816e7b09e0147",
        "IPY_MODEL_c78541bb1cc042689e463e891c1676f9",
        "IPY_MODEL_37ad3d25de25423aa68dd0c2f185de10"
       ],
       "layout": "IPY_MODEL_1a7c757198d04482967f172c44c07168",
       "tabbable": null,
       "tooltip": null
      }
     },
     "276b62defb65456b92ddf9b0470f142e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28a277a8fb104456a74d691689c840b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7359401f7fa644938932f706306c8ab3",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6a4867292379424b83eca085157110c6",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "2b2198dbdcb74158b34bf460b63dd7cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "37ad3d25de25423aa68dd0c2f185de10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e5f6bd19bbb34b569bb667df055db4ae",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_006e704aa6c944859bb0012667971140",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá1452/1452‚Äá[00:03&lt;00:00,‚Äá476.19‚Äáexamples/s]"
      }
     },
     "467e02d583c54e7fb140d85cf10d28a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66a1a97c09324c50964b24bd04a2fc5c",
        "IPY_MODEL_28a277a8fb104456a74d691689c840b8",
        "IPY_MODEL_1e9adeda3aad4a82b53cc784c141c0cb"
       ],
       "layout": "IPY_MODEL_865eb4fb87fe413db56a281d5e7bdc70",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4dc297123ac24df1b75b584d9c7f701a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52f062255cd941e3bb5d9b9e9616c805": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bd7545197a1349deaa902dfd002c54cb",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_940b13d03342464b968e822ed411db2c",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá9.01k/?‚Äá[00:00&lt;00:00,‚Äá1.02MB/s]"
      }
     },
     "54ac63530acd4a7ea511996d18038415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c9b8c2b47124449b80f89fc38d88cc18",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b71c04220ae84ed398f80345103403af",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "63f470b1c69a4b659c397a9131a1787e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66a1a97c09324c50964b24bd04a2fc5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e3ff9f8522974d489a6602947050b1f5",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_83b4199739ff4dbfa7fd6045b6592888",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
      }
     },
     "6a4867292379424b83eca085157110c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7359401f7fa644938932f706306c8ab3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "782121dc5bf5409787b423d190a97a2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "83b4199739ff4dbfa7fd6045b6592888": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "865eb4fb87fe413db56a281d5e7bdc70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "940b13d03342464b968e822ed411db2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "96980b824d484b4bb658d91f3181f000": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0da07a92ee1145c6ba75f73034a80896",
        "IPY_MODEL_fc47b1bb981d45e0b731240619bbe710",
        "IPY_MODEL_b8f18ea2ddb64b4c8db2e1e4f2fa45b4"
       ],
       "layout": "IPY_MODEL_1d1e2d02d3994b8197c937fe0cd36768",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a649a617000146cc901c681c0d541f1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b71c04220ae84ed398f80345103403af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b81ee50a854d42cbb085b2d6ed88f651": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dece704073e744fcb2777ac3e8d7a2f9",
        "IPY_MODEL_54ac63530acd4a7ea511996d18038415",
        "IPY_MODEL_52f062255cd941e3bb5d9b9e9616c805"
       ],
       "layout": "IPY_MODEL_4dc297123ac24df1b75b584d9c7f701a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b8f18ea2ddb64b4c8db2e1e4f2fa45b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1e6586f643174c608f4af4c3fa7cc77b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_2b2198dbdcb74158b34bf460b63dd7cb",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá77/77‚Äá[00:00&lt;00:00,‚Äá461.19‚Äáexamples/s]"
      }
     },
     "bb984c93171c4674b03816e7b09e0147": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be798dbd13b14c2fb91107dfb5f8d54f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_fde531abd1984d699ff0f100cd4e39ec",
       "tabbable": null,
       "tooltip": null,
       "value": "Map:‚Äá100%"
      }
     },
     "bd7545197a1349deaa902dfd002c54cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be798dbd13b14c2fb91107dfb5f8d54f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c78541bb1cc042689e463e891c1676f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_63f470b1c69a4b659c397a9131a1787e",
       "max": 1452,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e489c3a4e6184a24b04e1fb646771f00",
       "tabbable": null,
       "tooltip": null,
       "value": 1452
      }
     },
     "c9b8c2b47124449b80f89fc38d88cc18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "d0db33fadcf1494b976a13cb382b0610": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da06d7bfd43b40f5af5ad66daef05398": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dece704073e744fcb2777ac3e8d7a2f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1f09169e8dde4a9db3cf0afe7543792b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_d0db33fadcf1494b976a13cb382b0610",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
      }
     },
     "e127272439614b07988bebdd9358f449": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3ff9f8522974d489a6602947050b1f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e489c3a4e6184a24b04e1fb646771f00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e5f6bd19bbb34b569bb667df055db4ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc47b1bb981d45e0b731240619bbe710": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_276b62defb65456b92ddf9b0470f142e",
       "max": 77,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_da06d7bfd43b40f5af5ad66daef05398",
       "tabbable": null,
       "tooltip": null,
       "value": 77
      }
     },
     "fde531abd1984d699ff0f100cd4e39ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
