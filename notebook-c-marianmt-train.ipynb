{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":121150,"databundleVersionId":14976537,"sourceType":"competition"},{"sourceId":14236819,"sourceType":"datasetVersion","datasetId":9082937}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# C1. Imports & Configuration","metadata":{}},{"cell_type":"code","source":"pip install sacremoses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    DataCollatorForSeq2Seq, \n    Seq2SeqTrainer, \n    Seq2SeqTrainingArguments\n)\n\n# --- Configuration ---\n# Path provided by you for Notebook C\nMODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/opus-mt-mul-en\"\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nOUTPUT_DIR = \"./marian-mt-saved\"\n\n# MarianMT is optimized for sentence-level translation.\n# 128 tokens is usually plenty for a single Akkadian sentence.\nMAX_LENGTH = 160 \n\n# MarianMT does not strictly require a prefix, but adding one can help alignment.\n# We will use an empty prefix here as the model is already aligned for mul->en.\nPREFIX = \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T17:20:46.385450Z","iopub.execute_input":"2025-12-20T17:20:46.385959Z","iopub.status.idle":"2025-12-20T17:21:16.006482Z","shell.execute_reply.started":"2025-12-20T17:20:46.385925Z","shell.execute_reply":"2025-12-20T17:21:16.005820Z"}},"outputs":[{"name":"stderr","text":"2025-12-20 17:21:00.834252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766251261.017805      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766251261.070683      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766251261.512574      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766251261.512610      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766251261.512613      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766251261.512615      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# C2.Data Loading & Alignment","metadata":{}},{"cell_type":"code","source":"def load_and_align_data(filepath):\n    \"\"\"\n    Aligns Akkadian transliterations to English translations.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    aligned_rows = []\n\n    print(f\"Raw documents: {len(df)}\")\n\n    for _, row in df.iterrows():\n        src = str(row[\"transliteration\"]).strip()\n        tgt = str(row[\"translation\"]).strip()\n\n        # Split source by newlines\n        src_lines = [s.strip() for s in src.split(\"\\n\") if len(s.strip()) > 1]\n        \n        # Split target by sentence punctuation\n        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if len(t.strip()) > 1]\n\n        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n            for s, t in zip(src_lines, tgt_sents):\n                aligned_rows.append({\"src\": s, \"tgt\": t})\n        else:\n            aligned_rows.append({\"src\": src.replace(\"\\n\", \" \"), \"tgt\": tgt})\n\n    print(f\"Aligned training examples: {len(aligned_rows)}\")\n    return pd.DataFrame(aligned_rows)\n\n# Load and Split Data\ndf = load_and_align_data(f\"{DATA_DIR}/train.csv\")\ndataset = Dataset.from_pandas(df)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T17:21:16.007900Z","iopub.execute_input":"2025-12-20T17:21:16.008486Z","iopub.status.idle":"2025-12-20T17:21:16.179877Z","shell.execute_reply.started":"2025-12-20T17:21:16.008459Z","shell.execute_reply":"2025-12-20T17:21:16.179101Z"}},"outputs":[{"name":"stdout","text":"Raw documents: 1561\nAligned training examples: 1561\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# C3. Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndef preprocess_function(examples):\n    inputs = [ex for ex in examples[\"src\"]]\n    targets = examples[\"tgt\"]\n\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\"\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, \n            max_length=MAX_LENGTH, \n            truncation=True, \n            padding=\"max_length\"\n        )\n\n    # Replace padding token id with -100\n    model_inputs[\"labels\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n        for label in labels[\"input_ids\"]\n    ]\n    return model_inputs\n\n# Apply processing\ntokenized_train = dataset[\"train\"].map(preprocess_function, batched=True)\ntokenized_val = dataset[\"test\"].map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T17:21:16.180818Z","iopub.execute_input":"2025-12-20T17:21:16.181066Z","iopub.status.idle":"2025-12-20T17:21:20.448690Z","shell.execute_reply.started":"2025-12-20T17:21:16.181045Z","shell.execute_reply":"2025-12-20T17:21:20.447949Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1482 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fc7a90b9dd9492dbf0634a976c85e4e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/79 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa71162c0de046d08e360d4a2f7f6fce"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# C4. Model Setup","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    label_pad_token_id=-100\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T17:21:20.449697Z","iopub.execute_input":"2025-12-20T17:21:20.450079Z","iopub.status.idle":"2025-12-20T17:21:21.904791Z","shell.execute_reply.started":"2025-12-20T17:21:20.450051Z","shell.execute_reply":"2025-12-20T17:21:21.903974Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# C5. Training Configuration","metadata":{}},{"cell_type":"code","source":"# --- CORRECTED C5. Training Configuration (Disk Space Safe) ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    \n    # --- DISK SPACE FIXES ---\n    save_strategy=\"no\",           # Do NOT save checkpoints during training\n    eval_strategy=\"epoch\",        # Evaluate every epoch\n    load_best_model_at_end=False, # Must be False if we aren't saving checkpoints\n    # ------------------------\n    \n    learning_rate=2e-5, \n    \n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n\n    logging_steps=10, #added by me\n    \n    num_train_epochs=13, #increased to 13 from 7\n    weight_decay=0.01,\n    predict_with_generate=True,\n    \n    fp16=True, # MarianMT is safe with fp16\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# C6. Execution","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"Starting MarianMT Training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T17:21:22.071031Z","iopub.execute_input":"2025-12-20T17:21:22.071350Z","execution_failed":"2025-12-20T17:22:58.965Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/148008205.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n","output_type":"stream"},{"name":"stdout","text":"Starting MarianMT Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='333' max='651' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [333/651 01:32 < 01:29, 3.57 it/s, Epoch 3.57/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.983993</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>2.588560</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>2.435226</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# C7. Save Model","metadata":{}},{"cell_type":"code","source":"print(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Notebook C (MarianMT) Complete.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-20T17:22:58.965Z"}},"outputs":[],"execution_count":null}]}