{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-25T12:14:36.175832Z",
     "iopub.status.busy": "2025-12-25T12:14:36.175605Z",
     "iopub.status.idle": "2025-12-25T12:15:40.422643Z",
     "shell.execute_reply": "2025-12-25T12:15:40.422026Z",
     "shell.execute_reply.started": "2025-12-25T12:14:36.175811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Deep Past Neural Ensemble Inference ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 12:14:51.095337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766664891.298313      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766664891.357816      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766664891.851037      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851082      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851085      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851088      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286a0586afeb435ea31d31b4777fc8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference byt5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453f6b3dd3304ef99924faf6e3a6a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference t5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9f6a77e75d47d4adc7de4402091a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference marian:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview:\n",
      "   id                                        translation\n",
      "0   0  Kà-ar-ma ú big_gap da-tim aí-ip-ri-ni Akkadian...\n",
      "1   1  -ni i-na né-mì-lim da-aùr ú-lá e-WA ia-ra-tí-a...\n",
      "2   2  -it a-aí-im au-um-au ia-tí aé-bi„-lá-nim Trans...\n",
      "3   3  É-bi„-lá KÙ. AN Translate Akkadian to English:...\n"
     ]
    }
   ],
   "source": [
    "# DEEP PAST CHALLENGE - ENSEMBLE SUBMISSION\n",
    "\n",
    "# Install sacremoses for MarianMT (required in offline mode)\n",
    "try:\n",
    "    import sacremoses\n",
    "    print(\"✓ sacremoses already installed\")\n",
    "except ImportError:\n",
    "    import os\n",
    "    wheel_dir = \"/kaggle/input/sacremoses-wheel\"\n",
    "    wheel_glob = \"sacremoses-*.whl\"\n",
    "    if os.path.exists(wheel_dir):\n",
    "        print(\"Installing sacremoses from local wheel...\")\n",
    "        import glob, sys, subprocess\n",
    "        wheels = glob.glob(f\"{wheel_dir}/{wheel_glob}\")\n",
    "        if wheels:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheels[0], \"--no-deps\", \"-q\"])\n",
    "            import sacremoses\n",
    "            print(\"✓ sacremoses installed from wheel\")\n",
    "        else:\n",
    "            print(\"⚠️ No sacremoses wheel found in dataset folder\")\n",
    "    else:\n",
    "        print(\"⚠️ sacremoses not found - MarianMT may fail\")\n",
    "        print(\"To fix: Upload sacremoses wheel as dataset and run:\")\n",
    "        print(\"!pip install /kaggle/input/sacremoses-wheel/sacremoses-*.whl --no-deps\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL1_PATH = os.getenv(\"MODEL1_PATH\", \"/kaggle/input/notebook-a-byt5/byt5-base-saved\")\n",
    "MODEL2_PATH = os.getenv(\"MODEL2_PATH\", \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\")\n",
    "MODEL3_PATH = os.getenv(\"MODEL3_PATH\", \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\")\n",
    "\n",
    "TEST_DATA_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENSEMBLE STRATEGY: AUTO-DETECT\n",
    "# -----------------------------------------------------------------------------\n",
    "# This notebook automatically chooses the best strategy:\n",
    "# 1. Weight averaging if all models have same architecture\n",
    "# 2. Voting ensemble if models have different architectures\n",
    "# 3. Best single model as fallback\n",
    "\n",
    "ENSEMBLE_MODE = os.getenv(\"ENSEMBLE_MODE\", \"auto\")  # \"auto\", \"voting\", or \"averaging\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODEL CONFIGURATIONS & WEIGHTS\n",
    "# -----------------------------------------------------------------------------\n",
    "# Adjust these weights based on validation scores from find-optimal-weights.ipynb\n",
    "# Or use equal weights as baseline: {\"weight\": 0.333}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"byt5\": {\n",
    "        \"path\": MODEL1_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.35,  # Adjust based on validation\n",
    "        \"num_beams\": 4\n",
    "    },\n",
    "    \"t5\": {\n",
    "        \"path\": MODEL2_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.40,  # Usually best performer\n",
    "        \"num_beams\": 4\n",
    "    },\n",
    "    \"marian\": {\n",
    "        \"path\": MODEL3_PATH,\n",
    "        \"prefix\": \">>eng<< \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.25,  # Adjust based on validation\n",
    "        \"num_beams\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Ensemble Mode: {ENSEMBLE_MODE}\")\n",
    "print(f\"\\nModel Weights:\")\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    exists = \"✓\" if os.path.exists(config[\"path\"]) else \"✗\"\n",
    "    print(f\"  {name:10s} {exists} weight={config['weight']:.2f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GAP REPLACEMENT FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def replace_gaps(text):\n",
    "    \"\"\"Replace various gap notations with standardized tokens\"\"\"\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    \n",
    "    # Complex gap patterns (order matters)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    # Simple gap patterns\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "    text = re.sub(r'……', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "    text = re.sub(r'…', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD TEST DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "test_df['transliteration'] = test_df['transliteration'].apply(replace_gaps)\n",
    "test_inputs = test_df['transliteration'].astype(str).tolist()\n",
    "source_lengths = [len(t.split()) for t in test_inputs]\n",
    "\n",
    "print(f\"✓ Loaded {len(test_df)} test samples\")\n",
    "print(f\"✓ Average source length: {sum(source_lengths)/len(source_lengths):.1f} words\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# INFERENCE DATASET\n",
    "# -----------------------------------------------------------------------------\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length, prefix=\"\"):\n",
    "        self.texts = [prefix + str(t) for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override per-model generation configs to match training lengths and improve quality\n",
    "MODEL_CONFIGS['byt5']['max_length'] = 256\n",
    "MODEL_CONFIGS['t5']['max_length'] = 128\n",
    "MODEL_CONFIGS['marian']['max_length'] = 160\n",
    "for k in MODEL_CONFIGS.keys():\n",
    "    MODEL_CONFIGS[k]['num_beams'] = 6\n",
    "print(\"Adjusted MODEL_CONFIGS: max_length per model and num_beams=6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION METRIC: Geometric Mean of BLEU and chrF++\n",
    "\n",
    "## Deep Past Challenge Scoring\n",
    "\n",
    "The competition uses the **Geometric Mean** of BLEU and chrF++ scores:\n",
    "\n",
    "### Formula\n",
    "```\n",
    "Score = √(BLEU × chrF++)\n",
    "```\n",
    "\n",
    "### Why Geometric Mean?\n",
    "- **BLEU**: Measures n-gram precision (word choice accuracy)\n",
    "- **chrF++**: Measures character F-score (word order and morphology)\n",
    "- **Geometric Mean**: Balances both without favoring one\n",
    "\n",
    "### Score Range\n",
    "- 0-100 (higher is better)\n",
    "- Typical strong submission: 25-35\n",
    "- Excellent submission: 35+\n",
    "\n",
    "### What This Means\n",
    "- Both BLEU and chrF++ matter equally\n",
    "- Can't just optimize for one metric\n",
    "- Morphologically complex Akkadian benefits from chrF++ focus\n",
    "- Use this metric for validation in training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ENSEMBLE GENERATION & SUBMISSION\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING MODELS FOR ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    model_path = config[\"path\"]\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️  WARNING: {name} model not found at {model_path}\")\n",
    "        print(f\"   This model will be skipped\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {name} from {model_path}...\")\n",
    "        tokenizers[name] = AutoTokenizer.from_pretrained(model_path)\n",
    "        models[name] = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE)\n",
    "        models[name].eval()\n",
    "        print(f\"✓ {name} loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {name}: {e}\")\n",
    "\n",
    "if not models:\n",
    "    print(\"❌ ERROR: No models loaded! Cannot proceed with ensemble.\")\n",
    "    print(\"Please ensure model paths are correct in MODEL_CONFIGS.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\n✓ Successfully loaded {len(models)} models for ensemble\")\n",
    "\n",
    "# Normalize weights to sum to 1.0 for loaded models\n",
    "total_weight = sum(MODEL_CONFIGS[name][\"weight\"] for name in models.keys())\n",
    "for name in models.keys():\n",
    "    MODEL_CONFIGS[name][\"weight\"] /= total_weight\n",
    "    print(f\"  {name}: weight = {MODEL_CONFIGS[name]['weight']:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENSEMBLE GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_translations(texts, model_name, config):\n",
    "    \"\"\"Generate translations using a single model\"\"\"\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=f\"Generating with {model_name}\"):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        batch_inputs = [config[\"prefix\"] + str(t) for t in batch]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_inputs,\n",
    "            max_length=config[\"max_length\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=config[\"max_length\"],\n",
    "                num_beams=config[\"num_beams\"],\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.0,\n",
    "                temperature=1.0\n",
    "            )\n",
    "        \n",
    "        batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING TRANSLATIONS WITH EACH MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_predictions = {}\n",
    "\n",
    "for model_name in models.keys():\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    predictions = generate_translations(test_inputs, model_name, config)\n",
    "    all_predictions[model_name] = predictions\n",
    "    print(f\"  ✓ Generated {len(predictions)} predictions\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENSEMBLE VOTING / BLENDING\n",
    "# ============================================================================\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for comparison\"\"\"\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def postprocess_prediction(text):\n",
    "    \"\"\"Clean up prediction\"\"\"\n",
    "    text = str(text).strip()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Capitalize first letter\n",
    "    if text and text[0].islower():\n",
    "        text = text[0].upper() + text[1:]\n",
    "    # Add period if missing\n",
    "    if text and text[-1] not in '.!?':\n",
    "        text += '.'\n",
    "    return text\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE: VOTING STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_predictions = []\n",
    "\n",
    "for idx, input_text in enumerate(test_inputs):\n",
    "    # Get predictions from all models\n",
    "    model_preds = {}\n",
    "    for model_name in all_predictions.keys():\n",
    "        pred = all_predictions[model_name][idx]\n",
    "        model_preds[model_name] = postprocess_prediction(pred)\n",
    "    \n",
    "    if ENSEMBLE_MODE == \"averaging\":\n",
    "        # For text, we can't average, so we use the highest-weight model's prediction\n",
    "        best_model = max(all_predictions.keys(), key=lambda m: MODEL_CONFIGS[m][\"weight\"])\n",
    "        final_pred = model_preds[best_model]\n",
    "    else:\n",
    "        # Voting: weighted voting on tokens\n",
    "        words_freq = {}\n",
    "        for model_name, pred in model_preds.items():\n",
    "            weight = MODEL_CONFIGS[model_name][\"weight\"]\n",
    "            words = pred.split()\n",
    "            for word in words:\n",
    "                words_freq[word] = words_freq.get(word, 0) + weight\n",
    "        \n",
    "        if words_freq:\n",
    "            # Sort by frequency and weight\n",
    "            sorted_words = sorted(words_freq.items(), key=lambda x: -x[1])\n",
    "            # Use top predictions\n",
    "            num_words = max(5, min(20, len(sorted_words) // 3))\n",
    "            top_words = [w for w, _ in sorted_words[:num_words]]\n",
    "            final_pred = \" \".join(top_words) if top_words else model_preds[list(model_preds.keys())[0]]\n",
    "        else:\n",
    "            final_pred = model_preds[list(model_preds.keys())[0]]\n",
    "    \n",
    "    final_predictions.append(final_pred)\n",
    "    \n",
    "    if idx < 3:\n",
    "        print(f\"\\nExample {idx}:\")\n",
    "        print(f\"  Input: {input_text[:50]}...\")\n",
    "        for model_name, pred in model_preds.items():\n",
    "            print(f\"  {model_name:8s}: {pred[:60]}...\")\n",
    "        print(f\"  Ensemble: {final_pred[:60]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE SUBMISSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SUBMISSION FILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission_df = test_df.copy()\n",
    "submission_df[\"translation\"] = final_predictions\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"✓ Submission saved to submission.csv\")\n",
    "print(f\"  Total predictions: {len(submission_df)}\")\n",
    "print(f\"  Columns: {submission_df.columns.tolist()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Clean up memory\n",
    "print(\"\\nCleaning up GPU memory...\")\n",
    "for model in models.values():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"✓ Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION NOTES: Evaluation Metrics & Data Handling\n",
    "\n",
    "## Evaluation Metrics (Deep Past Challenge)\n",
    "\n",
    "Your submission is scored using the **Geometric Mean** of two metrics:\n",
    "\n",
    "### BLEU Score\n",
    "- Measures n-gram precision (matches between prediction and reference)\n",
    "- Range: 0-100\n",
    "- Formula: Precision of unigrams, bigrams, trigrams, 4-grams with brevity penalty\n",
    "- Focuses on: Exact word choices and common phrases\n",
    "\n",
    "### chrF++ Score  \n",
    "- Measures character-level F-score\n",
    "- Range: 0-100\n",
    "- Considers: Character and word order accuracy\n",
    "- Focuses on: Morphological correctness (important for Akkadian!)\n",
    "\n",
    "### Geometric Mean = √(BLEU × chrF++)\n",
    "- Balances both metrics equally\n",
    "- Typical score: 20-40\n",
    "- Excellent score: 35+\n",
    "\n",
    "## Data Preprocessing Checklist\n",
    "\n",
    "All training notebooks handle these formatting issues:\n",
    "\n",
    "✓ **Subscripts/Superscripts**: a₂ → a2, il₅ → il5\n",
    "✓ **Gaps & Lacunae**: [x] → <gap>, … → <big_gap>\n",
    "✓ **Scribal Notations**: Remove !, ?, /, :, etc.\n",
    "✓ **Bracket Content**: Keep content, remove brackets\n",
    "✓ **Determinatives**: {d} → d (extract content)\n",
    "✓ **Special Characters**: Preserve š, Š, ṣ, Ṣ, ṭ, Ṭ, ḫ, Ḫ, ʾ\n",
    "✓ **Capitalization**: Preserve (proper nouns & logograms)\n",
    "✓ **Quality Filtering**: Length validation, no duplicates\n",
    "\n",
    "## How Ensemble Works\n",
    "\n",
    "This notebook combines 3 models:\n",
    "1. **ByT5** - Character-level (good for morphology)\n",
    "2. **T5** - Token-level (good for semantics)\n",
    "3. **MarianMT** - Translation-specific (good for fluency)\n",
    "\n",
    "Each model votes with weighted predictions → final ensemble translation"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9084006,
     "sourceId": 14290093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084005,
     "sourceId": 14290098,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084224,
     "sourceId": 14290100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
