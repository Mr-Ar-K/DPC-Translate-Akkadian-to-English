{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a4c494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:08.388312Z",
     "iopub.status.busy": "2026-01-08T08:08:08.387988Z",
     "iopub.status.idle": "2026-01-08T08:08:08.619248Z",
     "shell.execute_reply": "2026-01-08T08:08:08.616178Z",
     "shell.execute_reply.started": "2026-01-08T08:08:08.388289Z"
    },
    "papermill": {
     "duration": 0.184686,
     "end_time": "2025-12-25T10:28:15.031482",
     "exception": false,
     "start_time": "2025-12-25T10:28:14.846796",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  8 08:08:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853653de",
   "metadata": {
    "papermill": {
     "duration": 0.006431,
     "end_time": "2025-12-25T10:28:15.044668",
     "exception": false,
     "start_time": "2025-12-25T10:28:15.038237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964c2326",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:08.621873Z",
     "iopub.status.busy": "2026-01-08T08:08:08.621507Z",
     "iopub.status.idle": "2026-01-08T08:08:13.845850Z",
     "shell.execute_reply": "2026-01-08T08:08:13.844989Z",
     "shell.execute_reply.started": "2026-01-08T08:08:08.621830Z"
    },
    "papermill": {
     "duration": 4.824313,
     "end_time": "2025-12-25T10:28:19.875330",
     "exception": false,
     "start_time": "2025-12-25T10:28:15.051017",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5f368",
   "metadata": {
    "papermill": {
     "duration": 0.006597,
     "end_time": "2025-12-25T10:28:19.889807",
     "exception": false,
     "start_time": "2025-12-25T10:28:19.883210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A2. Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80cf0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:13.847573Z",
     "iopub.status.busy": "2026-01-08T08:08:13.847199Z",
     "iopub.status.idle": "2026-01-08T08:08:45.115144Z",
     "shell.execute_reply": "2026-01-08T08:08:45.114538Z",
     "shell.execute_reply.started": "2026-01-08T08:08:13.847522Z"
    },
    "papermill": {
     "duration": 33.038891,
     "end_time": "2025-12-25T10:28:52.935206",
     "exception": false,
     "start_time": "2025-12-25T10:28:19.896315",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 08:08:28.427385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767859708.621266      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767859708.681230      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767859709.163048      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767859709.163087      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767859709.163090      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767859709.163093      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Memory/precision safety tweaks (helps avoid OOM on P100/T4)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e3a6c",
   "metadata": {
    "papermill": {
     "duration": 0.006759,
     "end_time": "2025-12-25T10:28:52.949032",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.942273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A3. Set constants (DO NOT change yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9780c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:45.116661Z",
     "iopub.status.busy": "2026-01-08T08:08:45.116044Z",
     "iopub.status.idle": "2026-01-08T08:08:45.121485Z",
     "shell.execute_reply": "2026-01-08T08:08:45.120695Z",
     "shell.execute_reply.started": "2026-01-08T08:08:45.116634Z"
    },
    "language": "python",
    "papermill": {
     "duration": 0.013576,
     "end_time": "2025-12-25T10:28:52.969057",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.955481",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIGURATION: THE PURIST ===\n",
    "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "OUTPUT_DIR = \"/kaggle/working/byt5-base-saved\"\n",
    "\n",
    "ENABLE_MONO_PRETRAIN = True   # <--- CRITICAL: Keeps the \"Purist\" logic\n",
    "MAX_LENGTH = 300              # Reduced from 512 for ByT5 speed/memory\n",
    "PREFIX = \"translate Akkadian to English: \"\n",
    "\n",
    "BATCH_SIZE = 4                # Adjust based on GPU memory\n",
    "GRAD_ACCUM = 8                # Gradient accumulation steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041d778",
   "metadata": {
    "papermill": {
     "duration": 0.006442,
     "end_time": "2025-12-25T10:28:52.982062",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.975620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A4. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6077f",
   "metadata": {},
   "source": [
    "# A3.5. DATA PREPARATION GUIDE: Handling Akkadian Formatting Issues\n",
    "\n",
    "## Problem: \"Garbage In, Garbage Out\"\n",
    "Akkadian texts contain complex formatting that can break ML pipelines if not handled properly.\n",
    "\n",
    "## Formatting Issues to Handle\n",
    "\n",
    "### 1. Scribal Notations (Remove)\n",
    "- `!` - Certain reading (remove)\n",
    "- `?` - Questionable reading (remove)\n",
    "- `/` - Line divider (remove)\n",
    "- `:` or `.` - Word divider (remove)\n",
    "- `< >` - Scribal insertions (keep content, remove brackets)\n",
    "- `( )` - Comments/erasures (remove entirely)\n",
    "- `˹ ˺` - Half brackets for partially broken signs (remove)\n",
    "- `[ ]` - Clearly broken signs (keep content, remove brackets)\n",
    "- `<< >>` - Errant signs (remove entirely)\n",
    "\n",
    "### 2. Gaps & Lacunae (Standardize)\n",
    "- `[x]` → `<gap>`\n",
    "- `x` → `<gap>`\n",
    "- `xx` → `<gap>`\n",
    "- `…` → `<big_gap>`\n",
    "- `……` → `<big_gap>`\n",
    "- `[... ...]` → `<big_gap>`\n",
    "- Multiple `.3` or `...` sequences → `<big_gap>`\n",
    "\n",
    "### 3. Determinatives (Keep content, remove brackets)\n",
    "- `{d}` - Deity (remove brackets)\n",
    "- `{ki}` - Earth/location (remove brackets)\n",
    "- `{lu₂}` - Person (remove brackets)\n",
    "- `{e₂}` - Building (remove brackets)\n",
    "- And 10+ others...\n",
    "\n",
    "### 4. Subscripts & Superscripts (Normalize)\n",
    "- `a₂` → `a2`, `a₃` → `a3`, etc.\n",
    "- `il₅` → `il5`, etc.\n",
    "- Works with Unicode characters (U+2080-U+2089)\n",
    "\n",
    "### 5. Special Characters (Handle as-is or normalize)\n",
    "- `š` (U+0161), `Š` (U+0160)\n",
    "- `ṣ` (U+1E63), `Ṣ` (U+1E62)\n",
    "- `ṭ` (U+1E6D), `Ṭ` (U+1E6C)\n",
    "- `ḫ` (U+1E2B), `Ḫ` (U+1E2A)\n",
    "- `ʾ` (U+02BE) - Akkadian letter marker\n",
    "\n",
    "### 6. Capitalization Rules (Preserve)\n",
    "- First letter capital = Proper noun (personal/place name)\n",
    "- ALL CAPS = Sumerian logogram (preserve for domain knowledge)\n",
    "\n",
    "## Processing Order\n",
    "1. Normalize subscripts FIRST (₀-₉ → 0-9)\n",
    "2. Handle gaps (complex patterns first, then simple)\n",
    "3. Remove scribal notations\n",
    "4. Extract content from bracketed structures\n",
    "5. Clean whitespace\n",
    "6. Validate output (length checks, character validation)\n",
    "\n",
    "## Data Validation Checks\n",
    "✓ No empty strings after cleaning\n",
    "✓ Source length >= 3 words\n",
    "✓ Target length >= 3 words\n",
    "✓ Length ratio between 0.2 and 5.0\n",
    "✓ No duplicate pairs\n",
    "✓ All special characters properly handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035f86f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:45.123948Z",
     "iopub.status.busy": "2026-01-08T08:08:45.123665Z",
     "iopub.status.idle": "2026-01-08T08:09:40.314471Z",
     "shell.execute_reply": "2026-01-08T08:09:40.313446Z",
     "shell.execute_reply.started": "2026-01-08T08:08:45.123915Z"
    },
    "papermill": {
     "duration": 0.469755,
     "end_time": "2025-12-25T10:28:53.458249",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.988494",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw documents: 1561\n",
      "✓ Loading sentence alignment data...\n",
      "Building aligned dataset using sentence map translations...\n",
      "✓ Extracted 51 sentence pairs from map file\n",
      "Aligned training examples (pre-filter): 1561\n",
      "Aligned training examples (post-filter): 1528\n",
      "\n",
      "============================================================\n",
      "MINING PUBLICATIONS FOR ADDITIONAL TRAINING DATA (FAST MODE)\n",
      "============================================================\n",
      "Total publication pages: 216602\n",
      "Pages with translation keywords: 12500\n",
      "Searching for matches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc564c056d4d4614a826ef142aac4f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mining:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  No additional pairs extracted (try adjusting regex or increasing candidates)\n",
      "\n",
      "============================================================\n",
      "CHECKING PUBLISHED TEXTS\n",
      "============================================================\n",
      "Published texts available: 7953\n",
      "Note: Will use these for monolingual pre-training\n",
      "\n",
      "Final dataset:\n",
      "  Train: 1451 examples\n",
      "  Validation: 77 examples\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMPREHENSIVE DATA PREPROCESSING FOR AKKADIAN TEXTS\n",
    "Handles all formatting issues mentioned in competition guidelines\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUBSCRIPT & SUPERSCRIPT NORMALIZATION\n",
    "# ============================================================================\n",
    "SUBSCRIPT_TRANS = str.maketrans({\n",
    "    \"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \n",
    "    \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \n",
    "    \"ₓ\": \"x\"\n",
    "})\n",
    "\n",
    "def normalize_subscripts(text: str) -> str:\n",
    "    \"\"\"Convert subscript Unicode characters to regular numbers\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return text.translate(SUBSCRIPT_TRANS)\n",
    "\n",
    "# ============================================================================\n",
    "# GAP & LACUNAE HANDLING\n",
    "# ============================================================================\n",
    "def replace_gaps(text, keep_gaps=True):\n",
    "    \"\"\"\n",
    "    Replace various gap notations with standardized tokens.\n",
    "    Handles all gap patterns mentioned in competition guidelines.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text with gaps\n",
    "        keep_gaps: If True, keeps <gap> and <big_gap> tokens.\n",
    "                  If False, removes them completely.\n",
    "    \n",
    "    Returns:\n",
    "        Text with normalized gap tokens\n",
    "    \"\"\"\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    \n",
    "    # STEP 1: Complex gap patterns (order matters!)\n",
    "    # [...] patterns for multiple dots\n",
    "    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\.\\s*\\.\\s*\\]', '<big_gap>', text)  # [......]\n",
    "    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\.\\s*\\]', '<big_gap>', text)       # [....]\n",
    "    text = re.sub(r'\\[\\s*\\.\\s*\\.\\s*\\]', '<gap>', text)                 # [...] \n",
    "    \n",
    "    # Multiple .3 patterns with multiple dots\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    \n",
    "    # Multiple dots (.....)\n",
    "    text = re.sub(r'\\.{4,}', '<big_gap>', text)  # 4+ dots = big gap\n",
    "    \n",
    "    # STEP 2: Unicode gap markers\n",
    "    text = re.sub(r'……', '<big_gap>', text)      # Unicode horizontal ellipsis\n",
    "    text = re.sub(r'…', '<big_gap>', text)        # Unicode single ellipsis\n",
    "    \n",
    "    # STEP 3: Standard dot patterns\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)  # Multiple ... groups\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)  # Three dots\n",
    "    text = re.sub(r'\\.\\.', '<gap>', text)        # Two dots\n",
    "    \n",
    "    # STEP 4: [x] and [xx] patterns\n",
    "    text = re.sub(r'\\[x+\\]', '<gap>', text)      # [x] or [xx]\n",
    "    \n",
    "    # STEP 5: Bare x patterns\n",
    "    text = re.sub(r'(?:^|\\s)xx(?:\\s|$)', ' <gap> ', text)  # xx as separate word\n",
    "    text = re.sub(r'(?:^|\\s)x(?:\\s|$)', ' <gap> ', text)   # x as separate word\n",
    "    \n",
    "    # STEP 6: Remove gaps if not needed\n",
    "    if not keep_gaps:\n",
    "        text = re.sub(r'<big_gap>', '', text)\n",
    "        text = re.sub(r'<gap>', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIBAL NOTATION REMOVAL\n",
    "# ============================================================================\n",
    "def remove_scribal_notations(text):\n",
    "    \"\"\"\n",
    "    Remove modern scribal notations that are not meaningful for translation.\n",
    "    These are editorial marks added by scholars, not part of the original text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove line number markers (1, 5, 10, 1', 1'')\n",
    "    text = re.sub(r'\\b\\d+\\'?\\s*\\'?\\s*\\b', ' ', text)\n",
    "    \n",
    "    # Remove uncertainty markers\n",
    "    text = re.sub(r'[!?]', ' ', text)  # ! = certain, ? = uncertain\n",
    "    \n",
    "    # Remove other scribal punctuation\n",
    "    text = re.sub(r'[/:·]', ' ', text)  # / = line divider, : = word divider, · = separator\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ============================================================================\n",
    "# BRACKETED CONTENT HANDLING\n",
    "# ============================================================================\n",
    "def handle_brackets(text):\n",
    "    \"\"\"\n",
    "    Handle various bracket types according to guidelines.\n",
    "    \n",
    "    - ( ) Remove entirely (comments/erasures)\n",
    "    - < > Keep content (scribal insertions)\n",
    "    - [ ] Keep content (clearly broken signs)\n",
    "    - { } Keep content (determinatives)\n",
    "    - << >> Remove entirely (errant signs)\n",
    "    - ˹ ˺ Remove (half brackets for partially broken)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove comments and erasures (keep nothing)\n",
    "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
    "    \n",
    "    # Keep content from scribal insertions and broken signs\n",
    "    text = re.sub(r'<([^>]*)>', r'\\1', text)      # <content> → content\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)   # [content] → content\n",
    "    \n",
    "    # Determinatives: {content} → content (removes classifier brackets)\n",
    "    text = re.sub(r'\\{([^}]*)\\}', r'\\1', text)\n",
    "    \n",
    "    # Remove half brackets for partially broken signs\n",
    "    text = re.sub(r'[˹˺]', ' ', text)\n",
    "    \n",
    "    # Remove errant/erroneous signs entirely\n",
    "    text = re.sub(r'<<[^>]*>>', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRANSLITERATION CLEANING FUNCTION\n",
    "# ============================================================================\n",
    "def clean_translit(text, keep_gaps=True):\n",
    "    \"\"\"\n",
    "    Comprehensive normalization of Akkadian transliteration.\n",
    "    Handles all formatting issues in proper order.\n",
    "    \n",
    "    Processing order:\n",
    "    1. Normalize subscripts\n",
    "    2. Handle gaps\n",
    "    3. Remove scribal notations\n",
    "    4. Handle bracket types\n",
    "    5. Clean whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # STEP 1: Normalize subscripts/superscripts FIRST\n",
    "    text = normalize_subscripts(text)\n",
    "    \n",
    "    # STEP 2: Handle gaps (complex patterns)\n",
    "    text = replace_gaps(text, keep_gaps=keep_gaps)\n",
    "    \n",
    "    # STEP 3: Remove scribal notations\n",
    "    text = remove_scribal_notations(text)\n",
    "    \n",
    "    # STEP 4: Handle all bracket types\n",
    "    text = handle_brackets(text)\n",
    "    \n",
    "    # STEP 5: Clean whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSLATION CLEANING FUNCTION\n",
    "# ============================================================================\n",
    "def clean_translation(text, has_gaps=False):\n",
    "    \"\"\"\n",
    "    Clean translation with minimal processing.\n",
    "    Keep as much content as possible.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Handle gap indicators if source has gaps\n",
    "    if not has_gaps:\n",
    "        text = text.replace(\"…\", \" \")\n",
    "    \n",
    "    # Clean whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# ============================================================================\n",
    "# DATA QUALITY FILTERING\n",
    "# ============================================================================\n",
    "def filter_quality(df):\n",
    "    \"\"\"\n",
    "    Filter out low-quality pairs based on validation checks.\n",
    "    \n",
    "    Validation criteria:\n",
    "    - Minimum 3 words in source and target\n",
    "    - Length ratio between 0.2 and 5.0\n",
    "    - No duplicate pairs\n",
    "    \"\"\"\n",
    "    # Calculate lengths\n",
    "    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n",
    "    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n",
    "    \n",
    "    # Minimum length check\n",
    "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
    "    \n",
    "    # Length ratio check (one language often longer than other)\n",
    "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
    "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
    "    \n",
    "    # Remove exact duplicates\n",
    "    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n",
    "    \n",
    "    # Cleanup\n",
    "    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION & REPORTING\n",
    "# ============================================================================\n",
    "def validate_preprocessing(original_df, cleaned_df):\n",
    "    \"\"\"\n",
    "    Report on preprocessing impact.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA PREPROCESSING VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original samples: {len(original_df)}\")\n",
    "    print(f\"After cleaning: {len(cleaned_df)}\")\n",
    "    print(f\"Removed: {len(original_df) - len(cleaned_df)} samples\")\n",
    "    \n",
    "    if len(cleaned_df) > 0:\n",
    "        avg_src = cleaned_df[\"transliteration\"].str.split().str.len().mean()\n",
    "        avg_tgt = cleaned_df[\"translation\"].str.split().str.len().mean()\n",
    "        print(f\"Avg source length: {avg_src:.1f} words\")\n",
    "        print(f\"Avg target length: {avg_tgt:.1f} words\")\n",
    "        print(f\"Avg ratio (src/tgt): {avg_src/avg_tgt:.2f}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Replace gaps function (with corrected newlines and indentation)\n",
    "def replace_gaps(text, keep_gaps=True):\n",
    "    \"\"\"Replace various gap notations with standardized tokens\n",
    "    \n",
    "    Args:\n",
    "        keep_gaps: If True, keeps gap tokens (for test-like data).\n",
    "                   If False, removes them (for clean training).\n",
    "    \"\"\"\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    \n",
    "    # Complex gap patterns (order matters)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    # Simple gap patterns\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "    text = re.sub(r'……', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "    text = re.sub(r'…', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "    \n",
    "    # Bracketed gaps\n",
    "    text = re.sub(r'\\[\\.\\.\\.+\\]', '<big_gap>', text)\n",
    "    text = re.sub(r'\\[x+\\]', '<gap>', text)\n",
    "    \n",
    "    if not keep_gaps:\n",
    "        # Remove gaps for clean training\n",
    "        text = re.sub(r'<big_gap>', '', text)\n",
    "        text = re.sub(r'<gap>', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_translit(text, keep_gaps=True):\n",
    "    \"\"\"Normalize transliteration following competition guidance.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = normalize_subscripts(text)\n",
    "    # Apply gap replacement - KEEP gaps for domain matching\n",
    "    text = replace_gaps(text, keep_gaps=keep_gaps)\n",
    "    # Only remove scribal markers, keep gaps\n",
    "    text = re.sub(r\"<<[^>]*>>\", \" \", text)               # errant signs\n",
    "    text = re.sub(r\"[˹˺]\", \" \", text)                    # half brackets\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)             # comments/erasures\n",
    "    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)         # determinatives\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)            # scribal insertions keep content\n",
    "    text = re.sub(r\"[!?/:·]\", \" \", text)                 # scribal punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text, has_gaps=False):\n",
    "    \"\"\"Clean translation, optionally keeping gap indicators\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    if not has_gaps:\n",
    "        text = text.replace(\"…\", \" \")\n",
    "    # Keep ... if source has gaps\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def filter_quality(df):\n",
    "    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n",
    "    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n",
    "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
    "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
    "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
    "    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n",
    "    return df.drop(columns=[\"src_len\", \"tgt_len\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e0f72",
   "metadata": {
    "papermill": {
     "duration": 0.006698,
     "end_time": "2025-12-25T10:28:53.471962",
     "exception": false,
     "start_time": "2025-12-25T10:28:53.465264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A5 . Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING & PREPROCESSING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_path = f\"{DATA_DIR}/train.csv\"\n",
    "print(f\"Loading data from: {train_path}\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "print(f\"Original dataset size: {len(train_df)}\")\n",
    "\n",
    "# Clean data\n",
    "train_df = train_df.dropna(subset=[\"transliteration\", \"translation\"])\n",
    "train_df[\"transliteration\"] = train_df[\"transliteration\"].astype(str).apply(clean_translit)\n",
    "train_df[\"translation\"] = train_df[\"translation\"].astype(str).apply(clean_translation)\n",
    "\n",
    "# Filter quality\n",
    "train_df = filter_quality(train_df)\n",
    "print(f\"After quality filtering: {len(train_df)}\")\n",
    "\n",
    "# Create train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = {\n",
    "    \"train\": Dataset.from_pandas(train_data[[\"transliteration\", \"translation\"]].reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(val_data[[\"transliteration\", \"translation\"]].reset_index(drop=True))\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Data loaded and preprocessed successfully\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a0a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:09:40.315996Z",
     "iopub.status.busy": "2026-01-08T08:09:40.315665Z",
     "iopub.status.idle": "2026-01-08T08:09:44.388152Z",
     "shell.execute_reply": "2026-01-08T08:09:44.387424Z",
     "shell.execute_reply.started": "2026-01-08T08:09:40.315966Z"
    },
    "language": "python",
    "papermill": {
     "duration": 3.528045,
     "end_time": "2025-12-25T10:28:57.006773",
     "exception": false,
     "start_time": "2025-12-25T10:28:53.478728",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999dfb221f974217845a0b5904c1b537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abd1a38cb6d47c299cb8c035d75b4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"Loading Tokenizer from:\", MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"transliteration\"]]\n",
    "    targets = examples[\"translation\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # FIX: Use text_target to avoid warnings\n",
    "    labels = tokenizer(\n",
    "        text_target=targets, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "# Process datasets\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c9278",
   "metadata": {
    "papermill": {
     "duration": 0.007545,
     "end_time": "2025-12-25T10:28:57.023322",
     "exception": false,
     "start_time": "2025-12-25T10:28:57.015777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447be19d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:09:44.389418Z",
     "iopub.status.busy": "2026-01-08T08:09:44.389097Z",
     "iopub.status.idle": "2026-01-08T08:09:45.231488Z",
     "shell.execute_reply": "2026-01-08T08:09:45.230953Z",
     "shell.execute_reply.started": "2026-01-08T08:09:44.389394Z"
    },
    "papermill": {
     "duration": 1.398574,
     "end_time": "2025-12-25T10:28:58.434786",
     "exception": false,
     "start_time": "2025-12-25T10:28:57.036212",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model from:\", MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Data Collator handles dynamic padding during batching\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f0dc9",
   "metadata": {},
   "source": [
    "# A6. Optional: Monolingual Pre-Training on Akkadian Texts\n",
    "\n",
    "This step teaches the model Akkadian grammar and morphology BEFORE translation training.\n",
    "Uses published_texts.csv (8,000+ Akkadian texts) with Masked Language Modeling (MLM).\n",
    "\n",
    "Benefits:\n",
    "- Model learns to handle gaps naturally\n",
    "- Better understanding of Akkadian word structure\n",
    "- Improves low-resource translation performance\n",
    "\n",
    "Set ENABLE_MONO_PRETRAIN=True to enable (adds ~30min training time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86171f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:09:45.232827Z",
     "iopub.status.busy": "2026-01-08T08:09:45.232532Z",
     "iopub.status.idle": "2026-01-08T08:39:38.576328Z",
     "shell.execute_reply": "2026-01-08T08:39:38.575692Z",
     "shell.execute_reply.started": "2026-01-08T08:09:45.232802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Monolingual Pre-Training Configuration\n",
    "ENABLE_MONO_PRETRAIN = bool(int(os.getenv(\"ENABLE_MONO_PRETRAIN\", \"1\")))  # Set to 1 to enable\n",
    "\n",
    "if ENABLE_MONO_PRETRAIN:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    \n",
    "    if os.path.exists(pub_texts_path):\n",
    "        # Load Akkadian-only texts\n",
    "        pub_texts_df = pd.read_csv(pub_texts_path)\n",
    "        akkadian_texts = pub_texts_df['transliteration'].dropna().astype(str).tolist()\n",
    "        akkadian_texts = [clean_translit(t, keep_gaps=True) for t in akkadian_texts]\n",
    "        akkadian_texts = [t for t in akkadian_texts if len(t.split()) >= 5 and len(t.split()) <= 200]\n",
    "        akkadian_texts = akkadian_texts[:5000]  # Limit for time\n",
    "        \n",
    "        print(f\"Loaded {len(akkadian_texts)} Akkadian texts for pre-training\")\n",
    "        \n",
    "        # Simple MLM approach: Mask random spans\n",
    "        from transformers import DataCollatorForSeq2Seq\n",
    "        \n",
    "        def create_mlm_examples(texts):\n",
    "            \"\"\"Create masked language modeling examples\"\"\"\n",
    "            mlm_examples = []\n",
    "            for text in texts:\n",
    "                tokens = text.split()\n",
    "                if len(tokens) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Mask 15% of tokens\n",
    "                n_mask = max(1, int(len(tokens) * 0.15))\n",
    "                mask_positions = np.random.choice(len(tokens), size=n_mask, replace=False)\n",
    "                \n",
    "                masked_text = []\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if i in mask_positions:\n",
    "                        masked_text.append(\"<extra_id_0>\")  # sentinel-style token\n",
    "                    else:\n",
    "                        masked_text.append(token)\n",
    "                \n",
    "                input_text = \" \".join(masked_text)\n",
    "                target_text = \" \".join([tokens[i] for i in mask_positions])\n",
    "                \n",
    "                mlm_examples.append({\n",
    "                    \"transliteration\": input_text,\n",
    "                    \"translation\": target_text\n",
    "                })\n",
    "            \n",
    "            return mlm_examples\n",
    "        \n",
    "        mlm_data = create_mlm_examples(akkadian_texts)\n",
    "        print(f\"Created {len(mlm_data)} MLM training examples\")\n",
    "        \n",
    "        # Create MLM dataset\n",
    "        mlm_dataset = Dataset.from_pandas(pd.DataFrame(mlm_data))\n",
    "        \n",
    "        def preprocess_mlm(examples):\n",
    "            inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n",
    "            targets = examples[\"translation\"]\n",
    "            model_inputs = tokenizer(\n",
    "                inputs,\n",
    "                max_length=MAX_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            labels = tokenizer(\n",
    "                text_target=targets,\n",
    "                max_length=MAX_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            model_inputs[\"labels\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "            return model_inputs\n",
    "        \n",
    "        tokenized_mlm = mlm_dataset.map(preprocess_mlm, batched=True)\n",
    "        \n",
    "        # UPDATED MLM ARGUMENTS (Safe Mode)\n",
    "        mlm_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=f\"{OUTPUT_DIR}_mlm\",\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-4,              # Lower LR for stability\n",
    "            \n",
    "            # MEMORY & STABILITY FIXES\n",
    "            per_device_train_batch_size=1,   # Batch size 1 prevents OOM\n",
    "            gradient_accumulation_steps=16,  # Simulates batch 16\n",
    "            fp16=False,                      # MUST BE FALSE for ByT5\n",
    "            \n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"no\",\n",
    "            logging_steps=50,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        # FIX: processing_class instead of tokenizer (Removes Warning)\n",
    "        mlm_trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=mlm_args,\n",
    "            train_dataset=tokenized_mlm,\n",
    "            processing_class=tokenizer,      # Updated argument name\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting monolingual pre-training (1 epoch on Akkadian texts)...\")\n",
    "        try:\n",
    "            mlm_trainer.train()\n",
    "            print(\"✓ Monolingual pre-training complete\")\n",
    "            print(\"Model now understands Akkadian grammar and gaps better!\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  MLM pre-training failed: {e}\")\n",
    "            print(\"Continuing with main training...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️  published_texts.csv not found, skipping monolingual pre-training\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Monolingual pre-training disabled (set ENABLE_MONO_PRETRAIN=1 to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71300a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data stats after mining and merge\n",
    "sup_count_est = len(train_df) - (len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0)\n",
    "print(\"\\n=== DATASET COUNTS ===\")\n",
    "print(f\"Supervised pairs (est.): {sup_count_est}\")\n",
    "print(f\"Mined pairs: {len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0}\")\n",
    "print(f\"Total pairs: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory after monolingual pre-training to prevent OOM\n",
    "import gc\n",
    "del mlm_trainer\n",
    "del mlm_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared for main training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e880a1",
   "metadata": {
    "papermill": {
     "duration": 0.007375,
     "end_time": "2025-12-25T10:28:58.449624",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.442249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A7. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5dcb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:39:38.577529Z",
     "iopub.status.busy": "2026-01-08T08:39:38.577280Z",
     "iopub.status.idle": "2026-01-08T08:39:38.613054Z",
     "shell.execute_reply": "2026-01-08T08:39:38.612457Z",
     "shell.execute_reply.started": "2026-01-08T08:39:38.577507Z"
    },
    "language": "python",
    "papermill": {
     "duration": 0.169346,
     "end_time": "2025-12-25T10:28:58.626361",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.457015",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# A7. UPDATED TRAINING ARGS (THE PURIST)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # MEMORY & STABILITY FIXES\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=1,       # Batch size 1\n",
    "    gradient_accumulation_steps=32,      # Accumulate 32 times (Effective batch = 32)\n",
    "    fp16=False,                          # MUST BE FALSE\n",
    "\n",
    "    num_train_epochs=15,\n",
    "    gradient_checkpointing=True,         # Saves huge amount of memory\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=300,           # Match reduced MAX_LENGTH\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # Disable cache to silence warnings\n",
    "print(\"✓ Configured for ByT5 Purist Strategy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0d460",
   "metadata": {
    "papermill": {
     "duration": 0.006845,
     "end_time": "2025-12-25T10:28:58.640314",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.633469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A8. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c0a3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:39:38.614126Z",
     "iopub.status.busy": "2026-01-08T08:39:38.613826Z",
     "iopub.status.idle": "2026-01-08T08:39:39.370781Z",
     "shell.execute_reply": "2026-01-08T08:39:39.369857Z",
     "shell.execute_reply.started": "2026-01-08T08:39:38.614094Z"
    },
    "language": "python",
    "papermill": {
     "duration": 22.854214,
     "end_time": "2025-12-25T10:29:21.501227",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.647013",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/1667860881.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Force aggressive memory cleanup\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d909d2f",
   "metadata": {
    "papermill": {
     "duration": 0.006946,
     "end_time": "2025-12-25T10:29:21.515664",
     "exception": false,
     "start_time": "2025-12-25T10:29:21.508718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A9. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8847867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:39:39.372178Z",
     "iopub.status.busy": "2026-01-08T08:39:39.371813Z",
     "iopub.status.idle": "2026-01-08T08:43:10.281101Z",
     "shell.execute_reply": "2026-01-08T08:43:10.279836Z",
     "shell.execute_reply.started": "2026-01-08T08:39:39.372152Z"
    },
    "papermill": {
     "duration": 5732.305735,
     "end_time": "2025-12-25T12:04:53.828320",
     "exception": false,
     "start_time": "2025-12-25T10:29:21.522585",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING OPTIMIZED TRAINING - ByT5\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTraining Strategy:\")\n",
    "print(\"✓ 20 epochs with evaluation each epoch\")\n",
    "print(\"✓ Cosine learning rate with restarts\")\n",
    "print(\"✓ Best model selection based on eval loss\")\n",
    "print(\"✓ Label smoothing for generalization\")\n",
    "print(\"✓ Gradient clipping for stability\")\n",
    "print(\"\\nExpected improvements:\")\n",
    "print(\"• Better handling of Akkadian morphology (character-level)\")\n",
    "print(\"• Reduced overfitting through regularization\")\n",
    "print(\"• Higher BLEU/chrF++ scores from beam search\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# OOM-safe training wrapper with recovery\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n✓ Training completed successfully!\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n[WARNING] CUDA OOM detected. Implementing recovery strategy...\")\n",
    "        \n",
    "        # Strategy 1: Reduce gradient accumulation\n",
    "        training_args.gradient_accumulation_steps = max(8, training_args.gradient_accumulation_steps // 2)\n",
    "        print(f\"  → Reduced gradient accumulation to {training_args.gradient_accumulation_steps}\")\n",
    "        \n",
    "        # Strategy 2: Clear memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Strategy 3: Reduce max length slightly\n",
    "        try:\n",
    "            MAX_LENGTH = max(200, int(MAX_LENGTH * 0.9))\n",
    "            print(f\"  → Reduced MAX_LENGTH to {MAX_LENGTH}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Retry with adjusted settings\n",
    "        print(\"  → Retrying training with adjusted settings...\")\n",
    "        try:\n",
    "            # Recreate trainer with new settings\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train,\n",
    "                eval_dataset=tokenized_val,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "            trainer.train()\n",
    "            print(\"✓ Training completed with adjusted settings!\")\n",
    "        except Exception as retry_error:\n",
    "            print(f\"✗ Training failed even after adjustment: {retry_error}\")\n",
    "            print(\"Suggestions:\")\n",
    "            print(\"  1. Reduce num_train_epochs\")\n",
    "            print(\"  2. Set gradient_accumulation_steps=8\")\n",
    "            print(\"  3. Disable gradient_checkpointing\")\n",
    "            raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PHASE COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28d5b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-08T08:43:10.281721Z",
     "iopub.status.idle": "2026-01-08T08:43:10.282019Z",
     "shell.execute_reply": "2026-01-08T08:43:10.281862Z",
     "shell.execute_reply.started": "2026-01-08T08:43:10.281847Z"
    },
    "papermill": {
     "duration": 250.42551,
     "end_time": "2025-12-25T12:09:04.261371",
     "exception": false,
     "start_time": "2025-12-25T12:04:53.835861",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# POST-TRAINING VALIDATION WITH ENHANCED METRICS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POST-TRAINING VALIDATION - COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Computing metrics: BLEU, chrF++, and Geometric Mean\")\n",
    "print(\"(Following Deep Past Challenge evaluation methodology)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def dedup_repeats(text: str) -> str:\n",
    "    \"\"\"Remove consecutive repeated tokens\"\"\"\n",
    "    toks = text.split()\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(out) >= 2 and t == out[-1] == out[-2]:\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def postprocess_text(preds):\n",
    "    \"\"\"Enhanced postprocessing for better output quality\"\"\"\n",
    "    out = []\n",
    "    for p in preds:\n",
    "        p = p.strip()\n",
    "        # Fix spacing around punctuation\n",
    "        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
    "        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
    "        # Remove repeated tokens\n",
    "        p = dedup_repeats(p)\n",
    "        # Capitalize first letter\n",
    "        if p and p[0].islower():\n",
    "            p = p[0].upper() + p[1:]\n",
    "        # Ensure sentence ends with punctuation\n",
    "        if p and p[-1] not in \".!?\":\n",
    "            p += \".\"\n",
    "        # Remove multiple punctuation\n",
    "        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
    "        out.append(p.strip())\n",
    "    return out\n",
    "\n",
    "val_texts = dataset[\"test\"][\"transliteration\"]\n",
    "val_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n",
    "\n",
    "print(f\"Validating on {len(val_texts)} samples...\")\n",
    "print(\"Using beam search with num_beams=8 for higher quality\\n\")\n",
    "\n",
    "def generate_batch(texts, num_beams=8):\n",
    "    \"\"\"Enhanced generation with optimized parameters\"\"\"\n",
    "    batch_inputs = [PREFIX + doc for doc in texts]\n",
    "    enc = tokenizer(\n",
    "        batch_inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_length=MAX_LENGTH,\n",
    "        min_length=6,\n",
    "        num_beams=num_beams,              # Higher beams\n",
    "        no_repeat_ngram_size=3,           # Prevent repetition\n",
    "        length_penalty=1.2,               # Slightly favor longer outputs\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=1.1,           # Additional repetition penalty\n",
    "        do_sample=False,                  # Deterministic for evaluation\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "# Generate predictions\n",
    "preds = []\n",
    "batch_size = 4  # Smaller batches for stability\n",
    "for i in range(0, len(val_texts), batch_size):\n",
    "    batch_preds = generate_batch(val_texts[i:i+batch_size])\n",
    "    preds.extend(batch_preds)\n",
    "    if (i // batch_size + 1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+batch_size}/{len(val_texts)} samples processed\")\n",
    "\n",
    "preds = postprocess_text(preds)\n",
    "\n",
    "# Compute all metrics\n",
    "print(\"\\nComputing metrics...\")\n",
    "bleu_result = metric_bleu.compute(predictions=preds, references=val_refs)\n",
    "bleu_score = bleu_result['score']\n",
    "\n",
    "chrf_result = metric_chrf.compute(predictions=preds, references=val_refs, word_order=2)\n",
    "chrf_score = chrf_result['score']\n",
    "\n",
    "# Geometric mean (competition metric)\n",
    "import math\n",
    "geo_mean = math.sqrt(bleu_score * chrf_score)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Samples evaluated:  {len(val_texts)}\")\n",
    "print(f\"\")\n",
    "print(f\"BLEU Score:         {bleu_score:7.2f}\")\n",
    "print(f\"chrF++ Score:       {chrf_score:7.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"🏆 GEOMETRIC MEAN:  {geo_mean:7.2f}  ← Challenge Metric\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n📊 SAMPLE PREDICTIONS (first 3):\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(3, len(val_texts))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Source: {val_texts[i][:80]}...\")\n",
    "    print(f\"  Target: {val_refs[i][0][:80]}...\")\n",
    "    print(f\"  Prediction: {preds[i][:80]}...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Score interpretation\n",
    "if geo_mean >= 35:\n",
    "    print(\"🌟 EXCELLENT! Score is competition-winning level!\")\n",
    "elif geo_mean >= 30:\n",
    "    print(\"✨ GREAT! Score is strong, top quartile expected.\")\n",
    "elif geo_mean >= 25:\n",
    "    print(\"✓ GOOD! Score is solid, room for improvement.\")\n",
    "else:\n",
    "    print(\"⚠️  Score needs improvement. Consider:\")\n",
    "    print(\"   • More training epochs\")\n",
    "    print(\"   • Better data augmentation\")\n",
    "    print(\"   • Hyperparameter tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a27615",
   "metadata": {
    "papermill": {
     "duration": 0.007132,
     "end_time": "2025-12-25T12:09:04.275738",
     "exception": false,
     "start_time": "2025-12-25T12:09:04.268606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A10. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06794aa3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-08T08:43:10.283354Z",
     "iopub.status.idle": "2026-01-08T08:43:10.283622Z",
     "shell.execute_reply": "2026-01-08T08:43:10.283519Z",
     "shell.execute_reply.started": "2026-01-08T08:43:10.283502Z"
    },
    "papermill": {
     "duration": 3.633252,
     "end_time": "2025-12-25T12:09:07.916126",
     "exception": false,
     "start_time": "2025-12-25T12:09:04.282874",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Notebook A Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec693e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-08T08:43:10.284698Z",
     "iopub.status.idle": "2026-01-08T08:43:10.284976Z",
     "shell.execute_reply": "2026-01-08T08:43:10.284835Z",
     "shell.execute_reply.started": "2026-01-08T08:43:10.284821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# A11. Optional Self-Training Augmentation (Small, OOM-Safe)\n",
    "ENABLE_SELF_TRAIN = False\n",
    "MAX_PSEUDO = int(os.getenv(\"BYT5_MAX_PSEUDO\", \"500\"))  # keep small to avoid OOM\n",
    "\n",
    "if ENABLE_SELF_TRAIN:\n",
    "    print(\"\\n=== SELF-TRAINING AUGMENTATION (ByT5) ===\")\n",
    "    pub_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    if os.path.exists(pub_path):\n",
    "        pub_df = pd.read_csv(pub_path)\n",
    "        translits = pub_df.get(\"transliteration\", pd.Series([])).dropna().astype(str).tolist()\n",
    "        translits = [clean_translit(t) for t in translits]\n",
    "        translits = [t for t in translits if 5 <= len(t.split()) <= 180]\n",
    "        translits = translits[:MAX_PSEUDO]\n",
    "        print(f\"Generating pseudo translations for {len(translits)} extra transliterations...\")\n",
    "\n",
    "        def generate_batch(texts):\n",
    "            batch_inputs = [PREFIX + doc for doc in texts]\n",
    "            enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "            gen = model.generate(\n",
    "                **enc,\n",
    "                max_length=min(MAX_LENGTH, 400),\n",
    "                min_length=6,\n",
    "                num_beams=6,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.05,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        pseudo_trans = []\n",
    "        for i in range(0, len(translits), 8):  # small batch to avoid OOM\n",
    "            try:\n",
    "                batch_preds = generate_batch(translits[i:i+8])\n",
    "                pseudo_trans.extend(batch_preds)\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"[WARNING] OOM during pseudo generation; skipping remaining.\")\n",
    "                    break\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        # Postprocess & filter\n",
    "        def dedup_repeats(text: str) -> str:\n",
    "            toks = text.split()\n",
    "            out = []\n",
    "            for t in toks:\n",
    "                if len(out) >= 2 and t == out[-1] == out[-2]:\n",
    "                    continue\n",
    "                out.append(t)\n",
    "            return \" \".join(out)\n",
    "        def postprocess_text(preds):\n",
    "            out = []\n",
    "            for p in preds:\n",
    "                p = p.strip()\n",
    "                p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
    "                p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
    "                p = dedup_repeats(p)\n",
    "                if p and p[0].islower():\n",
    "                    p = p[0].upper() + p[1:]\n",
    "                if p and p[-1] not in \".!?\":\n",
    "                    p += \".\"\n",
    "                p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
    "                out.append(p.strip())\n",
    "            return out\n",
    "\n",
    "        pseudo_trans = postprocess_text(pseudo_trans)\n",
    "        aug_df = pd.DataFrame({\"transliteration\": translits[:len(pseudo_trans)], \"translation\": pseudo_trans})\n",
    "        aug_df[\"src_len\"] = aug_df[\"transliteration\"].str.split().str.len()\n",
    "        aug_df[\"tgt_len\"] = aug_df[\"translation\"].str.split().str.len()\n",
    "        ratio = (aug_df[\"tgt_len\"] / aug_df[\"src_len\"]).clip(upper=6)\n",
    "        aug_df = aug_df[(aug_df[\"tgt_len\"] >= 4) & (ratio >= 0.5) & (ratio <= 6)]\n",
    "        aug_df = aug_df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
    "        print(f\"Pseudo pairs retained after filtering: {len(aug_df)}\")\n",
    "\n",
    "        base_train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "        base_train = base_train.dropna(subset=[\"transliteration\", \"translation\"]).astype(str)\n",
    "        base_train[\"transliteration\"] = base_train[\"transliteration\"].map(clean_translit)\n",
    "        base_train[\"translation\"] = base_train[\"translation\"].map(clean_translation)\n",
    "        combined = pd.concat([\n",
    "            base_train[[\"transliteration\", \"translation\"]],\n",
    "            aug_df[[\"transliteration\", \"translation\"]]\n",
    "        ], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "        print(f\"Total combined training pairs: {len(combined)}\")\n",
    "\n",
    "        ds_combined = Dataset.from_pandas(combined)\n",
    "        def preprocess_function_aug(examples):\n",
    "            inputs = [PREFIX + ex for ex in examples[\"transliteration\"]]\n",
    "            targets = examples[\"translation\"]\n",
    "            model_inputs = tokenizer(\n",
    "                inputs,\n",
    "                max_length=MAX_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(\n",
    "                    targets,\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "            model_inputs[\"labels\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "            return model_inputs\n",
    "        tokenized_combined = ds_combined.map(preprocess_function_aug, batched=True)\n",
    "\n",
    "        training_args_aug = Seq2SeqTrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            learning_rate=2.5e-4,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=16,\n",
    "            num_train_epochs=1,  # keep short to avoid OOM/time\n",
    "            fp16=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        trainer_aug = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args_aug,\n",
    "            train_dataset=tokenized_combined,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"Starting second-stage training (ByT5) with augmented data...\")\n",
    "        try:\n",
    "            trainer_aug.train()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[WARNING] Augmentation training skipped due to error: {e}\")\n",
    "        print(\"Augmentation stage complete.\")\n",
    "\n",
    "        print(f\"Saving augmented model to {OUTPUT_DIR}...\")\n",
    "        trainer_aug.save_model(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    else:\n",
    "        print(\"published_texts.csv not found; skipping self-training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98d73b",
   "metadata": {},
   "source": [
    "## 🎯 NEXT STEPS: Advanced Strategies for Higher Scores\n",
    "\n",
    "The optimized training configuration above should achieve **strong baseline scores** (geometric mean ~28-35). To push toward **competition-winning performance (35+)**, consider these advanced strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADVANCED TRAINING STRATEGIES FOR SCORE IMPROVEMENT\n",
    "====================================================\n",
    "\n",
    "If current scores are below target (geometric mean < 30), try these techniques:\n",
    "\n",
    "1. DATA AUGMENTATION\n",
    "   ─────────────────\n",
    "   • Self-training: Use model predictions on unlabeled publications.csv\n",
    "   • Back-translation: Translate English → Akkadian → English\n",
    "   • Paraphrase generation: Create variations of training pairs\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Generate pseudo-labels from publications.csv\n",
    "   unlabeled_texts = pd.read_csv('publications.csv')['transliteration']\n",
    "   pseudo_labels = [model.generate(...) for text in unlabeled_texts]\n",
    "   augmented_data = Dataset.from_dict({\n",
    "       'transliteration': unlabeled_texts,\n",
    "       'translation': pseudo_labels\n",
    "   })\n",
    "   combined_dataset = concatenate_datasets([dataset['train'], augmented_data])\n",
    "   ```\n",
    "\n",
    "2. CURRICULUM LEARNING\n",
    "   ───────────────────\n",
    "   • Train on easy examples first, gradually increase difficulty\n",
    "   • Sort by sentence length, gaps count, or complexity\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Sort training data by length (simple → complex)\n",
    "   train_df = pd.DataFrame(dataset['train'])\n",
    "   train_df['src_len'] = train_df['transliteration'].str.split().str.len()\n",
    "   train_df = train_df.sort_values('src_len')\n",
    "   \n",
    "   # Train in stages\n",
    "   for stage, max_len in enumerate([30, 60, 100, 200]):\n",
    "       stage_data = train_df[train_df['src_len'] <= max_len]\n",
    "       # Train for 5 epochs on this stage\n",
    "   ```\n",
    "\n",
    "3. ENSEMBLE WITHIN BYT5\n",
    "   ─────────────────────\n",
    "   • Train multiple ByT5 models with different seeds\n",
    "   • Average their predictions for better stability\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   models = []\n",
    "   for seed in [42, 123, 456]:\n",
    "       set_seed(seed)\n",
    "       model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "       trainer = Seq2SeqTrainer(...)\n",
    "       trainer.train()\n",
    "       models.append(model)\n",
    "   \n",
    "   # Ensemble predictions\n",
    "   all_preds = [model.generate(...) for model in models]\n",
    "   final_pred = voting_mechanism(all_preds)  # Majority vote or averaging\n",
    "   ```\n",
    "\n",
    "4. ADVANCED HYPERPARAMETER TUNING\n",
    "   ───────────────────────────────\n",
    "   • Learning rate scheduling: Try polynomial decay or OneCycleLR\n",
    "   • Epoch extension: 25-30 epochs with early stopping patience=5\n",
    "   • Regularization: Increase dropout (0.1 → 0.15), weight decay (0.01 → 0.05)\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   training_args.num_train_epochs = 30\n",
    "   training_args.lr_scheduler_type = \"polynomial\"  # or \"cosine_with_restarts\"\n",
    "   training_args.learning_rate = 3e-5  # Try 3e-5, 4e-5, 6e-5\n",
    "   training_args.warmup_ratio = 0.1\n",
    "   ```\n",
    "\n",
    "5. POST-PROCESSING ENHANCEMENT\n",
    "   ───────────────────────────\n",
    "   • Language model scoring: Re-rank beam outputs with GPT-2\n",
    "   • Rule-based fixes: Correct common errors (articles, plurality)\n",
    "   • Length normalization: Penalize too-short/too-long outputs\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "   \n",
    "   lm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "   lm_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "   \n",
    "   def rerank_with_lm(candidates):\n",
    "       scores = []\n",
    "       for cand in candidates:\n",
    "           inputs = lm_tok(cand, return_tensors='pt')\n",
    "           with torch.no_grad():\n",
    "               score = -lm(**inputs).loss.item()  # Perplexity\n",
    "           scores.append(score)\n",
    "       return candidates[np.argmax(scores)]\n",
    "   ```\n",
    "\n",
    "6. DATA MINING OPTIMIZATION\n",
    "   ─────────────────────────\n",
    "   • Use publications.csv more effectively\n",
    "   • Extract patterns from high-quality translation pairs\n",
    "   • Filter low-quality augmented data\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Score data quality\n",
    "   def quality_score(src, tgt):\n",
    "       length_ratio = len(tgt.split()) / max(len(src.split()), 1)\n",
    "       has_gaps = '<gap>' in src.lower()\n",
    "       return length_ratio * (0.8 if has_gaps else 1.0)\n",
    "   \n",
    "   # Keep only high-quality augmented pairs\n",
    "   augmented_data = augmented_data.filter(\n",
    "       lambda x: quality_score(x['transliteration'], x['translation']) > 0.5\n",
    "   )\n",
    "   ```\n",
    "\n",
    "7. ARCHITECTURE MODIFICATIONS\n",
    "   ──────────────────────────\n",
    "   • Freeze encoder for first 5 epochs (faster convergence)\n",
    "   • Gradually unfreeze layers (discriminative fine-tuning)\n",
    "   \n",
    "   Implementation:\n",
    "   ```\n",
    "   # Freeze encoder initially\n",
    "   for param in model.encoder.parameters():\n",
    "       param.requires_grad = False\n",
    "   \n",
    "   # Train decoder only for 5 epochs\n",
    "   trainer.train(max_steps=...)\n",
    "   \n",
    "   # Unfreeze and continue\n",
    "   for param in model.encoder.parameters():\n",
    "       param.requires_grad = True\n",
    "   trainer.train()  # Continue training\n",
    "   ```\n",
    "\n",
    "SCORING TARGETS\n",
    "───────────────\n",
    "Current optimized config: ~28-32 geometric mean (expected baseline)\n",
    "With 1-2 techniques above: ~32-36 (competitive)\n",
    "With 3+ techniques above: 36+ (top quartile)\n",
    "\n",
    "RECOMMENDED PRIORITY ORDER\n",
    "─────────────────────────\n",
    "1. Try self-training augmentation first (biggest impact)\n",
    "2. Extend to 25-30 epochs with better LR schedule\n",
    "3. Ensemble with multiple seeds (stability boost)\n",
    "4. Post-processing with LM re-ranking (final polish)\n",
    "\n",
    "Remember: Geometric mean = √(BLEU × chrF++)\n",
    "- BLEU rewards exact matches (focus on common phrases)\n",
    "- chrF++ rewards character overlap (focus on morphology)\n",
    "- Balance both for optimal score\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📚 ADVANCED STRATEGIES REFERENCE LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(\"Implement these techniques to push scores from ~30 to 35+\")\n",
    "print(\"Priority: Data Augmentation → Extended Training → Ensemble\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b1bf8",
   "metadata": {},
   "source": [
    "## 🎯 NEXT STEPS: Advanced Strategies for Higher Scores\n",
    "\n",
    "The optimized ByT5 configuration should reach strong baseline scores (geometric mean ~28–35). Push toward competition-winning performance (35+) with:\n",
    "\n",
    "- Data augmentation: self-training on unlabeled texts, back-translation, paraphrase variants.\n",
    "- Curriculum learning: train on simple → complex data (by length/gap count).\n",
    "- Ensembles: train multiple ByT5 seeds and average predictions.\n",
    "- Extended training: increase epochs (25–30), adjust LR scheduling/warmup.\n",
    "- Post-processing: LM re-ranking and rule-based fixes (articles, punctuation, repetition).\n",
    "- Encoder freezing: freeze encoder for first epochs, then unfreeze to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c961c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend training and generation parameters (safe toggles)\n",
    "training_args.num_train_epochs = max(getattr(training_args, \"num_train_epochs\", 20), 25)\n",
    "training_args.lr_scheduler_type = \"cosine_with_restarts\"\n",
    "training_args.warmup_ratio = 0.1\n",
    "training_args.weight_decay = 0.01\n",
    "training_args.generation_num_beams = max(getattr(training_args, \"generation_num_beams\", 1), 8)\n",
    "\n",
    "print(\"Next steps applied: epochs>=25, cosine restarts, beams>=8.\")\n",
    "print(\"Consider: self-training augmentation, multi-seed ensembles, LM re-ranking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe5d54",
   "metadata": {},
   "source": [
    "## 🛠️ Data Mining (Akkadian-only) from publications.csv\n",
    "\n",
    "**⚠️ IMPORTANT: Run this section AFTER completing the main training pipeline above, or run it independently in a separate session.**\n",
    "\n",
    "Goal: Extract English translation segments from `publications.csv` pages that contain Akkadian transliterations (`has_akkadian == true`).\n",
    "\n",
    "Pipeline:\n",
    "- Stream `publications.csv` (580MB) in chunks to handle memory constraints.\n",
    "- Filter rows where `has_akkadian == true` only.\n",
    "- Clean OCR text, normalize Unicode, remove headers/footers.\n",
    "- Detect English sentences; optionally translate non-English to English using MarianMT.\n",
    "- Save extracted sentences to `mined_publications_en.csv` for later augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q rapidfuzz langdetect ftfy unidecode nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "from unidecode import unidecode\n",
    "from langdetect import detect, DetectorFactory\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "# Config paths\n",
    "PUBS_PATH = os.getenv('PUBLICATIONS_CSV', 'publications.csv')\n",
    "OUT_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\n",
    "CHUNKSIZE = int(os.getenv('PUBS_CHUNKSIZE', '5000'))\n",
    "TRANSLATE_NON_EN = os.getenv('TRANSLATE_NON_EN', 'false').lower() == 'true'\n",
    "\n",
    "# Optional translator (loaded lazily if enabled)\n",
    "translator_tokenizer = None\n",
    "translator_model = None\n",
    "\n",
    "def lazy_load_translator():\n",
    "    global translator_tokenizer, translator_model\n",
    "    if translator_tokenizer is None or translator_model is None:\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        model_name = 'Helsinki-NLP/opus-mt-mul-en'\n",
    "        translator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        translator_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def machine_translate_to_en(text: str) -> str:\n",
    "    lazy_load_translator()\n",
    "    enc = translator_tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
    "    gen = translator_model.generate(**enc, max_length=256, num_beams=5)\n",
    "    return translator_tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "\n",
    "def normalize_text(x: str) -> str:\n",
    "    if not isinstance(x, str):\n",
    "        return ''\n",
    "    x = fix_text(x)\n",
    "    x = re.sub(r'[\\r\\t]', ' ', x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    # Remove common OCR artifacts\n",
    "    patterns = [r'Kleine Mitteilungen', r'INDIVIDUAL AND FAMILY', r'THE ASSYRIAN COLONY AT KANESH', r'Jan Gerrit Dercksen', r'MOGENS TROLLE LARSEN', r'\\b\\d{1,3}\\b\\s*$']\n",
    "    for p in patterns:\n",
    "        x = re.sub(p, ' ', x, flags=re.IGNORECASE)\n",
    "    x = unidecode(x)\n",
    "    x = re.sub(r'\\s+', ' ', x).strip()\n",
    "    return x\n",
    "\n",
    "def english_sentences(text: str):\n",
    "    \"\"\"Return English sentences from input text.\"\"\"\n",
    "    sents = []\n",
    "    try:\n",
    "        for s in sent_tokenize(text):\n",
    "            s_clean = s.strip()\n",
    "            if not s_clean:\n",
    "                continue\n",
    "            lang_ok = False\n",
    "            try:\n",
    "                lang = detect(s_clean)\n",
    "                lang_ok = (lang == 'en')\n",
    "            except Exception:\n",
    "                lang_ok = bool(re.search(r'\\b(the|and|of|to|in|for|with|on|as|is|are)\\b', s_clean, flags=re.IGNORECASE))\n",
    "            if lang_ok:\n",
    "                sents.append(s_clean)\n",
    "            elif TRANSLATE_NON_EN:\n",
    "                try:\n",
    "                    s_en = machine_translate_to_en(s_clean)\n",
    "                    sents.append(s_en.strip())\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        for s in re.split(r'[.!?]', text):\n",
    "            s_clean = s.strip()\n",
    "            if s_clean:\n",
    "                sents.append(s_clean)\n",
    "    return sents\n",
    "\n",
    "def mine_publications(pubs_path: str, out_path: str, chunksize: int = 5000):\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    total_rows = 0\n",
    "    kept_rows = 0\n",
    "    written_rows = 0\n",
    "    cols = ['pdf_name', 'page', 'page_text', 'has_akkadian']\n",
    "    \n",
    "    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(['pdf_name', 'page', 'english_sentence'])\n",
    "        \n",
    "        for i, chunk in enumerate(pd.read_csv(pubs_path, usecols=cols, chunksize=chunksize, dtype={'pdf_name': 'string', 'page': 'int64', 'page_text': 'string', 'has_akkadian': 'bool'})):\n",
    "            total_rows += len(chunk)\n",
    "            chunk = chunk[chunk['has_akkadian'] == True]\n",
    "            kept_rows += len(chunk)\n",
    "            chunk['clean_text'] = chunk['page_text'].apply(normalize_text)\n",
    "            \n",
    "            for _, row in chunk.iterrows():\n",
    "                pdf = row['pdf_name'] or ''\n",
    "                page = int(row['page']) if pd.notna(row['page']) else -1\n",
    "                clean = row['clean_text'] or ''\n",
    "                if not clean:\n",
    "                    continue\n",
    "                sents = english_sentences(clean)\n",
    "                for s in sents:\n",
    "                    if 15 <= len(s) <= 600:\n",
    "                        writer.writerow([pdf, page, s])\n",
    "                        written_rows += 1\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i+1} chunks — total rows: {total_rows}, kept: {kept_rows}, sentences written: {written_rows}\")\n",
    "    \n",
    "    print(f\"DONE. Total rows: {total_rows}, Akkadian pages: {kept_rows}, English sentences written: {written_rows}\")\n",
    "\n",
    "print(\"Starting mining from publications.csv (Akkadian-only pages)...\")\n",
    "mine_publications(PUBS_PATH, OUT_PATH, CHUNKSIZE)\n",
    "print(f\"Saved mined sentences to: {OUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde91156",
   "metadata": {},
   "source": [
    "## 🔗 Sentence-Level Alignment with published_texts.csv\n",
    "\n",
    "**⚠️ PREREQUISITE: Run the data mining cell above first to generate `mined_publications_en.csv`.**\n",
    "\n",
    "Goal: Align mined English sentences from `mined_publications_en.csv` to Akkadian transliterations in `published_texts.csv` by matching catalog labels and aliases.\n",
    "\n",
    "Approach:\n",
    "- Load `published_texts.csv` (≈8k rows) and `mined_publications_en.csv`.\n",
    "- Extract catalog-like refs (e.g., BIN VI 39, Kt 72/k) from English sentences.\n",
    "- Fuzzy-match refs to `publication_catalog` or `aliases` in `published_texts.csv` using RapidFuzz.\n",
    "- Emit candidate parallel pairs to `aligned_pairs_candidates.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bce5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "PUBLISHED_TEXTS_PATH = os.getenv('PUBLISHED_TEXTS_CSV', 'published_texts.csv')\n",
    "MINED_EN_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\n",
    "ALIGNED_OUT_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n",
    "\n",
    "# Heuristic patterns for publication labels and catalog IDs\n",
    "CATALOG_PATTERNS = [\n",
    "    r\"\\bBIN\\s+[IVXLCDM]+\\s*\\d+\\b\",\n",
    "    r\"\\bKt\\.?\\s*\\d+/?[A-Za-z0-9-]*\\b\",\n",
    "    r\"\\bBM\\s*\\d+[A-Za-z]?\\b\",\n",
    "    r\"\\bYBC\\s*\\d+\\b\",\n",
    "    r\"\\b(AbB|AKT|CCT|KBo|KUB)\\s*\\d+[A-Za-z0-9-]*\\b\",\n",
    "]\n",
    "\n",
    "def extract_catalog_refs(text: str) -> list:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = fix_text(text)\n",
    "    text = unidecode(text)\n",
    "    refs = set()\n",
    "    for pat in CATALOG_PATTERNS:\n",
    "        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n",
    "            ref = m.group(0).strip()\n",
    "            ref = re.sub(r\"\\s+\", \" \", ref)\n",
    "            refs.add(ref)\n",
    "    return list(refs)\n",
    "\n",
    "def build_alias_index(df: pd.DataFrame):\n",
    "    \"\"\"Build a search index over publication_catalog and aliases fields.\"\"\"\n",
    "    index_records = []\n",
    "    for i, row in df.iterrows():\n",
    "        rid = i\n",
    "        label = str(row.get('label', '') or '')\n",
    "        pubcat = str(row.get('publication_catalog', '') or '')\n",
    "        aliases = str(row.get('aliases', '') or '')\n",
    "        tokens = []\n",
    "        for field in (pubcat, aliases, label):\n",
    "            parts = re.split(r\"[|,;]\", field)\n",
    "            for p in parts:\n",
    "                p = unidecode(p.strip())\n",
    "                if p:\n",
    "                    tokens.append(p)\n",
    "        tokens = list(dict.fromkeys(tokens))\n",
    "        index_records.append({'rid': rid, 'tokens': tokens})\n",
    "    return index_records\n",
    "\n",
    "def find_matches(refs: list, index_records: list, score_cutoff: int = 85):\n",
    "    \"\"\"For each ref, fuzzy-match against index tokens.\"\"\"\n",
    "    candidates = set()\n",
    "    for ref in refs:\n",
    "        for rec in index_records:\n",
    "            for tok in rec['tokens']:\n",
    "                score = fuzz.token_set_ratio(ref, tok)\n",
    "                if score >= score_cutoff:\n",
    "                    candidates.add(rec['rid'])\n",
    "                    break\n",
    "    return list(candidates)\n",
    "\n",
    "def align_sentences(mined_path: str, published_path: str, out_path: str):\n",
    "    pub_df = pd.read_csv(published_path)\n",
    "    for col in ['transliteration', 'publication_catalog', 'aliases', 'label']:\n",
    "        if col not in pub_df.columns:\n",
    "            pub_df[col] = ''\n",
    "    alias_index = build_alias_index(pub_df)\n",
    "\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    written = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration'])\n",
    "\n",
    "        for chunk in pd.read_csv(mined_path, chunksize=5000):\n",
    "            for _, row in chunk.iterrows():\n",
    "                total += 1\n",
    "                pdf = str(row.get('pdf_name', '') or '')\n",
    "                page = int(row.get('page', -1)) if pd.notna(row.get('page')) else -1\n",
    "                sent = str(row.get('english_sentence', '') or '')\n",
    "                if not sent:\n",
    "                    continue\n",
    "                refs = extract_catalog_refs(sent)\n",
    "                if not refs:\n",
    "                    continue\n",
    "                cand_ids = find_matches(refs, alias_index, score_cutoff=85)\n",
    "                for rid in cand_ids:\n",
    "                    t_row = pub_df.iloc[rid]\n",
    "                    matched_label = str(t_row.get('label', '') or '')\n",
    "                    translit = str(t_row.get('transliteration', '') or '')\n",
    "                    if translit:\n",
    "                        writer.writerow([pdf, page, sent, matched_label, translit])\n",
    "                        written += 1\n",
    "            if total % 10000 == 0:\n",
    "                print(f\"Processed {total} sentences; wrote {written} candidate pairs...\")\n",
    "\n",
    "    print(f\"Alignment complete. Total sentences: {total}, candidates written: {written}\")\n",
    "    print(f\"Saved to: {out_path}\")\n",
    "\n",
    "print(\"Starting alignment: mined_publications_en.csv → published_texts.csv\")\n",
    "align_sentences(MINED_EN_PATH, PUBLISHED_TEXTS_PATH, ALIGNED_OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20cbaa7",
   "metadata": {},
   "source": [
    "## ✅ Quality Filter & Summary\n",
    "\n",
    "**⚠️ PREREQUISITE: Run the alignment cell above first to generate `aligned_pairs_candidates.csv`.**\n",
    "\n",
    "Filter aligned pairs for training quality:\n",
    "- Remove pairs where transliteration or English is too short/long\n",
    "- Discard pairs with extreme length ratios (likely misaligned)\n",
    "- Keep pairs with domain terms or high lexicon match\n",
    "- Sample results for sanity check\n",
    "- Output: `aligned_pairs_filtered.csv` ready for training augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185883b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ALIGNED_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n",
    "FILTERED_OUT_PATH = os.getenv('FILTERED_PAIRS_OUT', 'aligned_pairs_filtered.csv')\n",
    "\n",
    "def filter_quality(aligned_path: str, out_path: str):\n",
    "    \"\"\"Filter aligned pairs for training quality.\"\"\"\n",
    "    df = pd.read_csv(aligned_path)\n",
    "    print(f\"Loaded {len(df)} candidate pairs\")\n",
    "    \n",
    "    # Length filters\n",
    "    df['t_len'] = df['transliteration'].str.split().str.len()\n",
    "    df['e_len'] = df['english_sentence'].str.split().str.len()\n",
    "    \n",
    "    # Apply filters\n",
    "    df_filtered = df[\n",
    "        (df['t_len'] >= 3) & (df['t_len'] <= 150) &  # Transliteration length\n",
    "        (df['e_len'] >= 3) & (df['e_len'] <= 150) &  # English length\n",
    "        (df['t_len'] / (df['e_len'] + 1) >= 0.5) &   # Not too different\n",
    "        (df['t_len'] / (df['e_len'] + 1) <= 3.0)\n",
    "    ].copy()\n",
    "    \n",
    "    # Optional: domain term boost (heuristic)\n",
    "    domain_terms = ['tablet', 'seal', 'silver', 'tin', 'letter', 'text', 'archive', 'merchant', 'trade']\n",
    "    df_filtered['has_domain'] = df_filtered['english_sentence'].str.lower().str.contains('|'.join(domain_terms), na=False)\n",
    "    \n",
    "    # Save\n",
    "    df_filtered[['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration']].to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"After quality filtering: {len(df_filtered)} pairs retained\")\n",
    "    print(f\"Saved to: {out_path}\\n\")\n",
    "    \n",
    "    # Sample\n",
    "    print(\"Sample aligned pairs (first 5):\")\n",
    "    for i, row in df_filtered.head(5).iterrows():\n",
    "        print(f\"\\n[{i}]\")\n",
    "        print(f\"  EN: {row['english_sentence'][:80]}...\")\n",
    "        print(f\"  AK: {row['transliteration'][:80]}...\")\n",
    "    \n",
    "    return len(df_filtered)\n",
    "\n",
    "count = filter_quality(ALIGNED_PATH, FILTERED_OUT_PATH)\n",
    "print(f\"\\n✓ Quality filtering complete. {count} high-quality pairs ready for training augmentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b4e44",
   "metadata": {},
   "source": [
    "# A9. MULTI-SOURCE MINING: Extract from Sentences + Publications + Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e66c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-SOURCE MINING: Leverage Sentences_Oare + Publications + Lexicon\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def mine_from_sentences_oare():\n",
    "    \"\"\"\n",
    "    STRATEGY 1: Direct extraction from Sentences_Oare_FirstWord_LinNum.csv\n",
    "    (Already has English translations paired with Akkadian sentences!)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRATEGY 1: Mining Sentences_Oare (Already Translated)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    sentences_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n",
    "    \n",
    "    if not os.path.exists(sentences_path):\n",
    "        print(f\"⚠️ File not found: {sentences_path}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "    \n",
    "    try:\n",
    "        # Load with specific columns\n",
    "        df_sentences = pd.read_csv(sentences_path, dtype={'translation': str})\n",
    "        print(f\"Loaded {len(df_sentences)} sentence rows from Sentences_Oare\")\n",
    "        \n",
    "        # Extract pairs: display_name as source, translation as target\n",
    "        pairs = []\n",
    "        for _, row in df_sentences.iterrows():\n",
    "            src = str(row.get('display_name', '')).strip()\n",
    "            tgt = str(row.get('translation', '')).strip()\n",
    "            \n",
    "            # Validate\n",
    "            if src and tgt and len(src.split()) >= 2 and len(tgt.split()) >= 2:\n",
    "                pairs.append({\"src\": src, \"tgt\": tgt})\n",
    "        \n",
    "        result_df = pd.DataFrame(pairs)\n",
    "        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "        result_df = filter_quality(result_df)\n",
    "        \n",
    "        print(f\"✓ Extracted {len(result_df)} pairs from Sentences_Oare\")\n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Sentences_Oare: {e}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "def mine_from_publications_augmented():\n",
    "    \"\"\"\n",
    "    STRATEGY 2: Extract from publications.csv + match with published_texts.csv\n",
    "    Uses has_akkadian flag + NLTK sentence tokenization\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRATEGY 2: Mining Publications (Akkadian Pages)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pub_path = f\"{DATA_DIR}/publications.csv\"\n",
    "    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    \n",
    "    if not os.path.exists(pub_path):\n",
    "        print(f\"⚠️ File not found: {pub_path}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "    \n",
    "    try:\n",
    "        # Load publications\n",
    "        pubs = pd.read_csv(pub_path, dtype={'has_akkadian': str})\n",
    "        print(f\"Loaded {len(pubs)} publication pages\")\n",
    "        \n",
    "        # Filter for Akkadian pages\n",
    "        akkadian_mask = pubs['has_akkadian'].astype(str).str.lower() == 'true'\n",
    "        pubs_akk = pubs[akkadian_mask].copy()\n",
    "        print(f\"Found {len(pubs_akk)} pages marked with Akkadian\")\n",
    "        \n",
    "        # Extract English sentences using NLTK\n",
    "        mined_sentences = []\n",
    "        for idx, row in pubs_akk.iterrows():\n",
    "            page_text = str(row.get('page_text', ''))\n",
    "            if len(page_text.strip()) < 30:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                sentences = sent_tokenize(page_text)\n",
    "                for sent in sentences:\n",
    "                    sent_clean = sent.strip()\n",
    "                    # Keep sentences with reasonable length\n",
    "                    if 10 <= len(sent_clean) <= 500:\n",
    "                        # Check for English markers (common English words)\n",
    "                        if re.search(r'\\b(the|and|of|to|in|for|a|is|are|be|was|were|or|that|this|with)\\b', \n",
    "                                   sent_clean, re.I):\n",
    "                            mined_sentences.append(sent_clean)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        mined_sentences = list(dict.fromkeys(mined_sentences))  # Deduplicate\n",
    "        print(f\"Extracted {len(mined_sentences)} unique English sentences\")\n",
    "        \n",
    "        # Load Akkadian from published_texts\n",
    "        pub_texts = pd.read_csv(pub_texts_path)\n",
    "        pub_texts_clean = pub_texts.copy()\n",
    "        pub_texts_clean['translit_clean'] = pub_texts_clean['transliteration'].astype(str).apply(\n",
    "            lambda x: clean_translit(x) if isinstance(x, str) else \"\"\n",
    "        )\n",
    "        pub_texts_clean = pub_texts_clean[\n",
    "            (pub_texts_clean['translit_clean'].str.len() > 0) &\n",
    "            (pub_texts_clean['translit_clean'].str.split().str.len() >= 3)\n",
    "        ].reset_index(drop=True)\n",
    "        print(f\"Found {len(pub_texts_clean)} valid Akkadian transliterations\")\n",
    "        \n",
    "        # Create pairs: one random Akkadian per English sentence\n",
    "        pairs = []\n",
    "        if len(pub_texts_clean) > 0:\n",
    "            for sent in mined_sentences:\n",
    "                rand_akk = pub_texts_clean.sample(1).iloc[0]['translit_clean']\n",
    "                pairs.append({\"src\": rand_akk, \"tgt\": sent})\n",
    "        \n",
    "        result_df = pd.DataFrame(pairs)\n",
    "        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "        result_df = filter_quality(result_df)\n",
    "        \n",
    "        print(f\"✓ Created {len(result_df)} pairs from Publications\")\n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error mining publications: {e}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "def mine_from_lexicon_augmentation():\n",
    "    \"\"\"\n",
    "    STRATEGY 3: Use eBL_Dictionary to create word-level or phrase-level augmentations\n",
    "    Map Akkadian words to English definitions for data augmentation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRATEGY 3: Lexicon-Based Augmentation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lex_path = f\"{DATA_DIR}/eBL_Dictionary.csv\"\n",
    "    \n",
    "    if not os.path.exists(lex_path):\n",
    "        print(f\"⚠️ File not found: {lex_path}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "    \n",
    "    try:\n",
    "        df_lex = pd.read_csv(lex_path)\n",
    "        print(f\"Loaded {len(df_lex)} lexicon entries\")\n",
    "        \n",
    "        # Extract word-definition pairs\n",
    "        pairs = []\n",
    "        for _, row in df_lex.iterrows():\n",
    "            word = str(row.get('word', '')).strip()\n",
    "            definition = str(row.get('definition', '')).strip()\n",
    "            \n",
    "            if word and definition and len(word) > 0 and len(definition.split()) >= 2:\n",
    "                # Use cleaned word as source, definition as target\n",
    "                pairs.append({\"src\": word, \"tgt\": definition})\n",
    "        \n",
    "        result_df = pd.DataFrame(pairs)\n",
    "        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "        \n",
    "        print(f\"✓ Created {len(result_df)} word-definition pairs from Lexicon\")\n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading lexicon: {e}\")\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "def combine_mining_sources():\n",
    "    \"\"\"\n",
    "    Orchestrate all mining strategies and combine results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-SOURCE MINING ORCHESTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_pairs = []\n",
    "    source_counts = {}\n",
    "    \n",
    "    # Strategy 1: Sentences_Oare (highest priority - already translated)\n",
    "    print(\"\\n>>> Executing Strategy 1...\")\n",
    "    s1 = mine_from_sentences_oare()\n",
    "    if len(s1) > 0:\n",
    "        all_pairs.append(s1)\n",
    "        source_counts[\"Sentences_Oare\"] = len(s1)\n",
    "        print(f\"    ✓ {len(s1)} pairs added\")\n",
    "    \n",
    "    # Strategy 2: Publications (sentence extraction)\n",
    "    print(\"\\n>>> Executing Strategy 2...\")\n",
    "    s2 = mine_from_publications_augmented()\n",
    "    if len(s2) > 0:\n",
    "        all_pairs.append(s2)\n",
    "        source_counts[\"Publications\"] = len(s2)\n",
    "        print(f\"    ✓ {len(s2)} pairs added\")\n",
    "    \n",
    "    # Strategy 3: Lexicon augmentation\n",
    "    print(\"\\n>>> Executing Strategy 3...\")\n",
    "    s3 = mine_from_lexicon_augmentation()\n",
    "    if len(s3) > 0:\n",
    "        all_pairs.append(s3)\n",
    "        source_counts[\"Lexicon\"] = len(s3)\n",
    "        print(f\"    ✓ {len(s3)} pairs added\")\n",
    "    \n",
    "    # Combine all sources\n",
    "    if all_pairs:\n",
    "        combined = pd.concat(all_pairs, ignore_index=True)\n",
    "        combined = combined.drop_duplicates(subset=['src', 'tgt'])\n",
    "        combined = filter_quality(combined)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MINING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source:20s}: {count:6d} pairs\")\n",
    "        print(f\"  {'─'*20}  {'─'*6}\")\n",
    "        print(f\"  {'TOTAL':20s}: {len(combined):6d} pairs\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return combined\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n",
    "\n",
    "\n",
    "# Execute multi-source mining\n",
    "print(\"\\n\" + \"█\"*70)\n",
    "print(\"█\" + \" \"*68 + \"█\")\n",
    "print(\"█\" + \"  MULTI-SOURCE MINING PIPELINE - THINKING OUTSIDE THE BOX\".center(68) + \"█\")\n",
    "print(\"█\" + \" \"*68 + \"█\")\n",
    "print(\"█\"*70)\n",
    "\n",
    "mined_df = combine_mining_sources()\n",
    "\n",
    "# Load main training data\n",
    "train_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n",
    "\n",
    "# Merge with mined data\n",
    "if len(mined_df) > 0:\n",
    "    print(f\"\\n🔗 Merging {len(mined_df)} mined examples with {len(train_df)} supervised examples...\")\n",
    "    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n",
    "    train_df = train_df.drop_duplicates(subset=['src', 'tgt'])\n",
    "    print(f\"✓ Final dataset: {len(train_df)} total pairs\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ No mined data; using supervised data only: {len(train_df)} pairs\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Train: {len(dataset['train'])} examples\")\n",
    "print(f\"  Val:   {len(dataset['test'])} examples\")\n",
    "print(\"\\n✓ Data pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9082937,
     "sourceId": 14236819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6059.435801,
   "end_time": "2025-12-25T12:09:11.716859",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T10:28:12.281058",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1fe03852120a40cda6d5c85f4e29fcaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b48bf215217541358d86be12f43b7f3c",
       "placeholder": "​",
       "style": "IPY_MODEL_fcaea7f32ee24eebaebfc0e32266d44a",
       "tabbable": null,
       "tooltip": null,
       "value": " 8.15k/? [00:00&lt;00:00, 928kB/s]"
      }
     },
     "226d44a1d8884129a1d1230b2642a786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b0c8da6c308b446cb01170a376a943c3",
       "max": 77,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_39da431ed6e041ef8c50061ff0cb2734",
       "tabbable": null,
       "tooltip": null,
       "value": 77
      }
     },
     "2bccb998d49d415580aaed49b33a1b22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_791efc0e593d4599ba993239183a55e3",
       "placeholder": "​",
       "style": "IPY_MODEL_683b28591d2f4363aa74a0a1a44c7a1c",
       "tabbable": null,
       "tooltip": null,
       "value": " 77/77 [00:00&lt;00:00, 388.82 examples/s]"
      }
     },
     "2d21f5383a1548d98e116b290031fccd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39da431ed6e041ef8c50061ff0cb2734": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3b23edd7473a4b5cbeb0e7453bf9b6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "427b3335b5374628a175042c3a3382e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d21f5383a1548d98e116b290031fccd",
       "placeholder": "​",
       "style": "IPY_MODEL_9aed8f291fff48a5a708caf8c20013d8",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading builder script: "
      }
     },
     "44b7b6316c1c4a4bbc5e3b84e174e79f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6a4941ccd41469aa09a0ec5a804af6e",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b76718f5d5b1422ab37b370c9cc527ff",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "45c5ff13e0804ab38df84c594f5cbfe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c11ec6efe7cb4722883a279f3eb185ed",
       "placeholder": "​",
       "style": "IPY_MODEL_8846ab600ef243c39c4f1cb71a308f59",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading builder script: "
      }
     },
     "4a7eec71c5664bbd922f48d02fb439ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b48b9cb404748bdba5a1554f4687518": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e865f98ee6084edf9b9c63807b52da6b",
        "IPY_MODEL_9f53690cf1d84b5aa3e3929a66799044",
        "IPY_MODEL_f030f089b1124566ab437b62c99b2578"
       ],
       "layout": "IPY_MODEL_c2b4df78e626401c8a097c7890f1386e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6297171298534da8a8ae461611777771": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "67bff04e3ed849f8aadf4b6b7df2b9e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_427b3335b5374628a175042c3a3382e4",
        "IPY_MODEL_44b7b6316c1c4a4bbc5e3b84e174e79f",
        "IPY_MODEL_1fe03852120a40cda6d5c85f4e29fcaf"
       ],
       "layout": "IPY_MODEL_b8d2e1271f1e4afdb533651d811a2bb8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "683b28591d2f4363aa74a0a1a44c7a1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c90b63b47dc4de495594cfd1e6519b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "701dd03462844cf0af47a3ab8c452a04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_45c5ff13e0804ab38df84c594f5cbfe4",
        "IPY_MODEL_bc4d79d1c6384c23a736957e90afd304",
        "IPY_MODEL_9d58cd0d03464cd18c653fdec13d87e4"
       ],
       "layout": "IPY_MODEL_b0bcd130aa8341b1acef83b7cc545dce",
       "tabbable": null,
       "tooltip": null
      }
     },
     "71affa5ac8894c80959fbb0448728c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c324f021e0034d85bcf6459f17a7a752",
       "placeholder": "​",
       "style": "IPY_MODEL_6297171298534da8a8ae461611777771",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "791efc0e593d4599ba993239183a55e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8846ab600ef243c39c4f1cb71a308f59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f98493e151546f2baa629a4edf06a14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9369db00183844a5b53806dd64a214db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "969e0b0841ff4999a85c76101abdcc41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "9aed8f291fff48a5a708caf8c20013d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d58cd0d03464cd18c653fdec13d87e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b789e817126e4af1a184cfae19f2612f",
       "placeholder": "​",
       "style": "IPY_MODEL_3b23edd7473a4b5cbeb0e7453bf9b6bc",
       "tabbable": null,
       "tooltip": null,
       "value": " 9.01k/? [00:00&lt;00:00, 914kB/s]"
      }
     },
     "9f53690cf1d84b5aa3e3929a66799044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab64a8242ff94bdfbfceeabac64877cf",
       "max": 1452,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0f4b1fc30b24fd1a84d58113003352a",
       "tabbable": null,
       "tooltip": null,
       "value": 1452
      }
     },
     "a1151781b3e84a738ae66c7ac283ba3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_71affa5ac8894c80959fbb0448728c81",
        "IPY_MODEL_226d44a1d8884129a1d1230b2642a786",
        "IPY_MODEL_2bccb998d49d415580aaed49b33a1b22"
       ],
       "layout": "IPY_MODEL_4a7eec71c5664bbd922f48d02fb439ac",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a8e873b972004c38ae11c1d76cfe84ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ab64a8242ff94bdfbfceeabac64877cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae69eb96031c4d43bf4c52da5aedb198": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0bcd130aa8341b1acef83b7cc545dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0c8da6c308b446cb01170a376a943c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b48bf215217541358d86be12f43b7f3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b76718f5d5b1422ab37b370c9cc527ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b789e817126e4af1a184cfae19f2612f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8d2e1271f1e4afdb533651d811a2bb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc4d79d1c6384c23a736957e90afd304": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_969e0b0841ff4999a85c76101abdcc41",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a8e873b972004c38ae11c1d76cfe84ba",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "c11ec6efe7cb4722883a279f3eb185ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2b4df78e626401c8a097c7890f1386e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c324f021e0034d85bcf6459f17a7a752": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0f4b1fc30b24fd1a84d58113003352a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e6a4941ccd41469aa09a0ec5a804af6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "e865f98ee6084edf9b9c63807b52da6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ae69eb96031c4d43bf4c52da5aedb198",
       "placeholder": "​",
       "style": "IPY_MODEL_8f98493e151546f2baa629a4edf06a14",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "f030f089b1124566ab437b62c99b2578": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9369db00183844a5b53806dd64a214db",
       "placeholder": "​",
       "style": "IPY_MODEL_6c90b63b47dc4de495594cfd1e6519b0",
       "tabbable": null,
       "tooltip": null,
       "value": " 1452/1452 [00:03&lt;00:00, 448.42 examples/s]"
      }
     },
     "fcaea7f32ee24eebaebfc0e32266d44a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
