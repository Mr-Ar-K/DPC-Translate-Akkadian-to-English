{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"},{"sourceId":14290093,"sourceType":"datasetVersion","datasetId":9084006},{"sourceId":14290098,"sourceType":"datasetVersion","datasetId":9084005},{"sourceId":14290100,"sourceType":"datasetVersion","datasetId":9084224}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# DEEP PAST CHALLENGE - SINGLE HYBRID PIPELINE (Neural Ensemble Only)\nimport os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom tqdm.auto import tqdm\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# -----------------------------------------------------------------------------\n# CONFIG\n# -----------------------------------------------------------------------------\ndef resolve_path(env_name, working_default, input_fallback):\n    path = os.getenv(env_name, working_default)\n    if not os.path.exists(path) and os.path.exists(input_fallback):\n        path = input_fallback\n    return path\n\nMODEL_PATHS = {\n    \"byt5\": resolve_path(\"BYT5_PATH\", \"/kaggle/working/byt5-base-saved\", \"/kaggle/input/notebook-a-byt5/byt5-base-saved\"),\n    \"t5\": resolve_path(\"T5_PATH\", \"/kaggle/working/t5-base-fine-tuned\", \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\"),\n    \"marian\": resolve_path(\"MARIAN_PATH\", \"/kaggle/working/marian-mt-saved\", \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"),\n}\n\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\n\n# -----------------------------------------------------------------------------\n# CLEANING HELPERS (match training notebooks)\n# -----------------------------------------------------------------------------\nSUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n\ndef normalize_subscripts(text: str) -> str:\n    return text.translate(SUBSCRIPT_TRANS)\n\ndef clean_translit(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = normalize_subscripts(text)\n    text = text.replace(\"…\", \" <big_gap> \")\n    text = re.sub(r\"\\\\.\\\\.\\\\.+\", \" <big_gap> \", text)\n    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n    text = re.sub(r\"<<[^>]*>>\", \" \", text)\n    text = re.sub(r\"[˹˺]\", \" \", text)\n    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)\n    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n    text = re.sub(r\"[!?/:·]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef clean_translation(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.replace(\"…\", \" \")\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef dedup_repeats(text: str) -> str:\n    tokens = text.split()\n    out = []\n    for tok in tokens:\n        if len(out) >= 2 and tok == out[-1] == out[-2]:\n            continue\n        out.append(tok)\n    return \" \".join(out)\n\ndef postprocess_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.strip()\n    text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n    text = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", text)\n    text = dedup_repeats(text)\n    if text and text[0].islower():\n        text = text[0].upper() + text[1:]\n    if text and text[-1] not in \".!?\":\n        text += \".\"\n    text = re.sub(r\"([.!?]){2,}\", \".\", text)\n    return text.strip()\n\ndef sanitize_outputs(primary, backups):\n    cleaned = []\n    for i, text in enumerate(primary):\n        cand = postprocess_text(text)\n        if len(cand) < 5 or cand.lower() in {\"unknown\", \"\"}:\n            fallbacks = [postprocess_text(b[i]) for b in backups if i < len(b) and len(postprocess_text(b[i])) >= 5]\n            if fallbacks:\n                cand = max(fallbacks, key=len)\n            else:\n                cand = \"Unknown\"\n        cleaned.append(cand)\n    return cleaned\n\n# -----------------------------------------------------------------------------\n# DATASET + GENERATION\n# -----------------------------------------------------------------------------\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\n        self.texts = [prefix + str(t) for t in texts]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n        }\n\ndef generate_predictions(model_name, model_path, inputs):\n    if not os.path.exists(model_path):\n        print(f\"[WARNING] Missing model for {model_name}: {model_path}\")\n        return [\"\"] * len(inputs)\n\n    if \"byt5\" in model_name:\n        max_len = 420; prefix = \"translate Akkadian to English: \"; beams = 8\n    elif \"t5\" in model_name:\n        max_len = 280; prefix = \"translate Akkadian to English: \"; beams = 8\n    else:\n        max_len = 180; prefix = \"\"; beams = 6\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n    except Exception as e:\n        print(f\"[ERROR] Failed to load {model_name}: {e}\")\n        return [\"\"] * len(inputs)\n\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n    preds = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\n            gen_ids = model.generate(\n                input_ids=batch[\"input_ids\"].to(DEVICE),\n                attention_mask=batch[\"attention_mask\"].to(DEVICE),\n                max_length=max_len,\n                min_length=6,\n                num_beams=beams,\n                no_repeat_ngram_size=3,\n                repetition_penalty=1.08,\n                length_penalty=1.05,\n                early_stopping=True,\n            )\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n            preds.extend([postprocess_text(d) for d in decoded])\n\n    del model, tokenizer, dataset, loader\n    torch.cuda.empty_cache(); gc.collect()\n    return preds\n\n# -----------------------------------------------------------------------------\n# ENSEMBLE\n# -----------------------------------------------------------------------------\ndef score_candidate(text: str, src_len: int, base_weight: float) -> float:\n    tok_len = max(1, len(text.split()))\n    ratio = tok_len / max(1, src_len)\n    length_penalty = -abs(ratio - 1.3) * 0.8  # prefer around 1.3x source tokens\n    short_penalty = -3.0 if tok_len < 4 else 0.0\n    garbage_penalty = -4.0 if text.lower() in {\"unknown\", \"\"} else 0.0\n    return base_weight + length_penalty + short_penalty + garbage_penalty\n\ndef neural_vote(preds_dict, src_lens):\n    weights = {\"byt5\": 4.0, \"t5\": 1.6, \"marian\": 1.2}\n    final = []\n    n = max(len(v) for v in preds_dict.values()) if preds_dict else 0\n    for i in range(n):\n        best_text, best_score = \"\", -1e9\n        for name, preds in preds_dict.items():\n            if i >= len(preds):\n                continue\n            score = score_candidate(preds[i], src_lens[i], weights.get(name, 1.0))\n            if score > best_score:\n                best_score, best_text = score, preds[i]\n        final.append(best_text)\n    return final\n\n# -----------------------------------------------------------------------------\n# MAIN PIPELINE (Neural-only)\n# -----------------------------------------------------------------------------\ndef main():\n    print(\"=== Deep Past Neural Ensemble Inference ===\")\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n\n    test_inputs_raw = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\n    test_inputs = [clean_translit(t) for t in test_inputs_raw]\n    src_lens = [len(t.split()) for t in test_inputs]\n\n    neural_preds = {\n        \"byt5\": generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], test_inputs),\n        \"t5\": generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], test_inputs),\n        \"marian\": generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], test_inputs),\n    }\n    ensemble_preds = neural_vote(neural_preds, src_lens)\n\n    final_outputs = sanitize_outputs(ensemble_preds, list(neural_preds.values()))\n    submission = pd.DataFrame({\"id\": test_df[\"id\"], \"translation\": final_outputs})\n    submission[\"translation\"] = submission[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Unknown\")\n    submission.to_csv(\"submission.csv\", index=False)\n\n    print(\"\\nPreview:\")\n    print(submission.head())\n    return submission\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-12-25T12:14:36.175605Z","iopub.execute_input":"2025-12-25T12:14:36.175832Z","iopub.status.idle":"2025-12-25T12:15:40.422643Z","shell.execute_reply.started":"2025-12-25T12:14:36.175811Z","shell.execute_reply":"2025-12-25T12:15:40.422026Z"},"trusted":true},"outputs":[{"name":"stdout","text":"=== Deep Past Neural Ensemble Inference ===\n","output_type":"stream"},{"name":"stderr","text":"2025-12-25 12:14:51.095337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766664891.298313      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766664891.357816      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766664891.851037      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766664891.851082      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766664891.851085      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766664891.851088      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Inference byt5:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286a0586afeb435ea31d31b4777fc8d5"}},"metadata":{}},{"name":"stderr","text":"You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Inference t5:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453f6b3dd3304ef99924faf6e3a6a131"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Inference marian:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e9f6a77e75d47d4adc7de4402091a55"}},"metadata":{}},{"name":"stdout","text":"\nPreview:\n   id                                        translation\n0   0  Kà-ar-ma ú big_gap da-tim aí-ip-ri-ni Akkadian...\n1   1  -ni i-na né-mì-lim da-aùr ú-lá e-WA ia-ra-tí-a...\n2   2  -it a-aí-im au-um-au ia-tí aé-bi„-lá-nim Trans...\n3   3  É-bi„-lá KÙ. AN Translate Akkadian to English:...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}