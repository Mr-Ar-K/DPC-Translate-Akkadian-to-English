{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"},{"sourceId":14236819,"sourceType":"datasetVersion","datasetId":9082937}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":867.451795,"end_time":"2025-12-25T11:06:53.451666","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-25T10:52:25.999871","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"077ae2461fb6489b97106a1caaafb370":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb6470d874c040e698b88b4827bb0052","IPY_MODEL_772639f9a8104921b6f3b1caca957605","IPY_MODEL_c0e6baa9bf9a411b844486a5ac4eb0f2"],"layout":"IPY_MODEL_bd2909e5aebc4611a9e453f84a7e8035","tabbable":null,"tooltip":null}},"0adb4c94b7b74e03b9969a916a78f816":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a1fc3087f004cdcbe55deb12e9b9d55":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2adf7045efe64b7cb4c5556604eb75e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2d2eba874f204b159a6a962c13f13b19":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d432c967de54ce58ca8f97185c3edd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_907a9c57f14d46909be51efea7772bfe","max":1452,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfea59b2310d4bf7a765969e33c563e4","tabbable":null,"tooltip":null,"value":1452}},"310b3b3d8e6e4913bf9b553d7df21e92":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4948d8ef30b94d23ab5670946a3aea08":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"4ecbe0b10d9f4af2862caa0ea4819ad3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62b2c58f9ef642879c9120277b64490f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_ab9961958af74ef6b85fd74f35f7ab79","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2e79e4817c1410fad2a93325460dab6","tabbable":null,"tooltip":null,"value":1}},"6bfa7186924b4ef1b09723815e8039e7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e0dd8d999ad42c98136d00236800e95":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e3c59e812f44e02bbde7360657be1e9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7212b9bcee854738bc028dc516c6d9e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6bfa7186924b4ef1b09723815e8039e7","placeholder":"​","style":"IPY_MODEL_9c4f0d0224314af5a5348f524290228d","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"757491e8dcfb4e53bbde385ec37d11c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76050bf59c0b404eb86de944a9d2f440":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"772639f9a8104921b6f3b1caca957605":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_cbcf3f903c0142bb919b4c6c873027b8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ecbe0b10d9f4af2862caa0ea4819ad3","tabbable":null,"tooltip":null,"value":1}},"8ae65202365844b3823b46cc09af0d8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7212b9bcee854738bc028dc516c6d9e0","IPY_MODEL_62b2c58f9ef642879c9120277b64490f","IPY_MODEL_d254aa49e2c748839b11c3ae30b3ed4b"],"layout":"IPY_MODEL_0adb4c94b7b74e03b9969a916a78f816","tabbable":null,"tooltip":null}},"907a9c57f14d46909be51efea7772bfe":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939f39d353254f2188c3154b27045a38":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6e0dd8d999ad42c98136d00236800e95","placeholder":"​","style":"IPY_MODEL_4948d8ef30b94d23ab5670946a3aea08","tabbable":null,"tooltip":null,"value":" 1452/1452 [00:02&lt;00:00, 685.40 examples/s]"}},"9c4f0d0224314af5a5348f524290228d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a12b2c8b8d9045ad9880827234dd0592":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a151a03ca0304312ade7c4c2a3fca88b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a2e79e4817c1410fad2a93325460dab6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab9961958af74ef6b85fd74f35f7ab79":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ac4c4df806d943919a176f5abba4439e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2d2eba874f204b159a6a962c13f13b19","placeholder":"​","style":"IPY_MODEL_ea4d9cb6237b410fa26d0ef017695535","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"b584c545f2a04b22a8684541f3716bb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f29b32d0d9ff4816ae71db8424c042ee","IPY_MODEL_2d432c967de54ce58ca8f97185c3edd5","IPY_MODEL_939f39d353254f2188c3154b27045a38"],"layout":"IPY_MODEL_ed16ef8fdffb4b9b8536b8c6e9d727bf","tabbable":null,"tooltip":null}},"b999aa2ad3044b9cb4a67d342fc9aabc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac4c4df806d943919a176f5abba4439e","IPY_MODEL_d6dab87b9b4b4b0fa1f6ca17928ae214","IPY_MODEL_f4856502a22146fb8fde2d23cb451348"],"layout":"IPY_MODEL_6e3c59e812f44e02bbde7360657be1e9","tabbable":null,"tooltip":null}},"bd2909e5aebc4611a9e453f84a7e8035":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0e6baa9bf9a411b844486a5ac4eb0f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_310b3b3d8e6e4913bf9b553d7df21e92","placeholder":"​","style":"IPY_MODEL_d3822816c9214e8296dbc9d8f6a53130","tabbable":null,"tooltip":null,"value":" 9.01k/? [00:00&lt;00:00, 925kB/s]"}},"ca6c0b7ac45e453daf901dbe2533d931":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbcf3f903c0142bb919b4c6c873027b8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d254aa49e2c748839b11c3ae30b3ed4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a12b2c8b8d9045ad9880827234dd0592","placeholder":"​","style":"IPY_MODEL_2adf7045efe64b7cb4c5556604eb75e9","tabbable":null,"tooltip":null,"value":" 8.15k/? [00:00&lt;00:00, 832kB/s]"}},"d29e22650257471d8935422c67a4c6b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d3822816c9214e8296dbc9d8f6a53130":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"d6dab87b9b4b4b0fa1f6ca17928ae214":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_2a1fc3087f004cdcbe55deb12e9b9d55","max":77,"min":0,"orientation":"horizontal","style":"IPY_MODEL_757491e8dcfb4e53bbde385ec37d11c7","tabbable":null,"tooltip":null,"value":77}},"da9bc0ca3f1e42268d44dd4984814919":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfea59b2310d4bf7a765969e33c563e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea4d9cb6237b410fa26d0ef017695535":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"eb6470d874c040e698b88b4827bb0052":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ca6c0b7ac45e453daf901dbe2533d931","placeholder":"​","style":"IPY_MODEL_f1f41397162d4f36aba6f0441042960e","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"ed16ef8fdffb4b9b8536b8c6e9d727bf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1f41397162d4f36aba6f0441042960e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f29b32d0d9ff4816ae71db8424c042ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_76050bf59c0b404eb86de944a9d2f440","placeholder":"​","style":"IPY_MODEL_d29e22650257471d8935422c67a4c6b3","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"f4856502a22146fb8fde2d23cb451348":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_da9bc0ca3f1e42268d44dd4984814919","placeholder":"​","style":"IPY_MODEL_a151a03ca0304312ade7c4c2a3fca88b","tabbable":null,"tooltip":null,"value":" 77/77 [00:00&lt;00:00, 599.94 examples/s]"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"cb5f8de2","cell_type":"code","source":"!nvidia-smi","metadata":{"papermill":{"duration":0.184478,"end_time":"2025-12-25T10:52:28.811448","exception":false,"start_time":"2025-12-25T10:52:28.626970","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d9ba94f5","cell_type":"code","source":"!pip install -q evaluate sacrebleu","metadata":{"papermill":{"duration":4.831554,"end_time":"2025-12-25T10:52:33.649527","exception":false,"start_time":"2025-12-25T10:52:28.817973","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"66804faa","cell_type":"markdown","source":"# B1. Imports & Configuration","metadata":{"papermill":{"duration":0.006473,"end_time":"2025-12-25T10:52:33.662736","exception":false,"start_time":"2025-12-25T10:52:33.656263","status":"completed"},"tags":[]}},{"id":"da891048","cell_type":"code","source":"import os\nimport re\nimport gc\nimport pandas as pd\nimport torch\nimport evaluate\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    DataCollatorForSeq2Seq, \n    Seq2SeqTrainer, \n    Seq2SeqTrainingArguments,\n    set_seed,\n )\n\n# Memory safety tweaks\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntry:\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.benchmark = False\n    torch.set_float32_matmul_precision(\"medium\")\nexcept Exception:\n    pass\n\n# --- Configuration ---\nMODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/t5-base\"\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nOUTPUT_DIR = \"/kaggle/working/t5-base-fine-tuned\"\n\nMAX_LENGTH = 128\nPREFIX = \"translate Akkadian to English: \"\n\nset_seed(42)","metadata":{"papermill":{"duration":31.938008,"end_time":"2025-12-25T10:53:05.606920","exception":false,"start_time":"2025-12-25T10:52:33.668912","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"60ffc9ab","cell_type":"markdown","source":"# B2. Data Loading & Alignment","metadata":{"papermill":{"duration":0.006319,"end_time":"2025-12-25T10:53:05.620113","exception":false,"start_time":"2025-12-25T10:53:05.613794","status":"completed"},"tags":[]}},{"id":"1b0109c6","cell_type":"markdown","source":"# B1.5. DATA PREPARATION GUIDE: Handling Akkadian Formatting Issues\n\n## Problem: \"Garbage In, Garbage Out\"\nAkkadian texts contain complex formatting that can break ML pipelines if not handled properly.\n\n## Formatting Issues to Handle\n\n### 1. Scribal Notations (Remove)\n- `!` - Certain reading (remove)\n- `?` - Questionable reading (remove)\n- `/` - Line divider (remove)\n- `:` or `.` - Word divider (remove)\n- `< >` - Scribal insertions (keep content, remove brackets)\n- `( )` - Comments/erasures (remove entirely)\n- `˹ ˺` - Half brackets for partially broken signs (remove)\n- `[ ]` - Clearly broken signs (keep content, remove brackets)\n- `<< >>` - Errant signs (remove entirely)\n\n### 2. Gaps & Lacunae (Standardize)\n- `[x]` → `<gap>`\n- `x` → `<gap>`\n- `xx` → `<gap>`\n- `…` → `<big_gap>`\n- `……` → `<big_gap>`\n- `[... ...]` → `<big_gap>`\n- Multiple `.3` or `...` sequences → `<big_gap>`\n\n### 3. Determinatives (Keep content, remove brackets)\n- `{d}` - Deity (remove brackets)\n- `{ki}` - Earth/location (remove brackets)\n- `{lu₂}` - Person (remove brackets)\n- `{e₂}` - Building (remove brackets)\n- And 10+ others...\n\n### 4. Subscripts & Superscripts (Normalize)\n- `a₂` → `a2`, `a₃` → `a3`, etc.\n- `il₅` → `il5`, etc.\n- Works with Unicode characters (U+2080-U+2089)\n\n### 5. Special Characters (Handle as-is or normalize)\n- `š` (U+0161), `Š` (U+0160)\n- `ṣ` (U+1E63), `Ṣ` (U+1E62)\n- `ṭ` (U+1E6D), `Ṭ` (U+1E6C)\n- `ḫ` (U+1E2B), `Ḫ` (U+1E2A)\n- `ʾ` (U+02BE) - Akkadian letter marker\n\n### 6. Capitalization Rules (Preserve)\n- First letter capital = Proper noun (personal/place name)\n- ALL CAPS = Sumerian logogram (preserve for domain knowledge)\n\n## Processing Order\n1. Normalize subscripts FIRST (₀-₉ → 0-9)\n2. Handle gaps (complex patterns first, then simple)\n3. Remove scribal notations\n4. Extract content from bracketed structures\n5. Clean whitespace\n6. Validate output (length checks, character validation)\n\n## Data Validation Checks\n✓ No empty strings after cleaning\n✓ Source length >= 3 words\n✓ Target length >= 3 words\n✓ Length ratio between 0.2 and 5.0\n✓ No duplicate pairs\n✓ All special characters properly handled","metadata":{}},{"id":"14116bf4","cell_type":"code","source":"SUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n\ndef normalize_subscripts(text: str) -> str:\n    return text.translate(SUBSCRIPT_TRANS)\n\ndef replace_gaps(text, keep_gaps=True):\n    \"\"\"Replace various gap notations with standardized tokens\n    \n    Args:\n        keep_gaps: If True, keeps gap tokens (for test-like data).\n                   If False, removes them (for clean training).\n    \"\"\"\n    if pd.isna(text): \n        return text\n    \n    # Complex gap patterns (order matters)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n\n    # Simple gap patterns\n    text = re.sub(r'xx', '<gap>', text)\n    text = re.sub(r' x ', ' <gap> ', text)\n    text = re.sub(r'……', '<big_gap>', text)\n    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n    text = re.sub(r'…', '<big_gap>', text)\n    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n    \n    if not keep_gaps:\n        # Remove gaps for clean training\n        text = re.sub(r'<big_gap>', '', text)\n        text = re.sub(r'<gap>', '', text)\n\n    return text\n\ndef clean_translit(text, keep_gaps=True):\n    \"\"\"Normalize transliteration following competition guidance.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = normalize_subscripts(text)\n    text = replace_gaps(text, keep_gaps=keep_gaps)\n    text = re.sub(r\"<<[^>]*>>\", \" \", text)               # errant signs\n    text = re.sub(r\"[˹˺]\", \" \", text)                    # half brackets\n    text = re.sub(r\"\\([^)]*\\)\", \" \", text)             # comments/erasures\n    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)         # determinatives\n    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)            # scribal insertions keep content\n    text = re.sub(r\"[!?/:·]\", \" \", text)                 # scribal punctuation\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef clean_translation(text, has_gaps=False):\n    \"\"\"Clean translation, optionally keeping gap indicators\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    if not has_gaps:\n        text = text.replace(\"…\", \" \")\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef filter_quality(df):\n    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n    df = df[(ratio >= 0.2) & (ratio <= 5)]\n    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n\ndef load_and_align_data(filepath):\n    \"\"\"\n    Enhanced alignment with sentence-level mapping support\n    \"\"\"\n    df = pd.read_csv(filepath)\n    print(f\"Raw documents: {len(df)}\")\n    \n    aligned_rows = []\n\n    for _, row in df.iterrows():\n        src = clean_translit(row.get(\"transliteration\", \"\"), keep_gaps=True)\n        tgt = clean_translation(row.get(\"translation\", \"\"))\n\n        src_lines = [s.strip() for s in src.split(\"\\n\") if s.strip()]\n        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n\n        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n            for s, t in zip(src_lines, tgt_sents):\n                if len(s) > 3 and len(t) > 3:\n                    aligned_rows.append({\"transliteration\": s, \"translation\": t})\n        else:\n            merged_src = src.replace(\"\\n\", \" \")\n            if len(merged_src) > 3 and len(tgt) > 3:\n                aligned_rows.append({\"transliteration\": merged_src, \"translation\": tgt})\n\n    print(f\"Aligned training examples (pre-filter): {len(aligned_rows)}\")\n    out_df = filter_quality(pd.DataFrame(aligned_rows))\n    print(f\"Aligned training examples (post-filter): {len(out_df)}\")\n    return out_df\n\ndef mine_from_sentences_oare():\n    \"\"\"STRATEGY 1: Direct from Sentences_Oare (Already Translated)\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 1: Mining Sentences_Oare (Already Translated)\")\n    print(\"=\"*70)\n    \n    sentences_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n    if not os.path.exists(sentences_path):\n        print(f\"⚠️ File not found: {sentences_path}\")\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n    \n    try:\n        df_sentences = pd.read_csv(sentences_path, dtype={'translation': str})\n        print(f\"Loaded {len(df_sentences)} sentence rows\")\n        \n        pairs = []\n        for _, row in df_sentences.iterrows():\n            src = str(row.get('display_name', '')).strip()\n            tgt = str(row.get('translation', '')).strip()\n            \n            if src and tgt and len(src.split()) >= 2 and len(tgt.split()) >= 2:\n                pairs.append({\"transliteration\": src, \"translation\": tgt})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['transliteration', 'translation'])\n        result_df = filter_quality(result_df)\n        \n        print(f\"✓ Extracted {len(result_df)} pairs from Sentences_Oare\")\n        return result_df\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n\n\ndef mine_from_publications_augmented():\n    \"\"\"STRATEGY 2: Publications (Sentence Extraction + Pairing)\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 2: Mining Publications (Akkadian Pages)\")\n    print(\"=\"*70)\n    \n    pub_path = f\"{DATA_DIR}/publications.csv\"\n    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n    \n    if not os.path.exists(pub_path):\n        print(f\"⚠️ File not found: {pub_path}\")\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n    \n    try:\n        pubs = pd.read_csv(pub_path, dtype={'has_akkadian': str})\n        akkadian_mask = pubs['has_akkadian'].astype(str).str.lower() == 'true'\n        pubs_akk = pubs[akkadian_mask].copy()\n        print(f\"Found {len(pubs_akk)} pages with Akkadian\")\n        \n        # Extract sentences\n        import nltk\n        try:\n            nltk.data.find('tokenizers/punkt')\n        except LookupError:\n            nltk.download('punkt')\n        from nltk.tokenize import sent_tokenize\n        \n        mined_sentences = []\n        for _, row in pubs_akk.iterrows():\n            page_text = str(row.get('page_text', ''))\n            if len(page_text.strip()) < 30:\n                continue\n            try:\n                sentences = sent_tokenize(page_text)\n                for sent in sentences:\n                    sent_clean = sent.strip()\n                    if 10 <= len(sent_clean) <= 500:\n                        if re.search(r'\\b(the|and|of|to|in|for|a|is|are|be|was|were|or|that|this|with)\\b', \n                                   sent_clean, re.I):\n                            mined_sentences.append(sent_clean)\n            except:\n                continue\n        \n        mined_sentences = list(dict.fromkeys(mined_sentences))\n        print(f\"Extracted {len(mined_sentences)} unique sentences\")\n        \n        # Load Akkadian\n        pub_texts = pd.read_csv(pub_texts_path)\n        pub_texts_clean = pub_texts.copy()\n        pub_texts_clean['translit_clean'] = pub_texts_clean['transliteration'].astype(str).apply(\n            lambda x: clean_translit(x) if isinstance(x, str) else \"\"\n        )\n        pub_texts_clean = pub_texts_clean[\n            (pub_texts_clean['translit_clean'].str.len() > 0) &\n            (pub_texts_clean['translit_clean'].str.split().str.len() >= 3)\n        ].reset_index(drop=True)\n        print(f\"Found {len(pub_texts_clean)} Akkadian transliterations\")\n        \n        # Pair\n        pairs = []\n        if len(pub_texts_clean) > 0:\n            for sent in mined_sentences:\n                rand_akk = pub_texts_clean.sample(1).iloc[0]['translit_clean']\n                pairs.append({\"transliteration\": rand_akk, \"translation\": sent})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['transliteration', 'translation'])\n        result_df = filter_quality(result_df)\n        \n        print(f\"✓ Created {len(result_df)} pairs from Publications\")\n        return result_df\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n\n\ndef mine_from_lexicon_augmentation():\n    \"\"\"STRATEGY 3: Lexicon-Based Word-Definition Pairs\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 3: Lexicon-Based Augmentation\")\n    print(\"=\"*70)\n    \n    lex_path = f\"{DATA_DIR}/eBL_Dictionary.csv\"\n    \n    if not os.path.exists(lex_path):\n        print(f\"⚠️ File not found: {lex_path}\")\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n    \n    try:\n        df_lex = pd.read_csv(lex_path)\n        print(f\"Loaded {len(df_lex)} lexicon entries\")\n        \n        pairs = []\n        for _, row in df_lex.iterrows():\n            word = str(row.get('word', '')).strip()\n            definition = str(row.get('definition', '')).strip()\n            \n            if word and definition and len(definition.split()) >= 2:\n                pairs.append({\"transliteration\": word, \"translation\": definition})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['transliteration', 'translation'])\n        \n        print(f\"✓ Created {len(result_df)} word-definition pairs\")\n        return result_df\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n\n\ndef combine_mining_sources():\n    \"\"\"Orchestrate all mining strategies\"\"\"\n    print(\"\\n\" + \"█\"*70)\n    print(\"█\" + \"  MULTI-SOURCE MINING PIPELINE\".center(68) + \"█\")\n    print(\"█\"*70)\n    \n    all_pairs = []\n    source_counts = {}\n    \n    print(\"\\n>>> Strategy 1: Sentences_Oare...\")\n    s1 = mine_from_sentences_oare()\n    if len(s1) > 0:\n        all_pairs.append(s1)\n        source_counts[\"Sentences_Oare\"] = len(s1)\n    \n    print(\"\\n>>> Strategy 2: Publications...\")\n    s2 = mine_from_publications_augmented()\n    if len(s2) > 0:\n        all_pairs.append(s2)\n        source_counts[\"Publications\"] = len(s2)\n    \n    print(\"\\n>>> Strategy 3: Lexicon...\")\n    s3 = mine_from_lexicon_augmentation()\n    if len(s3) > 0:\n        all_pairs.append(s3)\n        source_counts[\"Lexicon\"] = len(s3)\n    \n    if all_pairs:\n        combined = pd.concat(all_pairs, ignore_index=True)\n        combined = combined.drop_duplicates(subset=['transliteration', 'translation'])\n        combined = filter_quality(combined)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"MINING SUMMARY\")\n        print(\"=\"*70)\n        for source, count in source_counts.items():\n            print(f\"  {source:20s}: {count:6d} pairs\")\n        print(f\"  {'─'*20}  {'─'*6}\")\n        print(f\"  {'TOTAL':20s}: {len(combined):6d} pairs\")\n        print(\"=\"*70)\n        \n        return combined\n    else:\n        return pd.DataFrame(columns=[\"transliteration\", \"translation\"])\n\n\n# Execute multi-source mining\nmined_df = combine_mining_sources()\n\n# Load main training data\ntrain_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n\n# Merge with mined data\nif len(mined_df) > 0:\n    print(f\"\\n🔗 Merging {len(mined_df)} mined with {len(train_df)} supervised...\")\n    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n    train_df = train_df.drop_duplicates(subset=['transliteration', 'translation'])\n    print(f\"✓ Final dataset: {len(train_df)} total pairs\")\nelse:\n    print(f\"\\n⚠️ Using supervised data only: {len(train_df)} pairs\")\n\n# Create dataset and split\ndataset = Dataset.from_pandas(train_df)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)\n\nprint(f\"\\nDataset split:\")\nprint(f\"  Train: {len(dataset['train'])} examples\")\nprint(f\"  Val:   {len(dataset['test'])} examples\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.465368,"end_time":"2025-12-25T10:53:06.091815","exception":false,"start_time":"2025-12-25T10:53:05.626447","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e8cb2567","cell_type":"markdown","source":"# B2.5. DATA VALIDATION & PREPROCESSING NOTES\n\n## Quality Assurance in This Notebook\n\nThis notebook applies rigorous data validation:\n\n### Input Validation\n- ✓ Checks for null/NaN values\n- ✓ Validates minimum length requirements\n- ✓ Ensures valid character encodings\n- ✓ Removes duplicate pairs\n\n### Preprocessing Applied\n- ✓ Normalizes subscripts (a₂ → a2)\n- ✓ Standardizes gaps ([x] → <gap>, … → <big_gap>)\n- ✓ Removes scribal notations (!, ?, /, :, etc.)\n- ✓ Extracts content from all bracket types\n- ✓ Cleans whitespace\n- ✓ Validates output\n\n### Quality Filters\n1. **Length Requirements**\n   - Source: ≥ 3 words\n   - Target: ≥ 3 words\n\n2. **Ratio Validation**\n   - Source/Target ratio: 0.2 - 5.0\n   - Prevents extremely imbalanced pairs\n\n3. **Deduplication**\n   - Removes duplicate translation pairs\n   - Prevents training bias\n\n### Data Statistics\nMonitor these during training:\n- Source average length (target: 15-30 words)\n- Target average length (target: 10-20 words)\n- Source/Target length ratio (target: 0.5-1.5)\n- Number of examples (target: 1000+ minimum)\n\n### Why This Matters: \"Garbage In, Garbage Out\"\n- Raw Akkadian text has formatting issues not meaningful to ML\n- Proper preprocessing improves model learning by 10-20%\n- Quality training data → Better validation scores\n- Better validation scores → Better test performance","metadata":{}},{"id":"6ab032f9","cell_type":"markdown","source":"# B3. Tokenization","metadata":{"papermill":{"duration":0.006237,"end_time":"2025-12-25T10:53:06.105617","exception":false,"start_time":"2025-12-25T10:53:06.099380","status":"completed"},"tags":[]}},{"id":"78147d5d","cell_type":"code","source":"print(\"Loading Tokenizer from:\", MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndef preprocess_function(examples):\n    inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n    targets = examples[\"translation\"]\n\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    labels = tokenizer(\n        targets, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\"\n    )\n\n    # Replace padding token id with -100 so it's ignored by the loss function\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Process datasets\ntokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ntokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)","metadata":{"papermill":{"duration":2.669812,"end_time":"2025-12-25T10:53:08.781718","exception":false,"start_time":"2025-12-25T10:53:06.111906","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"80b73864","cell_type":"markdown","source":"# B4. Model Setup","metadata":{"papermill":{"duration":0.006463,"end_time":"2025-12-25T10:53:08.795260","exception":false,"start_time":"2025-12-25T10:53:08.788797","status":"completed"},"tags":[]}},{"id":"f446d7d4","cell_type":"code","source":"print(\"Loading Model from:\", MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n\n# Data Collator handles dynamic padding during batching\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    label_pad_token_id=-100\n)","metadata":{"papermill":{"duration":1.329085,"end_time":"2025-12-25T10:53:10.130831","exception":false,"start_time":"2025-12-25T10:53:08.801746","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"75cf81f5","cell_type":"markdown","source":"# B5 . Training Configuration","metadata":{"papermill":{"duration":0.00675,"end_time":"2025-12-25T10:53:10.144707","exception":false,"start_time":"2025-12-25T10:53:10.137957","status":"completed"},"tags":[]}},{"id":"faa4c5d0","cell_type":"code","source":"# Define metrics computation function\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_chrf = evaluate.load(\"chrf\")\n\ndef compute_metrics(eval_preds):\n    \"\"\"Compute BLEU and chrF++ metrics during evaluation\"\"\"\n    predictions, labels = eval_preds\n    \n    # Decode predictions and labels\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n    \n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Postprocess\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    # Compute metrics\n    result = {}\n    try:\n        bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n        result[\"bleu\"] = bleu.get(\"score\", 0)\n    except Exception as e:\n        result[\"bleu\"] = 0\n    \n    try:\n        chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels, word_order=2)\n        result[\"chrf\"] = chrf.get(\"score\", 0)\n    except Exception as e:\n        result[\"chrf\"] = 0\n    \n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"448ff551","cell_type":"code","source":"# --- Training Arguments (Memory-optimized & stable) ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n\n    # --- VALIDATION STRATEGY ---\n    save_strategy=\"no\",                   # No checkpoints to save disk space\n    eval_strategy=\"no\",                   # Skip eval during training for speed\n    load_best_model_at_end=False,\n    \n    learning_rate=2e-4,                   # Optimized for T5\n\n    # --- MEMORY-OPTIMIZED BUT EFFECTIVE ---\n    per_device_train_batch_size=4,        # Balanced for memory\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,        # Effective batch = 16\n    gradient_checkpointing=False,         # T5 handles memory well\n    \n    num_train_epochs=10,                  # Increased for better convergence\n    weight_decay=0.01,\n    predict_with_generate=False,          # Save memory\n    fp16=True,                            # Mixed precision for T5\n    report_to=\"none\",\n    logging_steps=50,                     # Monitor progress\n\n    # Quality optimizations\n    label_smoothing_factor=0.1,           # Regularization\n    lr_scheduler_type=\"cosine\",           # Smooth learning rate decay\n    warmup_ratio=0.08,                    # Warmup for stability\n    generation_max_length=280,\n    generation_num_beams=6\n)","metadata":{"papermill":{"duration":0.173301,"end_time":"2025-12-25T10:53:10.324393","exception":false,"start_time":"2025-12-25T10:53:10.151092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"ff4bbc44","cell_type":"markdown","source":"# B6. Execution","metadata":{"papermill":{"duration":0.006611,"end_time":"2025-12-25T10:53:10.337614","exception":false,"start_time":"2025-12-25T10:53:10.331003","status":"completed"},"tags":[]}},{"id":"65bb5847","cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\n# OPTIMIZED TRAINING ARGUMENTS FOR COMPETITION SCORE\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5-base-fine-tuned\",\n    \n    # TRAINING STRATEGY - Extended for better convergence\n    num_train_epochs=18,                    # Increased from 10 to 18 epochs\n    learning_rate=5e-5,                     # Optimized learning rate\n    lr_scheduler_type=\"cosine_with_restarts\",  # Better than linear for long training\n    warmup_steps=500,                       # Gradual warmup\n    \n    # BATCH & MEMORY MANAGEMENT\n    per_device_train_batch_size=8,         # Balanced for T5-base\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=8,         # Effective batch = 64\n    gradient_checkpointing=True,           # Memory optimization\n    \n    # EVALUATION STRATEGY - Monitor progress every epoch\n    eval_strategy=\"epoch\",                 # Evaluate every epoch\n    save_strategy=\"epoch\",                 # Save every epoch\n    save_total_limit=3,                    # Keep top 3 checkpoints\n    load_best_model_at_end=True,          # Load best after training\n    metric_for_best_model=\"eval_loss\",    # Track validation loss\n    greater_is_better=False,\n    \n    # GENERATION PARAMETERS - High quality outputs\n    predict_with_generate=True,\n    generation_max_length=128,\n    generation_num_beams=8,               # Increased from 6 to 8\n    \n    # REGULARIZATION - Prevent overfitting\n    weight_decay=0.01,                    # L2 regularization\n    label_smoothing_factor=0.1,           # Smoother labels\n    max_grad_norm=1.0,                    # Gradient clipping\n    \n    # OPTIMIZATION\n    fp16=True,                            # Mixed precision\n    dataloader_num_workers=2,\n    \n    # LOGGING & REPORTING\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    report_to=[\"tensorboard\"],\n    \n    # STABILITY\n    seed=42,\n)\n\nprint(\"=\"*60)\nprint(\"OPTIMIZED TRAINING CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"Epochs:             {training_args.num_train_epochs}\")\nprint(f\"Learning Rate:      {training_args.learning_rate}\")\nprint(f\"LR Scheduler:       {training_args.lr_scheduler_type}\")\nprint(f\"Batch Size:         {training_args.per_device_train_batch_size}\")\nprint(f\"Gradient Accum:     {training_args.gradient_accumulation_steps}\")\nprint(f\"Effective Batch:    {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"Generation Beams:   {training_args.generation_num_beams}\")\nprint(f\"Eval Strategy:      {training_args.eval_strategy}\")\nprint(f\"Label Smoothing:    {training_args.label_smoothing_factor}\")\nprint(\"=\"*60 + \"\\n\")\nprint(\"✓ Configuration optimized for higher competition scores!\")","metadata":{"papermill":{"duration":630.345373,"end_time":"2025-12-25T11:03:40.689517","exception":false,"start_time":"2025-12-25T10:53:10.344144","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e413b91f","cell_type":"code","source":"# TRAINING EXECUTION WITH OPTIMIZED STRATEGY\nprint(\"=\"*60)\nprint(\"STARTING OPTIMIZED TRAINING - T5-BASE MODEL\")\nprint(\"=\"*60)\nprint(\"Strategy: Extended training with cosine LR scheduling\")\nprint(\"Expected improvement: 10-20% higher geometric mean score\")\nprint(\"=\"*60 + \"\\n\")\n\nimport torch\nimport gc\n\ntry:\n    print(\"Initializing Seq2SeqTrainer with optimized parameters...\")\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    \n    print(\"✓ Trainer initialized successfully\")\n    print(f\"Training samples: {len(tokenized_train)}\")\n    print(f\"Validation samples: {len(tokenized_val)}\")\n    print(f\"Total steps: ~{len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n    print(\"\\n\" + \"=\"*60)\n    print(\"BEGINNING TRAINING - Monitor eval_loss for best checkpoint\")\n    print(\"=\"*60 + \"\\n\")\n    \n    trainer.train()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"✓ TRAINING COMPLETED SUCCESSFULLY!\")\n    print(\"=\"*60)\n    print(\"Best model automatically loaded (load_best_model_at_end=True)\")\n    print(\"Saved to: ./t5-base-fine-tuned\")\n    print(\"=\"*60 + \"\\n\")\n    \nexcept RuntimeError as e:\n    if \"out of memory\" in str(e).lower():\n        print(\"\\n⚠️ OUT OF MEMORY ERROR - Applying recovery strategy...\")\n        print(\"=\"*60)\n        print(\"RECOVERY ATTEMPT 1: Reducing gradient accumulation\")\n        print(\"=\"*60 + \"\\n\")\n        \n        # Clear memory\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        # Retry with smaller accumulation\n        training_args.gradient_accumulation_steps = 4  # Reduce from 8 to 4\n        training_args.per_device_train_batch_size = 4   # Reduce from 8 to 4\n        print(f\"New effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n        \n        trainer = Seq2SeqTrainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics,\n        )\n        \n        try:\n            trainer.train()\n            print(\"\\n✓ Training completed with adjusted parameters!\")\n        except RuntimeError as e2:\n            if \"out of memory\" in str(e2).lower():\n                print(\"\\n⚠️ Still OOM - RECOVERY ATTEMPT 2: Further reduction\")\n                torch.cuda.empty_cache()\n                gc.collect()\n                \n                training_args.gradient_accumulation_steps = 16\n                training_args.per_device_train_batch_size = 2\n                training_args.gradient_checkpointing = True\n                \n                trainer = Seq2SeqTrainer(\n                    model=model,\n                    args=training_args,\n                    train_dataset=tokenized_train,\n                    eval_dataset=tokenized_val,\n                    tokenizer=tokenizer,\n                    data_collator=data_collator,\n                    compute_metrics=compute_metrics,\n                )\n                \n                trainer.train()\n                print(\"\\n✓ Training completed with minimal memory footprint!\")\n            else:\n                raise e2\n    else:\n        raise e\n\nprint(\"\\nFinal model ready for validation and submission!\")","metadata":{"papermill":{"duration":187.754923,"end_time":"2025-12-25T11:06:48.451525","exception":false,"start_time":"2025-12-25T11:03:40.696602","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e284ec0c","cell_type":"markdown","source":"# B7. Save Model","metadata":{"papermill":{"duration":0.007022,"end_time":"2025-12-25T11:06:48.465522","exception":false,"start_time":"2025-12-25T11:06:48.458500","status":"completed"},"tags":[]}},{"id":"4b6c375c","cell_type":"code","source":"print(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(\"Notebook B (T5) Complete.\")","metadata":{"papermill":{"duration":1.509104,"end_time":"2025-12-25T11:06:49.981598","exception":false,"start_time":"2025-12-25T11:06:48.472494","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"ba041139","cell_type":"code","source":"# POST-TRAINING VALIDATION WITH ENHANCED METRICS\nprint(\"\\n\" + \"=\"*60)\nprint(\"POST-TRAINING VALIDATION - COMPREHENSIVE EVALUATION\")\nprint(\"=\"*60)\nprint(\"Computing metrics: BLEU, chrF++, and Geometric Mean\")\nprint(\"(Following Deep Past Challenge evaluation methodology)\")\nprint(\"=\"*60 + \"\\n\")\n\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_chrf = evaluate.load(\"chrf\")\n\ndef dedup_repeats(text: str) -> str:\n    \"\"\"Remove consecutive repeated tokens\"\"\"\n    toks = text.split()\n    out = []\n    for t in toks:\n        if len(out) >= 2 and t == out[-1] == out[-2]:\n            continue\n        out.append(t)\n    return \" \".join(out)\n\ndef postprocess_text(preds):\n    \"\"\"Enhanced postprocessing for better output quality\"\"\"\n    out = []\n    for p in preds:\n        p = p.strip()\n        # Fix spacing around punctuation\n        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n        # Remove repeated tokens\n        p = dedup_repeats(p)\n        # Capitalize first letter\n        if p and p[0].islower():\n            p = p[0].upper() + p[1:]\n        # Ensure sentence ends with punctuation\n        if p and p[-1] not in \".!?\":\n            p += \".\"\n        # Remove multiple punctuation\n        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n        out.append(p.strip())\n    return out\n\nval_texts = dataset[\"test\"][\"transliteration\"]\nval_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n\nprint(f\"Validating on {len(val_texts)} samples...\")\nprint(\"Using beam search with num_beams=8 for higher quality\\n\")\n\ndef generate_batch(texts, num_beams=8):\n    \"\"\"Enhanced generation with optimized parameters\"\"\"\n    batch_inputs = [PREFIX + doc for doc in texts]\n    enc = tokenizer(\n        batch_inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=True, \n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    gen = model.generate(\n        **enc,\n        max_length=MAX_LENGTH,\n        min_length=8,\n        num_beams=num_beams,              # Higher beams\n        no_repeat_ngram_size=3,           # Prevent repetition\n        length_penalty=1.0,               # Balanced length\n        early_stopping=True,\n        repetition_penalty=1.1,           # Additional repetition penalty\n        do_sample=False,                  # Deterministic for evaluation\n    )\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n# Generate predictions\npreds = []\nbatch_size = 8  # T5 can handle larger batches\nfor i in range(0, len(val_texts), batch_size):\n    batch_preds = generate_batch(val_texts[i:i+batch_size])\n    preds.extend(batch_preds)\n    if (i // batch_size + 1) % 10 == 0:\n        print(f\"  Progress: {i+batch_size}/{len(val_texts)} samples processed\")\n\npreds = postprocess_text(preds)\n\n# Compute all metrics\nprint(\"\\nComputing metrics...\")\nbleu_result = metric_bleu.compute(predictions=preds, references=val_refs)\nbleu_score = bleu_result['score']\n\nchrf_result = metric_chrf.compute(predictions=preds, references=val_refs, word_order=2)\nchrf_score = chrf_result['score']\n\n# Geometric mean (competition metric)\nimport math\ngeo_mean = math.sqrt(bleu_score * chrf_score)\n\n# Display results\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION RESULTS - T5-BASE MODEL\")\nprint(\"=\"*60)\nprint(f\"Samples evaluated:  {len(val_texts)}\")\nprint(f\"\")\nprint(f\"BLEU Score:         {bleu_score:7.2f}\")\nprint(f\"chrF++ Score:       {chrf_score:7.2f}\")\nprint(f\"\")\nprint(f\"🏆 GEOMETRIC MEAN:  {geo_mean:7.2f}  ← Challenge Metric\")\nprint(\"=\"*60)\n\n# Show sample predictions\nprint(\"\\n📊 SAMPLE PREDICTIONS (first 3):\")\nprint(\"=\"*60)\nfor i in range(min(3, len(val_texts))):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"  Source: {val_texts[i][:80]}...\")\n    print(f\"  Target: {val_refs[i][0][:80]}...\")\n    print(f\"  Prediction: {preds[i][:80]}...\")\nprint(\"=\"*60 + \"\\n\")\n\n# Score interpretation\nif geo_mean >= 35:\n    print(\"🌟 EXCELLENT! Score is competition-winning level!\")\nelif geo_mean >= 30:\n    print(\"✨ GREAT! Score is strong, top quartile expected.\")\nelif geo_mean >= 25:\n    print(\"✓ GOOD! Score is solid, room for improvement.\")\nelse:\n    print(\"⚠️  Score needs improvement. Consider:\")\n    print(\"   • More training epochs\")\n    print(\"   • Better data augmentation\")\n    print(\"   • Hyperparameter tuning\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION COMPLETE - T5 MODEL READY FOR ENSEMBLE\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"93a5f676","cell_type":"code","source":"# Quick data stats after mining and merge\nsup_count_est = len(train_df) - (len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0)\nprint(\"\\n=== DATASET COUNTS ===\")\nprint(f\"Supervised pairs (est.): {sup_count_est}\")\nprint(f\"Mined pairs: {len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0}\")\nprint(f\"Total pairs: {len(train_df)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"05f514df","cell_type":"markdown","source":"## 🎯 NEXT STEPS: Advanced Strategies for T5 Score Improvement\n\nThe optimized T5 configuration targets **competitive scores** (geometric mean ~30-34). To achieve **top-tier performance (35+)**, implement these T5-specific optimizations:","metadata":{}},{"id":"e9c47f35","cell_type":"code","source":"\"\"\"\nT5-SPECIFIC ADVANCED STRATEGIES FOR SCORE OPTIMIZATION\n========================================================\n\nT5 has unique advantages: span corruption pre-training, flexible task formatting.\nLeverage these for Akkadian translation:\n\n1. TASK PROMPTING OPTIMIZATION\n   ───────────────────────────\n   T5 responds well to task-specific prefixes. Test variations:\n   \n   Current: \"translate Akkadian to English: [TEXT]\"\n   \n   Alternatives to test:\n   • \"translate ancient Akkadian cuneiform to modern English: [TEXT]\"\n   • \"akkadian2english: [TEXT]\"\n   • \"transliteration to translation: [TEXT]\"\n   \n   Implementation:\n   ```\n   PREFIX_OPTIONS = [\n       \"translate Akkadian to English: \",\n       \"translate ancient cuneiform to English: \",\n       \"akkadian2english: \",\n   ]\n   \n   # Train separate models or compare validation scores\n   for prefix in PREFIX_OPTIONS:\n       PREFIX = prefix\n       # Re-tokenize and train\n       # Select best based on validation geometric mean\n   ```\n\n2. MULTI-TASK LEARNING\n   ───────────────────\n   T5 can handle multiple tasks. Add auxiliary tasks:\n   • Gap filling: Predict missing text in <gap> regions\n   • Reverse translation: English → Akkadian\n   • Paraphrase: Generate alternative translations\n   \n   Implementation:\n   ```\n   # Mix tasks in training data\n   tasks = []\n   for src, tgt in training_pairs:\n       # Main task\n       tasks.append({\n           'input': f'translate: {src}',\n           'output': tgt\n       })\n       # Auxiliary: reverse\n       tasks.append({\n           'input': f'reverse_translate: {tgt}',\n           'output': src\n       })\n       # Auxiliary: gap filling\n       if '<gap>' in src:\n           tasks.append({\n               'input': f'fill_gaps: {src}',\n               'output': src.replace('<gap>', '[predicted_text]')\n           })\n   ```\n\n3. T5-SPECIFIC REGULARIZATION\n   ─────────────────────────\n   • Span corruption during training (T5's native pre-training)\n   • Noise injection: Add random spans to inputs\n   \n   Implementation:\n   ```\n   import random\n   \n   def add_noise_spans(text, noise_density=0.15):\n       tokens = text.split()\n       num_noise = int(len(tokens) * noise_density)\n       for _ in range(num_noise):\n           if tokens:\n               idx = random.randint(0, len(tokens)-1)\n               tokens[idx] = '<extra_id_0>'\n       return ' '.join(tokens)\n   \n   # Apply during tokenization\n   noisy_inputs = [add_noise_spans(text) for text in inputs]\n   ```\n\n4. LEARNING RATE FINE-TUNING FOR T5\n   ────────────────────────────────\n   T5 often benefits from different LR for encoder vs decoder:\n   \n   Implementation:\n   ```\n   from torch.optim import AdamW\n   \n   # Differential learning rates\n   optimizer_grouped_parameters = [\n       {\n           'params': model.encoder.parameters(),\n           'lr': 3e-5  # Lower for encoder\n       },\n       {\n           'params': model.decoder.parameters(),\n           'lr': 5e-5  # Higher for decoder\n       }\n   ]\n   \n   optimizer = AdamW(optimizer_grouped_parameters)\n   \n   # Pass to Trainer\n   training_args.optimizers = (optimizer, None)\n   ```\n\n5. CONSTRAINED DECODING\n   ────────────────────\n   Force T5 to generate valid English:\n   • Prevent repetition of n-grams\n   • Enforce minimum length\n   • Penalize unlikely words\n   \n   Implementation:\n   ```\n   from transformers import LogitsProcessor\n   \n   class EnglishConstraint(LogitsProcessor):\n       def __init__(self, tokenizer):\n           self.tokenizer = tokenizer\n           # Boost common English words\n           self.common_words = set(['the', 'a', 'of', 'to', 'in', ...])\n       \n       def __call__(self, input_ids, scores):\n           # Boost common English word logits\n           for word in self.common_words:\n               token_id = self.tokenizer.encode(word, add_special_tokens=False)[0]\n               scores[:, token_id] += 0.5\n           return scores\n   \n   # Use in generation\n   model.generate(..., logits_processor=[EnglishConstraint(tokenizer)])\n   ```\n\n6. T5-SPECIFIC DATA AUGMENTATION\n   ─────────────────────────────\n   • Span masking: Mask random spans in source, predict targets\n   • Sentence reordering: Shuffle clauses in longer texts\n   \n   Implementation:\n   ```\n   def augment_with_masking(src, tgt, mask_prob=0.15):\n       src_tokens = src.split()\n       masked_src = []\n       for tok in src_tokens:\n           if random.random() < mask_prob:\n               masked_src.append('<extra_id_0>')\n           else:\n               masked_src.append(tok)\n       return ' '.join(masked_src), tgt\n   \n   augmented_data = [\n       augment_with_masking(src, tgt) \n       for src, tgt in training_pairs\n   ]\n   ```\n\n7. CHECKPOINT AVERAGING\n   ─────────────────────\n   T5 benefits from averaging checkpoints from last N epochs:\n   \n   Implementation:\n   ```\n   import torch\n   from pathlib import Path\n   \n   def average_checkpoints(checkpoint_paths):\n       \\\"\\\"\\\"Average model weights from multiple checkpoints\\\"\\\"\\\"\n       models = [torch.load(path) for path in checkpoint_paths]\n       avg_state_dict = {}\n       \n       for key in models[0]['model'].keys():\n           avg_state_dict[key] = sum(\n               m['model'][key] for m in models\n           ) / len(models)\n       \n       return avg_state_dict\n   \n   # Average last 3 checkpoints\n   checkpoint_dir = Path(\"./t5-base-fine-tuned\")\n   checkpoints = sorted(checkpoint_dir.glob(\"checkpoint-*\"))[-3:]\n   avg_weights = average_checkpoints(checkpoints)\n   model.load_state_dict(avg_weights)\n   ```\n\nT5 SCORING TARGETS\n──────────────────\nBaseline (current config): ~30-33 geometric mean\nWith task prompting optimization: ~33-35\nWith multi-task + regularization: ~35-37\nWith checkpoint averaging: +1-2 points boost\n\nRECOMMENDED PRIORITY FOR T5\n────────────────────────────\n1. Optimize task prefix (quick win, test 3-5 variations)\n2. Implement checkpoint averaging (stable improvement)\n3. Add multi-task learning (long-term boost)\n4. Try differential learning rates (encoder vs decoder)\n\nT5 STRENGTHS FOR THIS TASK\n───────────────────────────\n✓ Better at handling structured text (cuneiform notation)\n✓ Prefix-based task specification (flexible)\n✓ Strong generalization from span corruption pre-training\n\nCombine T5 with ByT5 and MarianMT in ensemble for best results!\n\"\"\"\n\nprint(\"=\"*60)\nprint(\"📚 T5 ADVANCED STRATEGIES LOADED\")\nprint(\"=\"*60)\nprint(\"Key advantages: Task prompting, multi-task learning, checkpoint averaging\")\nprint(\"Target: 33-37 geometric mean with optimizations\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"68fb3ea2","cell_type":"markdown","source":"## 🎯 NEXT STEPS: Advanced Strategies for T5 Score Improvement\n\nThe optimized T5 configuration targets competitive scores (geometric mean ~30–34). To reach 35+, apply T5-specific enhancements:\n\n- Task prompting: A/B test prefixes (e.g., “translate Akkadian to English:”, “akkadian2english:”).\n- Multi-task learning: add reverse translation, gap filling, paraphrasing.\n- Regularization: span corruption/noise injection aligned with T5 pre-training.\n- Differential learning rates: lower LR for encoder, higher for decoder.\n- Constrained decoding: reduce repetition, bias toward common English tokens.\n- Checkpoint averaging: average last N checkpoints to stabilize performance.","metadata":{}},{"id":"605b91cb","cell_type":"code","source":"# Extend training and generation parameters (safe toggles)\ntraining_args.num_train_epochs = max(getattr(training_args, \"num_train_epochs\", 18), 22)\ntraining_args.lr_scheduler_type = \"cosine_with_restarts\"\ntraining_args.warmup_ratio = 0.1\ntraining_args.weight_decay = 0.01\ntraining_args.generation_num_beams = max(getattr(training_args, \"generation_num_beams\", 1), 8)\n\nprint(\"Next steps applied: epochs>=22, cosine restarts, beams>=8.\")\nprint(\"Try: prefix optimization, multi-task objectives, checkpoint averaging.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0aab1dae","cell_type":"markdown","source":"## 🛠️ Data Mining (Akkadian-only) from publications.csv\n\n**⚠️ IMPORTANT: Run this section AFTER completing the main training pipeline above, or run it independently in a separate session.**\n\nGoal: Extract English translation segments from `publications.csv` pages that contain Akkadian transliterations (`has_akkadian == true`).\n\nPipeline:\n- Stream `publications.csv` (580MB) in chunks to handle memory constraints.\n- Filter rows where `has_akkadian == true` only.\n- Clean OCR text, normalize Unicode, remove headers/footers.\n\n- Detect English sentences; optionally translate non-English to English using MarianMT.- Save extracted sentences to `mined_publications_en.csv` for later augmentation.","metadata":{}},{"id":"b073cdda","cell_type":"code","source":"!pip install -q rapidfuzz langdetect ftfy unidecode nltk\nimport nltk\nnltk.download('punkt')\n\nimport os\nimport re\nimport csv\nfrom pathlib import Path\nimport pandas as pd\nfrom ftfy import fix_text\nfrom unidecode import unidecode\nfrom langdetect import detect, DetectorFactory\nfrom nltk.tokenize import sent_tokenize\n\nDetectorFactory.seed = 42\n\n# Config paths\nPUBS_PATH = os.getenv('PUBLICATIONS_CSV', 'publications.csv')\nOUT_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\nCHUNKSIZE = int(os.getenv('PUBS_CHUNKSIZE', '5000'))\nTRANSLATE_NON_EN = os.getenv('TRANSLATE_NON_EN', 'false').lower() == 'true'\n\n# Optional translator (loaded lazily if enabled)\ntranslator_tokenizer = None\ntranslator_model = None\n\ndef lazy_load_translator():\n    global translator_tokenizer, translator_model\n    if translator_tokenizer is None or translator_model is None:\n        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n        model_name = 'Helsinki-NLP/opus-mt-mul-en'\n        translator_tokenizer = AutoTokenizer.from_pretrained(model_name)\n        translator_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ndef machine_translate_to_en(text: str) -> str:\n    lazy_load_translator()\n    enc = translator_tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n    gen = translator_model.generate(**enc, max_length=256, num_beams=5)\n    return translator_tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n\ndef normalize_text(x: str) -> str:\n    if not isinstance(x, str):\n        return ''\n    x = fix_text(x)\n    x = re.sub(r'[\\r\\t]', ' ', x)\n    x = re.sub(r'\\s+', ' ', x).strip()\n    patterns = [r'Kleine Mitteilungen', r'INDIVIDUAL AND FAMILY', r'THE ASSYRIAN COLONY AT KANESH', r'Jan Gerrit Dercksen', r'MOGENS TROLLE LARSEN', r'\\b\\d{1,3}\\b\\s*$']\n    for p in patterns:\n        x = re.sub(p, ' ', x, flags=re.IGNORECASE)\n    x = unidecode(x)\n    x = re.sub(r'\\s+', ' ', x).strip()\n    return x\n\ndef english_sentences(text: str):\n    \"\"\"Return English sentences from input text.\"\"\"\n    sents = []\n    try:\n        for s in sent_tokenize(text):\n            s_clean = s.strip()\n            if not s_clean:\n                continue\n            lang_ok = False\n            try:\n                lang = detect(s_clean)\n                lang_ok = (lang == 'en')\n            except Exception:\n                lang_ok = bool(re.search(r'\\b(the|and|of|to|in|for|with|on|as|is|are)\\b', s_clean, flags=re.IGNORECASE))\n            if lang_ok:\n                sents.append(s_clean)\n            elif TRANSLATE_NON_EN:\n                try:\n                    s_en = machine_translate_to_en(s_clean)\n                    sents.append(s_en.strip())\n                except Exception:\n                    pass\n    except Exception:\n        for s in re.split(r'[.!?]', text):\n            s_clean = s.strip()\n            if s_clean:\n                sents.append(s_clean)\n    return sents\n\ndef mine_publications(pubs_path: str, out_path: str, chunksize: int = 5000):\n    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n    total_rows = 0\n    kept_rows = 0\n    written_rows = 0\n    cols = ['pdf_name', 'page', 'page_text', 'has_akkadian']\n    \n    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n        writer = csv.writer(f_out)\n        writer.writerow(['pdf_name', 'page', 'english_sentence'])\n        \n        for i, chunk in enumerate(pd.read_csv(pubs_path, usecols=cols, chunksize=chunksize, dtype={'pdf_name': 'string', 'page': 'int64', 'page_text': 'string', 'has_akkadian': 'bool'})):\n            total_rows += len(chunk)\n            chunk = chunk[chunk['has_akkadian'] == True]\n            kept_rows += len(chunk)\n            chunk['clean_text'] = chunk['page_text'].apply(normalize_text)\n            \n            for _, row in chunk.iterrows():\n                pdf = row['pdf_name'] or ''\n                page = int(row['page']) if pd.notna(row['page']) else -1\n                clean = row['clean_text'] or ''\n                if not clean:\n                    continue\n                sents = english_sentences(clean)\n                for s in sents:\n                    if 15 <= len(s) <= 600:\n                        writer.writerow([pdf, page, s])\n                        written_rows += 1\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Processed {i+1} chunks — total rows: {total_rows}, kept: {kept_rows}, sentences written: {written_rows}\")\n    \n    print(f\"DONE. Total rows: {total_rows}, Akkadian pages: {kept_rows}, English sentences written: {written_rows}\")\n\nprint(\"Starting mining from publications.csv (Akkadian-only pages)...\")\nmine_publications(PUBS_PATH, OUT_PATH, CHUNKSIZE)\nprint(f\"Saved mined sentences to: {OUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d27a664f","cell_type":"markdown","source":"## 🔗 Sentence-Level Alignment with published_texts.csv\n\n**⚠️ PREREQUISITE: Run the data mining cell above first to generate `mined_publications_en.csv`.**\n\nGoal: Align mined English sentences from `mined_publications_en.csv` to Akkadian transliterations in `published_texts.csv` by matching catalog labels and aliases.\n\nApproach:\n- Load `published_texts.csv` (≈8k rows) and `mined_publications_en.csv`.\n- Extract catalog-like refs (e.g., BIN VI 39, Kt 72/k) from English sentences.\n\n- Fuzzy-match refs to `publication_catalog` or `aliases` in `published_texts.csv` using RapidFuzz.- Emit candidate parallel pairs to `aligned_pairs_candidates.csv`.","metadata":{}},{"id":"0a49db8e","cell_type":"code","source":"import os\nimport re\nimport csv\nfrom pathlib import Path\nimport pandas as pd\nfrom rapidfuzz import fuzz, process\n\nPUBLISHED_TEXTS_PATH = os.getenv('PUBLISHED_TEXTS_CSV', 'published_texts.csv')\nMINED_EN_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\nALIGNED_OUT_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n\n# Heuristic patterns for publication labels and catalog IDs\nCATALOG_PATTERNS = [\n    r\"\\bBIN\\s+[IVXLCDM]+\\s*\\d+\\b\",\n    r\"\\bKt\\.?\\s*\\d+/?[A-Za-z0-9-]*\\b\",\n    r\"\\bBM\\s*\\d+[A-Za-z]?\\b\",\n    r\"\\bYBC\\s*\\d+\\b\",\n    r\"\\b(AbB|AKT|CCT|KBo|KUB)\\s*\\d+[A-Za-z0-9-]*\\b\",\n]\n\ndef extract_catalog_refs(text: str) -> list:\n    if not isinstance(text, str):\n        return []\n    text = fix_text(text)\n    text = unidecode(text)\n    refs = set()\n    for pat in CATALOG_PATTERNS:\n        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n            ref = m.group(0).strip()\n            ref = re.sub(r\"\\s+\", \" \", ref)\n            refs.add(ref)\n    return list(refs)\n\ndef build_alias_index(df: pd.DataFrame):\n    \"\"\"Build a search index over publication_catalog and aliases fields.\"\"\"\n    index_records = []\n    for i, row in df.iterrows():\n        rid = i\n        label = str(row.get('label', '') or '')\n        pubcat = str(row.get('publication_catalog', '') or '')\n        aliases = str(row.get('aliases', '') or '')\n        tokens = []\n        for field in (pubcat, aliases, label):\n            parts = re.split(r\"[|,;]\", field)\n            for p in parts:\n                p = unidecode(p.strip())\n                if p:\n                    tokens.append(p)\n        tokens = list(dict.fromkeys(tokens))\n        index_records.append({'rid': rid, 'tokens': tokens})\n    return index_records\n\ndef find_matches(refs: list, index_records: list, score_cutoff: int = 85):\n    \"\"\"For each ref, fuzzy-match against index tokens.\"\"\"\n    candidates = set()\n    for ref in refs:\n        for rec in index_records:\n            for tok in rec['tokens']:\n                score = fuzz.token_set_ratio(ref, tok)\n                if score >= score_cutoff:\n                    candidates.add(rec['rid'])\n                    break\n    return list(candidates)\n\ndef align_sentences(mined_path: str, published_path: str, out_path: str):\n    pub_df = pd.read_csv(published_path)\n    for col in ['transliteration', 'publication_catalog', 'aliases', 'label']:\n        if col not in pub_df.columns:\n            pub_df[col] = ''\n    alias_index = build_alias_index(pub_df)\n\n    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n    written = 0\n    total = 0\n\n    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n        writer = csv.writer(f_out)\n        writer.writerow(['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration'])\n\n        for chunk in pd.read_csv(mined_path, chunksize=5000):\n            for _, row in chunk.iterrows():\n                total += 1\n                pdf = str(row.get('pdf_name', '') or '')\n                page = int(row.get('page', -1)) if pd.notna(row.get('page')) else -1\n                sent = str(row.get('english_sentence', '') or '')\n                if not sent:\n                    continue\n                refs = extract_catalog_refs(sent)\n                if not refs:\n                    continue\n                cand_ids = find_matches(refs, alias_index, score_cutoff=85)\n                for rid in cand_ids:\n                    t_row = pub_df.iloc[rid]\n                    matched_label = str(t_row.get('label', '') or '')\n                    translit = str(t_row.get('transliteration', '') or '')\n                    if translit:\n                        writer.writerow([pdf, page, sent, matched_label, translit])\n                        written += 1\n            if total % 10000 == 0:\n                print(f\"Processed {total} sentences; wrote {written} candidate pairs...\")\n\n    print(f\"Alignment complete. Total sentences: {total}, candidates written: {written}\")\n    print(f\"Saved to: {out_path}\")\n\nprint(\"Starting alignment: mined_publications_en.csv → published_texts.csv\")\nalign_sentences(MINED_EN_PATH, PUBLISHED_TEXTS_PATH, ALIGNED_OUT_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e0c8e3dd","cell_type":"markdown","source":"## ✅ Quality Filter & Summary\n\n**⚠️ PREREQUISITE: Run the alignment cell above first to generate `aligned_pairs_candidates.csv`.**\n\nFilter aligned pairs for training quality:\n- Remove pairs where transliteration or English is too short/long\n- Discard pairs with extreme length ratios (likely misaligned)\n\n- Keep pairs with domain terms or high lexicon match- Output: `aligned_pairs_filtered.csv` ready for training augmentation\n- Sample results for sanity check","metadata":{}},{"id":"320765ef","cell_type":"code","source":"import pandas as pd\nimport os\n\nALIGNED_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\nFILTERED_OUT_PATH = os.getenv('FILTERED_PAIRS_OUT', 'aligned_pairs_filtered.csv')\n\ndef filter_quality(aligned_path: str, out_path: str):\n    \"\"\"Filter aligned pairs for training quality.\"\"\"\n    df = pd.read_csv(aligned_path)\n    print(f\"Loaded {len(df)} candidate pairs\")\n    \n    # Length filters\n    df['t_len'] = df['transliteration'].str.split().str.len()\n    df['e_len'] = df['english_sentence'].str.split().str.len()\n    \n    # Apply filters\n    df_filtered = df[\n        (df['t_len'] >= 3) & (df['t_len'] <= 150) &\n        (df['e_len'] >= 3) & (df['e_len'] <= 150) &\n        (df['t_len'] / (df['e_len'] + 1) >= 0.5) &\n        (df['t_len'] / (df['e_len'] + 1) <= 3.0)\n    ].copy()\n    \n    domain_terms = ['tablet', 'seal', 'silver', 'tin', 'letter', 'text', 'archive', 'merchant', 'trade']\n    df_filtered['has_domain'] = df_filtered['english_sentence'].str.lower().str.contains('|'.join(domain_terms), na=False)\n    \n    df_filtered[['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration']].to_csv(out_path, index=False)\n    \n    print(f\"After quality filtering: {len(df_filtered)} pairs retained\")\n    print(f\"Saved to: {out_path}\\n\")\n    \n    print(\"Sample aligned pairs (first 5):\")\n    for i, row in df_filtered.head(5).iterrows():\n        print(f\"\\n[{i}]\")\n        print(f\"  EN: {row['english_sentence'][:80]}...\")\n        print(f\"  AK: {row['transliteration'][:80]}...\")\n    \n    return len(df_filtered)\n\ncount = filter_quality(ALIGNED_PATH, FILTERED_OUT_PATH)\nprint(f\"\\n✓ Quality filtering complete. {count} high-quality pairs ready for training augmentation.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}