{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-21T12:18:33.672428Z",
     "iopub.status.busy": "2025-12-21T12:18:33.672157Z",
     "iopub.status.idle": "2025-12-21T12:18:33.686263Z",
     "shell.execute_reply": "2025-12-21T12:18:33.685656Z",
     "shell.execute_reply.started": "2025-12-21T12:18:33.672384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:47: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_55/2929549046.py:47: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# ==================================================================================\\n#   DEEP PAST CHALLENGE - FINAL ROBUST ENSEMBLE\\n#   ------------------------------------------------------------------------------\\n#   Logic: ByT5 (Best Morphology) + T5 (Best Grammar) + Marian (Fluency)\\n#   Metric: GeoMean(BLEU, chrF++) optimized via weighted voting & keyword validation.\\n# ==================================================================================\\n\\nimport os\\nimport gc\\nimport sys\\nimport re\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nfrom tqdm.auto import tqdm\\nimport difflib\\n\\n# ==================================================================================\\n# 1. CONFIGURATION (!!! UPDATE THESE PATHS !!!)\\n# ==================================================================================\\n# Check the \"Data\" tab in Kaggle to find the exact paths to your saved models.\\n# They will look something like \"/kaggle/input/your-notebook-name/byt5-base-saved\"\\n\\nMODEL_PATHS = {\\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",       # Update this\\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\",       # Update this\\n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"   # Update this\\n}\\n\\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nBATCH_SIZE = 16\\n\\n# ==================================================================================\\n# 2. ADVANCED CLEANING & UTILS\\n# ==================================================================================\\n\\ndef clean_prediction(text):\\n    \"\"\"Post-processing to fix common NMT artifacts.\"\"\"\\n    if not isinstance(text, str): return \"\"\\n    text = text.strip()\\n    \\n    # 1. Fix punctuation spacing (e.g., \"city .\" -> \"city.\")\\n    text = re.sub(r\\'\\\\s+([.,!?;:])\\', r\\'\\x01\\', text)\\n    \\n    # 2. Capitalize first letter\\n    if text:\\n        text = text[0].upper() + text[1:]\\n        \\n    # 3. Ensure sentence ending punctuation (if missing)\\n    if text and text[-1] not in \".!?\":\\n        text += \".\"\\n        \\n    return text\\n\\ndef is_garbage(text, source_text=\"\"):\\n    \"\"\"Returns True if the prediction is likely a hallucination or failure.\"\"\"\\n    if len(text) < 3: return True\\n    \\n    # Check for repetition loops (e.g., \"the silver the silver the silver\")\\n    if len(text) > 20 and len(set(text.split())) < 3:\\n        return True\\n        \\n    # Check if model just copied the input (common failure mode)\\n    ratio = difflib.SequenceMatcher(None, text.lower(), source_text.lower()).ratio()\\n    if ratio > 0.8: # If translation is 80% identical to source transliteration\\n        return True\\n        \\n    return False\\n\\n# ==================================================================================\\n# 3. EFFICIENT INFERENCE ENGINE\\n# ==================================================================================\\n\\nclass TextDataset(Dataset):\\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\\n        self.texts = [prefix + str(t) for t in texts]\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n\\n    def __len__(self):\\n        return len(self.texts)\\n\\n    def __getitem__(self, idx):\\n        enc = self.tokenizer(\\n            self.texts[idx], \\n            truncation=True, \\n            padding=\"max_length\", \\n            max_length=self.max_len, \\n            return_tensors=\"pt\"\\n        )\\n        return {\\n            \"input_ids\": enc[\"input_ids\"].squeeze(0), \\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\\n        }\\n\\ndef generate_predictions(model_name, model_path, inputs):\\n    \"\"\"Loads model, predicts, then UNLOADS model to save RAM.\"\"\"\\n    print(f\"\\n[INFO] Processing with {model_name.upper()}...\")\\n    \\n    if not os.path.exists(model_path):\\n        print(f\"[WARNING] Path not found: {model_path}. Skipping.\")\\n        return [\"\"] * len(inputs)\\n\\n    # 1. Config based on model type\\n    if \"byt5\" in model_name:\\n        max_len = 512\\n        prefix = \"translate Akkadian to English: \"\\n    elif \"t5\" in model_name:\\n        max_len = 256\\n        prefix = \"translate Akkadian to English: \"\\n    else: # Marian\\n        max_len = 128\\n        prefix = \"\"\\n\\n    # 2. Load\\n    try:\\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\\n    except Exception as e:\\n        print(f\"[ERROR] Failed to load {model_name}: {e}\")\\n        return [\"\"] * len(inputs)\\n\\n    # 3. Predict\\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\\n    \\n    preds = []\\n    with torch.no_grad():\\n        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\\n            input_ids = batch[\"input_ids\"].to(DEVICE)\\n            mask = batch[\"attention_mask\"].to(DEVICE)\\n            \\n            # Beam search with penalties for repetition (Crucial for score)\\n            gen_ids = model.generate(\\n                input_ids=input_ids,\\n                attention_mask=mask,\\n                max_length=max_len,\\n                num_beams=5,               # Higher beam = better quality\\n                no_repeat_ngram_size=3,    # Prevent \"and the and the\"\\n                repetition_penalty=1.2,    # Penalty for loops\\n                early_stopping=True\\n            )\\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\\n            preds.extend([clean_prediction(d) for d in decoded])\\n\\n    # 4. Cleanup (CRITICAL for 16GB GPU)\\n    del model, tokenizer, dataset, loader\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    return preds\\n\\n# ==================================================================================\\n# 4. DOMAIN-AWARE ENSEMBLE STRATEGY\\n# ==================================================================================\\n\\n# Dictionary of high-value Akkadian keywords\\nKEYWORDS = {\\n    \"qi-bi-ma\": [\"speak\", \"tell\", \"say\"],\\n    \"um-ma\": [\"thus\"],\\n    \"a-na\": [\"to\", \"for\"],\\n    \"ku-babbar\": [\"silver\", \"money\"],\\n    \"ku-gi\": [\"gold\"],\\n    \"a-lim\": [\"city\"],\\n    \"e-gal\": [\"palace\"],\\n    \"dam-qar\": [\"merchant\", \"agent\"],\\n    \"tup-pi\": [\"tablet\", \"letter\", \"document\", \"record\"],\\n    \"be-li\": [\"lord\", \"master\", \"boss\"],\\n    \"a-hi\": [\"brother\", \"partner\"],\\n    \"i-na\": [\"in\", \"from\", \"on\"],\\n    \"su-be2-el\": [\"send\"],\\n    \"u2-bi-il\": [\"brought\", \"carried\"],\\n    \"li-bi-shi\": [\"should be\", \"let it be\"],\\n    \"mi3-ma\": [\"anything\", \"something\", \"property\"]\\n}\\n\\ndef ensemble_vote(inputs, predictions_dict):\\n    \"\"\"\\n    Selects the best translation based on:\\n    1. Model Trust (ByT5 > T5 > Marian)\\n    2. Keyword Coverage (Did it translate \\'silver\\' correctly?)\\n    3. Garbage Detection\\n    \"\"\"\\n    final_translations = []\\n    \\n    # Trust weights based on your training logs (ByT5 was superior)\\n    MODEL_WEIGHTS = {\"byt5\": 3.0, \"t5\": 2.0, \"marian\": 1.0}\\n    \\n    print(\"\\n[INFO] Running Weighted Ensemble Voting...\")\\n    \\n    for i in tqdm(range(len(inputs))):\\n        src = inputs[i].lower()\\n        candidates = {\\n            k: predictions_dict[k][i] \\n            for k in predictions_dict \\n            if predictions_dict[k][i] # Only consider if prediction exists\\n        }\\n        \\n        if not candidates:\\n            final_translations.append(\"Broken text.\")\\n            continue\\n\\n        best_score = -1\\n        best_text = \"\"\\n        \\n        for model_name, text in candidates.items():\\n            # A. Base Score (Model Confidence)\\n            score = MODEL_WEIGHTS.get(model_name, 1.0)\\n            \\n            # B. Filter Garbage\\n            if is_garbage(text, src):\\n                score -= 10 # Heavily penalize garbage\\n                \\n            # C. Keyword Bonus (The Secret Sauce)\\n            text_lower = text.lower()\\n            for akk, eng_list in KEYWORDS.items():\\n                if akk in src:\\n                    # If the source has \\'silver\\', and translation has \\'silver\\', BOOST IT\\n                    if any(eng in text_lower for eng in eng_list):\\n                        score += 2.0\\n            \\n            # D. Length Penalty (Too short is usually bad, unless input is short)\\n            if len(text) < 10 and len(src) > 20:\\n                score -= 1.0\\n                \\n            if score > best_score:\\n                best_score = score\\n                best_text = text\\n        \\n        # Fallback if all scores are negative (all garbage)\\n        if best_score < 0:\\n            # Prefer T5 or ByT5 output even if garbage, better than empty\\n            best_text = candidates.get(\"byt5\", candidates.get(\"t5\", \"Broken text.\"))\\n\\n        final_translations.append(best_text)\\n        \\n    return final_translations\\n\\n# ==================================================================================\\n# 5. EXECUTION PIPELINE\\n# ==================================================================================\\n\\ndef main():\\n    print(\"=== STARTING INFERENCE PIPELINE ===\")\\n    \\n    # 1. Load Test Data\\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\\n    ids = test_df[\"id\"].tolist()\\n    \\n    print(f\"Loaded {len(inputs)} test sentences.\")\\n    \\n    # 2. Run Inference (Sequential to save memory)\\n    all_preds = {}\\n    \\n    # Run ByT5 (The Specialist)\\n    all_preds[\"byt5\"] = generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], inputs)\\n    \\n    # Run T5 (The Generalist)\\n    all_preds[\"t5\"] = generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], inputs)\\n    \\n    # Run Marian (The Fluency Expert)\\n    all_preds[\"marian\"] = generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], inputs)\\n    \\n    # 3. Ensemble\\n    final_preds = ensemble_vote(inputs, all_preds)\\n    \\n    # 4. Save Submission\\n    sub = pd.DataFrame({\"id\": ids, \"translation\": final_preds})\\n    \\n    # Final check for empty strings\\n    sub[\"translation\"] = sub[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Broken text.\")\\n    \\n    sub.to_csv(\"submission.csv\", index=False)\\n    print(\"\\n[SUCCESS] submission.csv generated successfully.\")\\n    print(sub.head())\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEEP PAST CHALLENGE - SINGLE HYBRID PIPELINE (Retrieval + Neural Ensemble)\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL_PATHS = {\n",
    "    \"byt5\": os.getenv(\"BYT5_PATH\", \"/kaggle/input/notebook-a-byt5/byt5-base-saved\"),\n",
    "    \"t5\": os.getenv(\"T5_PATH\", \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\"),\n",
    "    \"marian\": os.getenv(\"MARIAN_PATH\", \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"),\n",
    "}\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "GLOBAL_MATCH_THRESHOLD = 0.80\n",
    "LOCAL_MATCH_THRESHOLD = 0.75\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLEANING HELPERS (match training notebooks)\n",
    "# -----------------------------------------------------------------------------\n",
    "def clean_translit(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" <big_gap> \", text)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"[:]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def postprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n",
    "    if text and text[0].islower():\n",
    "        text = text[0].upper() + text[1:]\n",
    "    if text and text[-1] not in \".!?\":\n",
    "        text += \".\"\n",
    "    return text\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RETRIEVAL ENGINE\n",
    "# -----------------------------------------------------------------------------\n",
    "class RetrievalEngine:\n",
    "    def __init__(self, train_df):\n",
    "        self.df = train_df.copy()\n",
    "        self.df[\"transliteration_clean\"] = self.df[\"transliteration\"].fillna(\"\").map(clean_translit)\n",
    "        self.vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2, 6), min_df=1, sublinear_tf=True)\n",
    "        self.train_vectors = self.vectorizer.fit_transform(self.df[\"transliteration_clean\"])\n",
    "        print(f\"[INFO] Indexed {self.train_vectors.shape[0]} training rows for retrieval.\")\n",
    "\n",
    "    def find_global_parent(self, test_clean_texts):\n",
    "        full_test = \" \".join(test_clean_texts)\n",
    "        test_vec = self.vectorizer.transform([full_test])\n",
    "        sims = cosine_similarity(test_vec, self.train_vectors).flatten()\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        return self.df.iloc[best_idx][\"translation\"], float(sims[best_idx])\n",
    "\n",
    "    def find_local_matches(self, test_clean_texts):\n",
    "        test_vecs = self.vectorizer.transform(test_clean_texts)\n",
    "        matches = []\n",
    "        sims = cosine_similarity(test_vecs, self.train_vectors)\n",
    "        for i in range(test_vecs.shape[0]):\n",
    "            best_idx = int(np.argmax(sims[i]))\n",
    "            matches.append((self.df.iloc[best_idx][\"translation\"], float(sims[i][best_idx])))\n",
    "        return matches\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DATASET + GENERATION\n",
    "# -----------------------------------------------------------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\n",
    "        self.texts = [prefix + str(t) for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "def generate_predictions(model_name, model_path, inputs):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"[WARNING] Missing model for {model_name}: {model_path}\")\n",
    "        return [\"\"] * len(inputs)\n",
    "\n",
    "    if \"byt5\" in model_name:\n",
    "        max_len = 420; prefix = \"translate Akkadian to English: \"; beams = 6\n",
    "    elif \"t5\" in model_name:\n",
    "        max_len = 280; prefix = \"translate Akkadian to English: \"; beams = 6\n",
    "    else:\n",
    "        max_len = 180; prefix = \"\"; beams = 5\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {model_name}: {e}\")\n",
    "        return [\"\"] * len(inputs)\n",
    "\n",
    "    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                max_length=max_len,\n",
    "                num_beams=beams,\n",
    "                no_repeat_ngram_size=3,\n",
    "                repetition_penalty=1.15,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            preds.extend([postprocess_text(d) for d in decoded])\n",
    "\n",
    "    del model, tokenizer, dataset, loader\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return preds\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENSEMBLE\n",
    "# -----------------------------------------------------------------------------\n",
    "def neural_vote(preds_dict):\n",
    "    weights = {\"byt5\": 5.0, \"t5\": 1.5, \"marian\": 1.0}\n",
    "    final = []\n",
    "    n = max(len(v) for v in preds_dict.values()) if preds_dict else 0\n",
    "    for i in range(n):\n",
    "        best_text, best_score = \"\", -1e9\n",
    "        for name, preds in preds_dict.items():\n",
    "            if i >= len(preds):\n",
    "                continue\n",
    "            score = weights.get(name, 1.0)\n",
    "            if len(preds[i]) < 5:\n",
    "                score -= 5.0\n",
    "            if score > best_score:\n",
    "                best_score, best_text = score, preds[i]\n",
    "        final.append(best_text)\n",
    "    return final\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SLICING FOR GLOBAL PARENT MATCH\n",
    "# -----------------------------------------------------------------------------\n",
    "def extract_translation_segment(translation, line_start, line_end, total_lines):\n",
    "    if not isinstance(translation, str) or total_lines <= 0:\n",
    "        return translation if isinstance(translation, str) else \"\"\n",
    "    start_ratio = max(0, (line_start - 1) / total_lines)\n",
    "    end_ratio = min(1, line_end / total_lines)\n",
    "    start_char = int(len(translation) * start_ratio)\n",
    "    end_char = int(len(translation) * end_ratio)\n",
    "\n",
    "    if start_char > 0:\n",
    "        last_period = translation.rfind('.', max(0, start_char - 150), start_char)\n",
    "        if last_period > 0:\n",
    "            start_char = last_period + 2\n",
    "\n",
    "    if end_char < len(translation):\n",
    "        next_period = translation.find('.', end_char, min(len(translation), end_char + 150))\n",
    "        if next_period > 0:\n",
    "            end_char = next_period + 1\n",
    "\n",
    "    if start_char >= end_char:\n",
    "        start_char = int(len(translation) * start_ratio)\n",
    "        end_char = int(len(translation) * end_ratio)\n",
    "\n",
    "    return translation[start_char:end_char].strip()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    print(\"=== Deep Past Hybrid Inference ===\")\n",
    "    train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "    test_inputs_raw = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\n",
    "    test_inputs = [clean_translit(t) for t in test_inputs_raw]\n",
    "\n",
    "    retriever = RetrievalEngine(train_df)\n",
    "\n",
    "    # Global parent search\n",
    "    parent_translation, global_score = retriever.find_global_parent(test_inputs)\n",
    "    print(f\"[INFO] Global similarity: {global_score:.2%}\")\n",
    "\n",
    "    if global_score >= GLOBAL_MATCH_THRESHOLD:\n",
    "        print(\"[INFO] Strong global match -> using proportional slicing.\")\n",
    "        total_lines = test_df[\"line_end\"].max() if \"line_end\" in test_df.columns else len(test_df) * 10\n",
    "        final_outputs = []\n",
    "        for _, row in test_df.iterrows():\n",
    "            l_start = int(row.get(\"line_start\", 1)) if not pd.isna(row.get(\"line_start\", 1)) else 1\n",
    "            l_end = int(row.get(\"line_end\", l_start)) if not pd.isna(row.get(\"line_end\", l_start)) else l_start\n",
    "            final_outputs.append(extract_translation_segment(parent_translation, l_start, l_end, total_lines))\n",
    "        sources = [f\"Global Retrieval ({global_score:.2f})\"] * len(final_outputs)\n",
    "    else:\n",
    "        print(\"[INFO] No strong global match -> hybrid local + neural.\")\n",
    "        neural_preds = {\n",
    "            \"byt5\": generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], test_inputs),\n",
    "            \"t5\": generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], test_inputs),\n",
    "            \"marian\": generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], test_inputs),\n",
    "        }\n",
    "        ensemble_preds = neural_vote(neural_preds)\n",
    "        local_matches = retriever.find_local_matches(test_inputs)\n",
    "\n",
    "        final_outputs, sources = [], []\n",
    "        for i, (retr_text, score) in enumerate(local_matches):\n",
    "            if retr_text and score >= LOCAL_MATCH_THRESHOLD:\n",
    "                final_outputs.append(clean_translation(retr_text))\n",
    "                sources.append(f\"Local Retrieval ({score:.2f})\")\n",
    "            else:\n",
    "                final_outputs.append(ensemble_preds[i])\n",
    "                sources.append(\"Neural Ensemble\")\n",
    "\n",
    "    submission = pd.DataFrame({\"id\": test_df[\"id\"], \"translation\": final_outputs})\n",
    "    submission[\"translation\"] = submission[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Unknown\")\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"\\nPreview:\")\n",
    "    print(submission.head())\n",
    "    return submission, sources\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9084005,
     "sourceId": 14238328,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084006,
     "sourceId": 14238329,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084224,
     "sourceId": 14238668,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
