{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32f78a",
   "metadata": {},
   "source": [
    "# Find Optimal Ensemble Weights\n",
    "\n",
    "This notebook helps you determine the best weights for your 3-model ensemble by evaluating on validation data.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Loads validation data** (5% of training set)\n",
    "2. **Generates predictions** from all 3 models\n",
    "3. **Calculates BLEU scores** for each model individually\n",
    "4. **Performs grid search** over weight combinations\n",
    "5. **Outputs optimal weights** to use in submission notebook\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Train all 3 models** first\n",
    "2. **Add them as inputs** to this notebook\n",
    "3. **Run all cells**\n",
    "4. **Copy the optimal weights** to `final-submission-notebook.ipynb`\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "```python\n",
    "ByT5:     BLEU = 25.5    weight = 0.35\n",
    "T5:       BLEU = 26.8    weight = 0.42  ← Best\n",
    "MarianMT: BLEU = 22.7    weight = 0.23\n",
    "\n",
    "Ensemble: BLEU = 28.1   (+1.3 improvement)\n",
    "```\n",
    "\n",
    "## Time Required\n",
    "\n",
    "- Small validation set (100 samples): ~5 minutes\n",
    "- Full validation (1000+ samples): ~30-60 minutes\n",
    "- Worth it for optimal performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa301adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL1_PATH = \"/kaggle/input/notebook-a-byt5/byt5-base-saved\"\n",
    "MODEL2_PATH = \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\"\n",
    "MODEL3_PATH = \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOAD VALIDATION DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create validation split from training data\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "\n",
    "# Take last 5% as validation\n",
    "val_size = int(len(train_df) * 0.05)\n",
    "val_df = train_df.tail(val_size).reset_index(drop=True)\n",
    "\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Prepare validation data\n",
    "val_sources = val_df['transliteration'].astype(str).tolist()\n",
    "val_targets = val_df['translation'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b73331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GAP REPLACEMENT\n",
    "# -----------------------------------------------------------------------------\n",
    "def replace_gaps(text):\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "    text = re.sub(r'……', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "    text = re.sub(r'…', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "    return text\n",
    "\n",
    "val_sources = [replace_gaps(s) for s in val_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GENERATE PREDICTIONS FROM EACH MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length, prefix=\"\"):\n",
    "        self.texts = [prefix + str(t) for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "def generate_predictions(model_path, sources, prefix=\"translate Akkadian to English: \"):\n",
    "    \"\"\"Generate predictions from a model\"\"\"\n",
    "    print(f\"\\nGenerating from {model_path}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = InferenceDataset(sources, tokenizer, MAX_LENGTH, prefix)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=MAX_LENGTH,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend([d.strip() for d in decoded])\n",
    "    \n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Generate from each model\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS FROM ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "preds_byt5 = generate_predictions(MODEL1_PATH, val_sources, \"translate Akkadian to English: \")\n",
    "preds_t5 = generate_predictions(MODEL2_PATH, val_sources, \"translate Akkadian to English: \")\n",
    "preds_marian = generate_predictions(MODEL3_PATH, val_sources, \">>eng<< \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a41c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# EVALUATE INDIVIDUAL MODELS\n",
    "# -----------------------------------------------------------------------------\n",
    "def calculate_bleu(predictions, references):\n",
    "    \"\"\"Calculate BLEU score\"\"\"\n",
    "    result = bleu_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[[ref] for ref in references]\n",
    "    )\n",
    "    return result['bleu'] * 100  # Convert to percentage\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL MODEL SCORES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "score_byt5 = calculate_bleu(preds_byt5, val_targets)\n",
    "score_t5 = calculate_bleu(preds_t5, val_targets)\n",
    "score_marian = calculate_bleu(preds_marian, val_targets)\n",
    "\n",
    "print(f\"ByT5:     BLEU = {score_byt5:.2f}\")\n",
    "print(f\"T5:       BLEU = {score_t5:.2f}\")\n",
    "print(f\"MarianMT: BLEU = {score_marian:.2f}\")\n",
    "\n",
    "# Calculate proportional weights\n",
    "total_score = score_byt5 + score_t5 + score_marian\n",
    "w1_prop = score_byt5 / total_score\n",
    "w2_prop = score_t5 / total_score\n",
    "w3_prop = score_marian / total_score\n",
    "\n",
    "print(f\"\\nProportional weights:\")\n",
    "print(f\"  ByT5:     {w1_prop:.4f}\")\n",
    "print(f\"  T5:       {w2_prop:.4f}\")\n",
    "print(f\"  MarianMT: {w3_prop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# GRID SEARCH FOR OPTIMAL WEIGHTS\n",
    "# -----------------------------------------------------------------------------\n",
    "def ensemble_with_weights(preds1, preds2, preds3, w1, w2, w3, references):\n",
    "    \"\"\"Create ensemble predictions and evaluate\"\"\"\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    for i in range(len(references)):\n",
    "        # Simple voting: pick prediction from highest weighted model\n",
    "        # For more sophisticated approach, use scoring + weighted combination\n",
    "        options = [\n",
    "            (w1, preds1[i]),\n",
    "            (w2, preds2[i]),\n",
    "            (w3, preds3[i])\n",
    "        ]\n",
    "        # Pick from best weighted model\n",
    "        best_pred = max(options, key=lambda x: x[0] * len(x[1].split()))[1]\n",
    "        ensemble_preds.append(best_pred)\n",
    "    \n",
    "    score = calculate_bleu(ensemble_preds, references)\n",
    "    return score, ensemble_preds\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRID SEARCH FOR OPTIMAL WEIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_score = 0\n",
    "best_weights = None\n",
    "best_predictions = None\n",
    "\n",
    "# Search over weight combinations\n",
    "search_range = np.arange(0.2, 0.6, 0.1)\n",
    "\n",
    "results = []\n",
    "\n",
    "for w1 in search_range:\n",
    "    for w2 in search_range:\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        \n",
    "        # Skip invalid combinations\n",
    "        if w3 < 0.1 or w3 > 0.6:\n",
    "            continue\n",
    "        \n",
    "        score, preds = ensemble_with_weights(\n",
    "            preds_byt5, preds_t5, preds_marian,\n",
    "            w1, w2, w3,\n",
    "            val_targets\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'w1': w1,\n",
    "            'w2': w2,\n",
    "            'w3': w3,\n",
    "            'bleu': score\n",
    "        })\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_weights = (w1, w2, w3)\n",
    "            best_predictions = preds\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values('bleu', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 weight combinations:\")\n",
    "print(results_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST WEIGHTS FOUND\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ByT5:     {best_weights[0]:.4f}\")\n",
    "print(f\"T5:       {best_weights[1]:.4f}\")\n",
    "print(f\"MarianMT: {best_weights[2]:.4f}\")\n",
    "print(f\"BLEU:     {best_score:.2f}\")\n",
    "\n",
    "print(f\"\\nImprovement over best single model: +{best_score - max(score_byt5, score_t5, score_marian):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# SAVE OPTIMAL WEIGHTS\n",
    "# -----------------------------------------------------------------------------\n",
    "optimal_config = {\n",
    "    'weights': {\n",
    "        'byt5': best_weights[0],\n",
    "        't5': best_weights[1],\n",
    "        'marian': best_weights[2]\n",
    "    },\n",
    "    'validation_bleu': best_score,\n",
    "    'individual_scores': {\n",
    "        'byt5': score_byt5,\n",
    "        't5': score_t5,\n",
    "        'marian': score_marian\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDED CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Copy these weights to your ensemble notebook:\n",
    "\n",
    "MODEL_CONFIGS = {{\n",
    "    \"byt5\": {{\n",
    "        \"path\": MODEL1_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": {best_weights[0]:.4f},\n",
    "        \"num_beams\": 4\n",
    "    }},\n",
    "    \"t5\": {{\n",
    "        \"path\": MODEL2_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": {best_weights[1]:.4f},\n",
    "        \"num_beams\": 4\n",
    "    }},\n",
    "    \"marian\": {{\n",
    "        \"path\": MODEL3_PATH,\n",
    "        \"prefix\": \">>eng<< \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": {best_weights[2]:.4f},\n",
    "        \"num_beams\": 4\n",
    "    }}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('optimal_weights.json', 'w') as f:\n",
    "    json.dump(optimal_config, f, indent=2)\n",
    "\n",
    "print(\"✅ Optimal weights saved to optimal_weights.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d37e0a",
   "metadata": {},
   "source": [
    "## How to Use These Results\n",
    "\n",
    "1. **Copy the optimal weights** to your ensemble notebook\n",
    "2. **Update MODEL_CONFIGS** with the recommended values\n",
    "3. **Run the ensemble** on test data\n",
    "4. **Submit** to competition\n",
    "\n",
    "## Tips for Further Improvement\n",
    "\n",
    "- Try different **num_beams** values (4, 6, 8)\n",
    "- Adjust **max_length** per model based on output patterns\n",
    "- Add **repetition_penalty** if outputs are repetitive\n",
    "- Implement **sophisticated voting** (e.g., BLEU-based selection)\n",
    "- Use **temperature** sampling for diversity\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "With optimal weights, expect:\n",
    "- **Validation BLEU**: ~{best_score:.1f}\n",
    "- **Improvement over single best**: +{best_score - max(score_byt5, score_t5, score_marian):.2f} points\n",
    "- **Test performance**: Should generalize well if validation set is representative"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
