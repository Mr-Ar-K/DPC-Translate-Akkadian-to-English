{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":121150,"databundleVersionId":14976537,"sourceType":"competition"},{"sourceId":14236819,"sourceType":"datasetVersion","datasetId":9082937}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A1. Install required libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate sacrebleu","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A2. Imports & config","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments\n)\nimport evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A3. Set constants (DO NOT change yet)","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nOUTPUT_DIR = \"./byt5-base-saved\"\n\n# ByT5 is character-based, so sequences are longer. \n# Akkadian texts can be long, but 512 is usually a safe upper limit for sentence-level.\nMAX_LENGTH = 400 \nPREFIX = \"translate Akkadian to English: \"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A4. Data Loading & Cleaning","metadata":{}},{"cell_type":"code","source":"def load_and_align_data(filepath):\n    \"\"\"\n    Reads the csv and aligns transliterations to translations at the sentence level.\n    This is critical because the raw data is document-level.\n    \"\"\"\n    df = pd.read_csv(filepath)\n    aligned_rows = []\n\n    print(f\"Raw documents: {len(df)}\")\n\n    for _, row in df.iterrows():\n        src = str(row[\"transliteration\"])\n        tgt = str(row[\"translation\"])\n\n        # Simple heuristic alignment\n        # Split source by newlines (traditional for cuneiform transliteration)\n        src_lines = [s.strip() for s in src.split(\"\\n\") if s.strip()]\n        \n        # Split target by sentence punctuation\n        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n\n        # If the counts match roughly, we assume 1:1 mapping (High Quality Data)\n        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n            for s, t in zip(src_lines, tgt_sents):\n                # Filter out very short garbage fragments\n                if len(s) > 3 and len(t) > 3:\n                    aligned_rows.append({\"transliteration\": s, \"translation\": t})\n        else:\n            # Fallback: Use the whole document if we can't align perfectly\n            # This ensures we don't lose data, even if it's noisy\n            aligned_rows.append({\"transliteration\": src, \"translation\": tgt})\n\n    print(f\"Aligned training examples: {len(aligned_rows)}\")\n    return pd.DataFrame(aligned_rows)\n\n# Load data\ntrain_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n\n# Convert to HuggingFace Dataset\ndataset = Dataset.from_pandas(train_df)\n# Split: 95% Train, 5% Validation (we need max data for training)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A5 . Tokenization","metadata":{}},{"cell_type":"code","source":"print(\"Loading Tokenizer from:\", MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndef preprocess_function(examples):\n    inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n    targets = examples[\"translation\"]\n\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\" # Consistent padding helps training stability\n    )\n    \n    labels = tokenizer(\n        targets, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\"\n    )\n\n    # Replace padding token id with -100 so it's ignored by the loss function\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Process datasets\ntokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ntokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A6. Model Setup","metadata":{}},{"cell_type":"code","source":"print(\"Loading Model from:\", MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n\n# Data Collator handles dynamic padding during batching\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    label_pad_token_id=-100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A7. Training Arguments","metadata":{}},{"cell_type":"code","source":"# --- 5. Training Arguments ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    \n    # --- CRITICAL FIX: Stop saving checkpoints to save disk space ---\n    save_strategy=\"no\",           # Do not save every epoch\n    eval_strategy=\"epoch\",        # Still evaluate to see progress\n    load_best_model_at_end=False, # Must be False if save_strategy is \"no\"\n    # ---------------------------------------------------------------\n\n    learning_rate=3e-4,\n    \n    # Keep your memory fixes\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing=True,\n    \n    num_train_epochs=9, #increased to 9 from 5\n    weight_decay=0.01,\n    predict_with_generate=True,\n    fp16=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A8. Trainer","metadata":{}},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A9. Execution","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"Starting Training with Memory Fixes...\")\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# A10. Save Final Model","metadata":{}},{"cell_type":"code","source":"print(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(\"Notebook A Complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}