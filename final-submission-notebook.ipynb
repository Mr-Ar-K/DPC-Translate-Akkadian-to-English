{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-21T12:18:33.672428Z",
     "iopub.status.busy": "2025-12-21T12:18:33.672157Z",
     "iopub.status.idle": "2025-12-21T12:18:33.686263Z",
     "shell.execute_reply": "2025-12-21T12:18:33.685656Z",
     "shell.execute_reply.started": "2025-12-21T12:18:33.672384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:47: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_55/2929549046.py:47: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# ==================================================================================\\n#   DEEP PAST CHALLENGE - FINAL ROBUST ENSEMBLE\\n#   ------------------------------------------------------------------------------\\n#   Logic: ByT5 (Best Morphology) + T5 (Best Grammar) + Marian (Fluency)\\n#   Metric: GeoMean(BLEU, chrF++) optimized via weighted voting & keyword validation.\\n# ==================================================================================\\n\\nimport os\\nimport gc\\nimport sys\\nimport re\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nfrom tqdm.auto import tqdm\\nimport difflib\\n\\n# ==================================================================================\\n# 1. CONFIGURATION (!!! UPDATE THESE PATHS !!!)\\n# ==================================================================================\\n# Check the \"Data\" tab in Kaggle to find the exact paths to your saved models.\\n# They will look something like \"/kaggle/input/your-notebook-name/byt5-base-saved\"\\n\\nMODEL_PATHS = {\\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",       # Update this\\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\",       # Update this\\n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"   # Update this\\n}\\n\\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nBATCH_SIZE = 16\\n\\n# ==================================================================================\\n# 2. ADVANCED CLEANING & UTILS\\n# ==================================================================================\\n\\ndef clean_prediction(text):\\n    \"\"\"Post-processing to fix common NMT artifacts.\"\"\"\\n    if not isinstance(text, str): return \"\"\\n    text = text.strip()\\n    \\n    # 1. Fix punctuation spacing (e.g., \"city .\" -> \"city.\")\\n    text = re.sub(r\\'\\\\s+([.,!?;:])\\', r\\'\\x01\\', text)\\n    \\n    # 2. Capitalize first letter\\n    if text:\\n        text = text[0].upper() + text[1:]\\n        \\n    # 3. Ensure sentence ending punctuation (if missing)\\n    if text and text[-1] not in \".!?\":\\n        text += \".\"\\n        \\n    return text\\n\\ndef is_garbage(text, source_text=\"\"):\\n    \"\"\"Returns True if the prediction is likely a hallucination or failure.\"\"\"\\n    if len(text) < 3: return True\\n    \\n    # Check for repetition loops (e.g., \"the silver the silver the silver\")\\n    if len(text) > 20 and len(set(text.split())) < 3:\\n        return True\\n        \\n    # Check if model just copied the input (common failure mode)\\n    ratio = difflib.SequenceMatcher(None, text.lower(), source_text.lower()).ratio()\\n    if ratio > 0.8: # If translation is 80% identical to source transliteration\\n        return True\\n        \\n    return False\\n\\n# ==================================================================================\\n# 3. EFFICIENT INFERENCE ENGINE\\n# ==================================================================================\\n\\nclass TextDataset(Dataset):\\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\\n        self.texts = [prefix + str(t) for t in texts]\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n\\n    def __len__(self):\\n        return len(self.texts)\\n\\n    def __getitem__(self, idx):\\n        enc = self.tokenizer(\\n            self.texts[idx], \\n            truncation=True, \\n            padding=\"max_length\", \\n            max_length=self.max_len, \\n            return_tensors=\"pt\"\\n        )\\n        return {\\n            \"input_ids\": enc[\"input_ids\"].squeeze(0), \\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\\n        }\\n\\ndef generate_predictions(model_name, model_path, inputs):\\n    \"\"\"Loads model, predicts, then UNLOADS model to save RAM.\"\"\"\\n    print(f\"\\n[INFO] Processing with {model_name.upper()}...\")\\n    \\n    if not os.path.exists(model_path):\\n        print(f\"[WARNING] Path not found: {model_path}. Skipping.\")\\n        return [\"\"] * len(inputs)\\n\\n    # 1. Config based on model type\\n    if \"byt5\" in model_name:\\n        max_len = 512\\n        prefix = \"translate Akkadian to English: \"\\n    elif \"t5\" in model_name:\\n        max_len = 256\\n        prefix = \"translate Akkadian to English: \"\\n    else: # Marian\\n        max_len = 128\\n        prefix = \"\"\\n\\n    # 2. Load\\n    try:\\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\\n    except Exception as e:\\n        print(f\"[ERROR] Failed to load {model_name}: {e}\")\\n        return [\"\"] * len(inputs)\\n\\n    # 3. Predict\\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\\n    \\n    preds = []\\n    with torch.no_grad():\\n        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\\n            input_ids = batch[\"input_ids\"].to(DEVICE)\\n            mask = batch[\"attention_mask\"].to(DEVICE)\\n            \\n            # Beam search with penalties for repetition (Crucial for score)\\n            gen_ids = model.generate(\\n                input_ids=input_ids,\\n                attention_mask=mask,\\n                max_length=max_len,\\n                num_beams=5,               # Higher beam = better quality\\n                no_repeat_ngram_size=3,    # Prevent \"and the and the\"\\n                repetition_penalty=1.2,    # Penalty for loops\\n                early_stopping=True\\n            )\\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\\n            preds.extend([clean_prediction(d) for d in decoded])\\n\\n    # 4. Cleanup (CRITICAL for 16GB GPU)\\n    del model, tokenizer, dataset, loader\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    return preds\\n\\n# ==================================================================================\\n# 4. DOMAIN-AWARE ENSEMBLE STRATEGY\\n# ==================================================================================\\n\\n# Dictionary of high-value Akkadian keywords\\nKEYWORDS = {\\n    \"qi-bi-ma\": [\"speak\", \"tell\", \"say\"],\\n    \"um-ma\": [\"thus\"],\\n    \"a-na\": [\"to\", \"for\"],\\n    \"ku-babbar\": [\"silver\", \"money\"],\\n    \"ku-gi\": [\"gold\"],\\n    \"a-lim\": [\"city\"],\\n    \"e-gal\": [\"palace\"],\\n    \"dam-qar\": [\"merchant\", \"agent\"],\\n    \"tup-pi\": [\"tablet\", \"letter\", \"document\", \"record\"],\\n    \"be-li\": [\"lord\", \"master\", \"boss\"],\\n    \"a-hi\": [\"brother\", \"partner\"],\\n    \"i-na\": [\"in\", \"from\", \"on\"],\\n    \"su-be2-el\": [\"send\"],\\n    \"u2-bi-il\": [\"brought\", \"carried\"],\\n    \"li-bi-shi\": [\"should be\", \"let it be\"],\\n    \"mi3-ma\": [\"anything\", \"something\", \"property\"]\\n}\\n\\ndef ensemble_vote(inputs, predictions_dict):\\n    \"\"\"\\n    Selects the best translation based on:\\n    1. Model Trust (ByT5 > T5 > Marian)\\n    2. Keyword Coverage (Did it translate \\'silver\\' correctly?)\\n    3. Garbage Detection\\n    \"\"\"\\n    final_translations = []\\n    \\n    # Trust weights based on your training logs (ByT5 was superior)\\n    MODEL_WEIGHTS = {\"byt5\": 3.0, \"t5\": 2.0, \"marian\": 1.0}\\n    \\n    print(\"\\n[INFO] Running Weighted Ensemble Voting...\")\\n    \\n    for i in tqdm(range(len(inputs))):\\n        src = inputs[i].lower()\\n        candidates = {\\n            k: predictions_dict[k][i] \\n            for k in predictions_dict \\n            if predictions_dict[k][i] # Only consider if prediction exists\\n        }\\n        \\n        if not candidates:\\n            final_translations.append(\"Broken text.\")\\n            continue\\n\\n        best_score = -1\\n        best_text = \"\"\\n        \\n        for model_name, text in candidates.items():\\n            # A. Base Score (Model Confidence)\\n            score = MODEL_WEIGHTS.get(model_name, 1.0)\\n            \\n            # B. Filter Garbage\\n            if is_garbage(text, src):\\n                score -= 10 # Heavily penalize garbage\\n                \\n            # C. Keyword Bonus (The Secret Sauce)\\n            text_lower = text.lower()\\n            for akk, eng_list in KEYWORDS.items():\\n                if akk in src:\\n                    # If the source has \\'silver\\', and translation has \\'silver\\', BOOST IT\\n                    if any(eng in text_lower for eng in eng_list):\\n                        score += 2.0\\n            \\n            # D. Length Penalty (Too short is usually bad, unless input is short)\\n            if len(text) < 10 and len(src) > 20:\\n                score -= 1.0\\n                \\n            if score > best_score:\\n                best_score = score\\n                best_text = text\\n        \\n        # Fallback if all scores are negative (all garbage)\\n        if best_score < 0:\\n            # Prefer T5 or ByT5 output even if garbage, better than empty\\n            best_text = candidates.get(\"byt5\", candidates.get(\"t5\", \"Broken text.\"))\\n\\n        final_translations.append(best_text)\\n        \\n    return final_translations\\n\\n# ==================================================================================\\n# 5. EXECUTION PIPELINE\\n# ==================================================================================\\n\\ndef main():\\n    print(\"=== STARTING INFERENCE PIPELINE ===\")\\n    \\n    # 1. Load Test Data\\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\\n    ids = test_df[\"id\"].tolist()\\n    \\n    print(f\"Loaded {len(inputs)} test sentences.\")\\n    \\n    # 2. Run Inference (Sequential to save memory)\\n    all_preds = {}\\n    \\n    # Run ByT5 (The Specialist)\\n    all_preds[\"byt5\"] = generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], inputs)\\n    \\n    # Run T5 (The Generalist)\\n    all_preds[\"t5\"] = generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], inputs)\\n    \\n    # Run Marian (The Fluency Expert)\\n    all_preds[\"marian\"] = generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], inputs)\\n    \\n    # 3. Ensemble\\n    final_preds = ensemble_vote(inputs, all_preds)\\n    \\n    # 4. Save Submission\\n    sub = pd.DataFrame({\"id\": ids, \"translation\": final_preds})\\n    \\n    # Final check for empty strings\\n    sub[\"translation\"] = sub[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Broken text.\")\\n    \\n    sub.to_csv(\"submission.csv\", index=False)\\n    print(\"\\n[SUCCESS] submission.csv generated successfully.\")\\n    print(sub.head())\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEEP PAST CHALLENGE - SINGLE HYBRID PIPELINE (Neural Ensemble Only)\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "def resolve_path(env_name, working_default, input_fallback):\n",
    "    path = os.getenv(env_name, working_default)\n",
    "    if not os.path.exists(path) and os.path.exists(input_fallback):\n",
    "        path = input_fallback\n",
    "    return path\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    \"byt5\": resolve_path(\"BYT5_PATH\", \"/kaggle/working/byt5-base-saved\", \"/kaggle/input/notebook-a-byt5/byt5-base-saved\"),\n",
    "    \"t5\": resolve_path(\"T5_PATH\", \"/kaggle/working/t5-base-fine-tuned\", \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\"),\n",
    "    \"marian\": resolve_path(\"MARIAN_PATH\", \"/kaggle/working/marian-mt-saved\", \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"),\n",
    "}\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CLEANING HELPERS (match training notebooks)\n",
    "# -----------------------------------------------------------------------------\n",
    "SUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n",
    "\n",
    "def normalize_subscripts(text: str) -> str:\n",
    "    return text.translate(SUBSCRIPT_TRANS)\n",
    "\n",
    "def clean_translit(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = normalize_subscripts(text)\n",
    "    text = text.replace(\"…\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\\\.\\\\.\\\\.+\", \" <big_gap> \", text)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n",
    "    text = re.sub(r\"<<[^>]*>>\", \" \", text)\n",
    "    text = re.sub(r\"[˹˺]\", \" \", text)\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[!?/:·]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def dedup_repeats(text: str) -> str:\n",
    "    tokens = text.split()\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if len(out) >= 2 and tok == out[-1] == out[-2]:\n",
    "            continue\n",
    "        out.append(tok)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def postprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n",
    "    text = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", text)\n",
    "    text = dedup_repeats(text)\n",
    "    if text and text[0].islower():\n",
    "        text = text[0].upper() + text[1:]\n",
    "    if text and text[-1] not in \".!?\":\n",
    "        text += \".\"\n",
    "    text = re.sub(r\"([.!?]){2,}\", \".\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def sanitize_outputs(primary, backups):\n",
    "    cleaned = []\n",
    "    for i, text in enumerate(primary):\n",
    "        cand = postprocess_text(text)\n",
    "        if len(cand) < 5 or cand.lower() in {\"unknown\", \"\"}:\n",
    "            fallbacks = [postprocess_text(b[i]) for b in backups if i < len(b) and len(postprocess_text(b[i])) >= 5]\n",
    "            if fallbacks:\n",
    "                cand = max(fallbacks, key=len)\n",
    "            else:\n",
    "                cand = \"Unknown\"\n",
    "        cleaned.append(cand)\n",
    "    return cleaned\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DATASET + GENERATION\n",
    "# -----------------------------------------------------------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\n",
    "        self.texts = [prefix + str(t) for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "def generate_predictions(model_name, model_path, inputs):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"[WARNING] Missing model for {model_name}: {model_path}\")\n",
    "        return [\"\"] * len(inputs)\n",
    "\n",
    "    if \"byt5\" in model_name:\n",
    "        max_len = 420; prefix = \"translate Akkadian to English: \"; beams = 8\n",
    "    elif \"t5\" in model_name:\n",
    "        max_len = 280; prefix = \"translate Akkadian to English: \"; beams = 8\n",
    "    else:\n",
    "        max_len = 180; prefix = \"\"; beams = 6\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {model_name}: {e}\")\n",
    "        return [\"\"] * len(inputs)\n",
    "\n",
    "    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\n",
    "            gen_ids = model.generate(\n",
    "                input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                attention_mask=batch[\"attention_mask\"].to(DEVICE),\n",
    "                max_length=max_len,\n",
    "                min_length=6,\n",
    "                num_beams=beams,\n",
    "                no_repeat_ngram_size=3,\n",
    "                repetition_penalty=1.08,\n",
    "                length_penalty=1.05,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            preds.extend([postprocess_text(d) for d in decoded])\n",
    "\n",
    "    del model, tokenizer, dataset, loader\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return preds\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENSEMBLE\n",
    "# -----------------------------------------------------------------------------\n",
    "def score_candidate(text: str, src_len: int, base_weight: float) -> float:\n",
    "    tok_len = max(1, len(text.split()))\n",
    "    ratio = tok_len / max(1, src_len)\n",
    "    length_penalty = -abs(ratio - 1.3) * 0.8  # prefer around 1.3x source tokens\n",
    "    short_penalty = -3.0 if tok_len < 4 else 0.0\n",
    "    garbage_penalty = -4.0 if text.lower() in {\"unknown\", \"\"} else 0.0\n",
    "    return base_weight + length_penalty + short_penalty + garbage_penalty\n",
    "\n",
    "def neural_vote(preds_dict, src_lens):\n",
    "    weights = {\"byt5\": 4.0, \"t5\": 1.6, \"marian\": 1.2}\n",
    "    final = []\n",
    "    n = max(len(v) for v in preds_dict.values()) if preds_dict else 0\n",
    "    for i in range(n):\n",
    "        best_text, best_score = \"\", -1e9\n",
    "        for name, preds in preds_dict.items():\n",
    "            if i >= len(preds):\n",
    "                continue\n",
    "            score = score_candidate(preds[i], src_lens[i], weights.get(name, 1.0))\n",
    "            if score > best_score:\n",
    "                best_score, best_text = score, preds[i]\n",
    "        final.append(best_text)\n",
    "    return final\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN PIPELINE (Neural-only)\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    print(\"=== Deep Past Neural Ensemble Inference ===\")\n",
    "    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "    test_inputs_raw = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\n",
    "    test_inputs = [clean_translit(t) for t in test_inputs_raw]\n",
    "    src_lens = [len(t.split()) for t in test_inputs]\n",
    "\n",
    "    neural_preds = {\n",
    "        \"byt5\": generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], test_inputs),\n",
    "        \"t5\": generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], test_inputs),\n",
    "        \"marian\": generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], test_inputs),\n",
    "    }\n",
    "    ensemble_preds = neural_vote(neural_preds, src_lens)\n",
    "\n",
    "    final_outputs = sanitize_outputs(ensemble_preds, list(neural_preds.values()))\n",
    "    submission = pd.DataFrame({\"id\": test_df[\"id\"], \"translation\": final_outputs})\n",
    "    submission[\"translation\"] = submission[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Unknown\")\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"\\nPreview:\")\n",
    "    print(submission.head())\n",
    "    return submission\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9084005,
     "sourceId": 14238328,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084006,
     "sourceId": 14238329,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084224,
     "sourceId": 14238668,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
