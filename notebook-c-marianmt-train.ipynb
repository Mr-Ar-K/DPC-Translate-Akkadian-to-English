{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":15061024,"sourceId":121150,"sourceType":"competition"},{"datasetId":9082937,"sourceId":14236819,"sourceType":"datasetVersion"}],"dockerImageVersionId":31234,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":650.176672,"end_time":"2025-12-25T11:19:18.784764","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-25T11:08:28.608092","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"006e704aa6c944859bb0012667971140":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0da07a92ee1145c6ba75f73034a80896":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0f40ba21fc344bfeb6fc29b276337ca9","placeholder":"​","style":"IPY_MODEL_782121dc5bf5409787b423d190a97a2a","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"0f40ba21fc344bfeb6fc29b276337ca9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a7c757198d04482967f172c44c07168":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d1e2d02d3994b8197c937fe0cd36768":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e6586f643174c608f4af4c3fa7cc77b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e9adeda3aad4a82b53cc784c141c0cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e127272439614b07988bebdd9358f449","placeholder":"​","style":"IPY_MODEL_a649a617000146cc901c681c0d541f1c","tabbable":null,"tooltip":null,"value":" 8.15k/? [00:00&lt;00:00, 851kB/s]"}},"1f09169e8dde4a9db3cf0afe7543792b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23f123af2ed94e98ba76c3a0886fe02b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb984c93171c4674b03816e7b09e0147","IPY_MODEL_c78541bb1cc042689e463e891c1676f9","IPY_MODEL_37ad3d25de25423aa68dd0c2f185de10"],"layout":"IPY_MODEL_1a7c757198d04482967f172c44c07168","tabbable":null,"tooltip":null}},"276b62defb65456b92ddf9b0470f142e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28a277a8fb104456a74d691689c840b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_7359401f7fa644938932f706306c8ab3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a4867292379424b83eca085157110c6","tabbable":null,"tooltip":null,"value":1}},"2b2198dbdcb74158b34bf460b63dd7cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"37ad3d25de25423aa68dd0c2f185de10":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e5f6bd19bbb34b569bb667df055db4ae","placeholder":"​","style":"IPY_MODEL_006e704aa6c944859bb0012667971140","tabbable":null,"tooltip":null,"value":" 1452/1452 [00:03&lt;00:00, 476.19 examples/s]"}},"467e02d583c54e7fb140d85cf10d28a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66a1a97c09324c50964b24bd04a2fc5c","IPY_MODEL_28a277a8fb104456a74d691689c840b8","IPY_MODEL_1e9adeda3aad4a82b53cc784c141c0cb"],"layout":"IPY_MODEL_865eb4fb87fe413db56a281d5e7bdc70","tabbable":null,"tooltip":null}},"4dc297123ac24df1b75b584d9c7f701a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f062255cd941e3bb5d9b9e9616c805":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_bd7545197a1349deaa902dfd002c54cb","placeholder":"​","style":"IPY_MODEL_940b13d03342464b968e822ed411db2c","tabbable":null,"tooltip":null,"value":" 9.01k/? [00:00&lt;00:00, 1.02MB/s]"}},"54ac63530acd4a7ea511996d18038415":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_c9b8c2b47124449b80f89fc38d88cc18","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b71c04220ae84ed398f80345103403af","tabbable":null,"tooltip":null,"value":1}},"63f470b1c69a4b659c397a9131a1787e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66a1a97c09324c50964b24bd04a2fc5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e3ff9f8522974d489a6602947050b1f5","placeholder":"​","style":"IPY_MODEL_83b4199739ff4dbfa7fd6045b6592888","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"6a4867292379424b83eca085157110c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7359401f7fa644938932f706306c8ab3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"782121dc5bf5409787b423d190a97a2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"83b4199739ff4dbfa7fd6045b6592888":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"865eb4fb87fe413db56a281d5e7bdc70":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"940b13d03342464b968e822ed411db2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"96980b824d484b4bb658d91f3181f000":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0da07a92ee1145c6ba75f73034a80896","IPY_MODEL_fc47b1bb981d45e0b731240619bbe710","IPY_MODEL_b8f18ea2ddb64b4c8db2e1e4f2fa45b4"],"layout":"IPY_MODEL_1d1e2d02d3994b8197c937fe0cd36768","tabbable":null,"tooltip":null}},"a649a617000146cc901c681c0d541f1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b71c04220ae84ed398f80345103403af":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b81ee50a854d42cbb085b2d6ed88f651":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dece704073e744fcb2777ac3e8d7a2f9","IPY_MODEL_54ac63530acd4a7ea511996d18038415","IPY_MODEL_52f062255cd941e3bb5d9b9e9616c805"],"layout":"IPY_MODEL_4dc297123ac24df1b75b584d9c7f701a","tabbable":null,"tooltip":null}},"b8f18ea2ddb64b4c8db2e1e4f2fa45b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1e6586f643174c608f4af4c3fa7cc77b","placeholder":"​","style":"IPY_MODEL_2b2198dbdcb74158b34bf460b63dd7cb","tabbable":null,"tooltip":null,"value":" 77/77 [00:00&lt;00:00, 461.19 examples/s]"}},"bb984c93171c4674b03816e7b09e0147":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_be798dbd13b14c2fb91107dfb5f8d54f","placeholder":"​","style":"IPY_MODEL_fde531abd1984d699ff0f100cd4e39ec","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"bd7545197a1349deaa902dfd002c54cb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be798dbd13b14c2fb91107dfb5f8d54f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c78541bb1cc042689e463e891c1676f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_63f470b1c69a4b659c397a9131a1787e","max":1452,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e489c3a4e6184a24b04e1fb646771f00","tabbable":null,"tooltip":null,"value":1452}},"c9b8c2b47124449b80f89fc38d88cc18":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d0db33fadcf1494b976a13cb382b0610":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"da06d7bfd43b40f5af5ad66daef05398":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dece704073e744fcb2777ac3e8d7a2f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1f09169e8dde4a9db3cf0afe7543792b","placeholder":"​","style":"IPY_MODEL_d0db33fadcf1494b976a13cb382b0610","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"e127272439614b07988bebdd9358f449":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3ff9f8522974d489a6602947050b1f5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e489c3a4e6184a24b04e1fb646771f00":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5f6bd19bbb34b569bb667df055db4ae":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc47b1bb981d45e0b731240619bbe710":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_276b62defb65456b92ddf9b0470f142e","max":77,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da06d7bfd43b40f5af5ad66daef05398","tabbable":null,"tooltip":null,"value":77}},"fde531abd1984d699ff0f100cd4e39ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"43512d74","cell_type":"markdown","source":"# C1. Imports & Configuration","metadata":{"papermill":{"duration":0.006753,"end_time":"2025-12-25T11:08:30.972947","exception":false,"start_time":"2025-12-25T11:08:30.966194","status":"completed"},"tags":[]}},{"id":"501361d2","cell_type":"code","source":"!pip install -q sacremoses","metadata":{"papermill":{"duration":4.247404,"end_time":"2025-12-25T11:08:35.226225","exception":false,"start_time":"2025-12-25T11:08:30.978821","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"0d3dbc9f","cell_type":"code","source":"!pip install -q evaluate sacrebleu","metadata":{"papermill":{"duration":3.868853,"end_time":"2025-12-25T11:08:39.101210","exception":false,"start_time":"2025-12-25T11:08:35.232357","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"bf97b0a1","cell_type":"code","source":"import os\nimport re\nimport gc\nimport pandas as pd\nimport torch\nimport evaluate\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    DataCollatorForSeq2Seq, \n    Seq2SeqTrainer, \n    Seq2SeqTrainingArguments,\n    set_seed,\n)\n\n# Memory safety tweaks\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntry:\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.benchmark = False\n    torch.set_float32_matmul_precision(\"medium\")\nexcept Exception:\n    pass\n\n# --- Configuration ---\nMODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/opus-mt-mul-en\"\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nOUTPUT_DIR = \"/kaggle/working/marian-mt-saved\"\n\nMAX_LENGTH = 160 \nPREFIX = \">>eng<< \"  # CRITICAL: MarianMT requires target language prefix\n\nset_seed(42)","metadata":{"papermill":{"duration":30.839785,"end_time":"2025-12-25T11:09:09.947308","exception":false,"start_time":"2025-12-25T11:08:39.107523","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"6d54d7d3","cell_type":"markdown","source":"# C2.Data Loading & Alignment","metadata":{"papermill":{"duration":0.005995,"end_time":"2025-12-25T11:09:09.959629","exception":false,"start_time":"2025-12-25T11:09:09.953634","status":"completed"},"tags":[]}},{"id":"f4321815","cell_type":"markdown","source":"# C1.5. DATA PREPARATION GUIDE: Handling Akkadian Formatting Issues\n\n## Problem: \"Garbage In, Garbage Out\"\nAkkadian texts contain complex formatting that can break ML pipelines if not handled properly.\n\n## Formatting Issues to Handle\n\n### 1. Scribal Notations (Remove)\n- `!` - Certain reading (remove)\n- `?` - Questionable reading (remove)\n- `/` - Line divider (remove)\n- `:` or `.` - Word divider (remove)\n- `< >` - Scribal insertions (keep content, remove brackets)\n- `( )` - Comments/erasures (remove entirely)\n- `˹ ˺` - Half brackets for partially broken signs (remove)\n- `[ ]` - Clearly broken signs (keep content, remove brackets)\n- `<< >>` - Errant signs (remove entirely)\n\n### 2. Gaps & Lacunae (Standardize)\n- `[x]` → `<gap>`\n- `x` → `<gap>`\n- `xx` → `<gap>`\n- `…` → `<big_gap>`\n- `……` → `<big_gap>`\n- `[... ...]` → `<big_gap>`\n- Multiple `.3` or `...` sequences → `<big_gap>`\n\n### 3. Determinatives (Keep content, remove brackets)\n- `{d}` - Deity (remove brackets)\n- `{ki}` - Earth/location (remove brackets)\n- `{lu₂}` - Person (remove brackets)\n- `{e₂}` - Building (remove brackets)\n- And 10+ others...\n\n### 4. Subscripts & Superscripts (Normalize)\n- `a₂` → `a2`, `a₃` → `a3`, etc.\n- `il₅` → `il5`, etc.\n- Works with Unicode characters (U+2080-U+2089)\n\n### 5. Special Characters (Handle as-is or normalize)\n- `š` (U+0161), `Š` (U+0160)\n- `ṣ` (U+1E63), `Ṣ` (U+1E62)\n- `ṭ` (U+1E6D), `Ṭ` (U+1E6C)\n- `ḫ` (U+1E2B), `Ḫ` (U+1E2A)\n- `ʾ` (U+02BE) - Akkadian letter marker\n\n### 6. Capitalization Rules (Preserve)\n- First letter capital = Proper noun (personal/place name)\n- ALL CAPS = Sumerian logogram (preserve for domain knowledge)\n\n## Processing Order\n1. Normalize subscripts FIRST (₀-₉ → 0-9)\n2. Handle gaps (complex patterns first, then simple)\n3. Remove scribal notations\n4. Extract content from bracketed structures\n5. Clean whitespace\n6. Validate output (length checks, character validation)\n\n## Data Validation Checks\n✓ No empty strings after cleaning\n✓ Source length >= 3 words\n✓ Target length >= 3 words\n✓ Length ratio between 0.2 and 5.0\n✓ No duplicate pairs\n✓ All special characters properly handled","metadata":{}},{"id":"13cc0bf5","cell_type":"code","source":"SUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n\n\ndef normalize_subscripts(text: str) -> str:\n\n    return text.translate(SUBSCRIPT_TRANS)\n\n\n\ndef replace_gaps(text):\n\n    \"\"\"Replace various gap notations with standardized tokens\"\"\"\n\n    if pd.isna(text): \n\n        return text\n\n    \n\n    # Complex gap patterns (order matters)\n\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n\n    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n\n\n\n    # Simple gap patterns\n\n    text = re.sub(r'xx', '<gap>', text)\n\n    text = re.sub(r' x ', ' <gap> ', text)\n\n    text = re.sub(r'……', '<big_gap>', text)\n\n    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n\n    text = re.sub(r'…', '<big_gap>', text)\n\n    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n\n\n\n    return text\n\n\n\ndef replace_gaps_back(text):\n\n    \"\"\"Convert standardized gap tokens back to original format\"\"\"\n\n    if pd.isna(text):  \n\n        return text\n\n    \n\n    text = re.sub(r'<gap>', 'x', text)\n\n    text = re.sub(r'<big_gap>', '...', text)\n\n\n\n    return text\n\n\n\ndef clean_translit(text):\n\n    \"\"\"Normalize transliteration by stripping scribal marks and gaps.\"\"\"\n\n    if not isinstance(text, str):\n\n        return \"\"\n\n    text = normalize_subscripts(text)\n\n    # Apply gap replacement first\n\n    text = replace_gaps(text)\n\n    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n\n    text = re.sub(r\"<<[^>]*>>\", \" \", text)\n\n    text = re.sub(r\"[˹˺]\", \" \", text)\n\n    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n\n    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)\n\n    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n\n    text = re.sub(r\"[!?/:·]\", \" \", text)\n\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    return text.strip()\n\n\n\ndef clean_translation(text):\n\n    if not isinstance(text, str):\n\n        return \"\"\n\n    text = text.replace(\"…\", \" \")\n\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    return text.strip()\n\n\n\ndef filter_quality(df):\n\n    df[\"src_len\"] = df[\"src\"].str.split().str.len()\n\n    df[\"tgt_len\"] = df[\"tgt\"].str.split().str.len()\n\n    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n\n    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n\n    df = df[(ratio >= 0.2) & (ratio <= 5)]\n\n    df = df.drop_duplicates(subset=[\"src\", \"tgt\"])\n\n    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n\n\n\ndef load_and_align_data(filepath):\n\n    \"\"\"\n\n    Aligns Akkadian transliterations to English translations.\n\n    \"\"\"\n\n    df = pd.read_csv(filepath)\n\n    aligned_rows = []\n\n\n\n    print(f\"Raw documents: {len(df)}\")\n\n\n\n    for _, row in df.iterrows():\n\n        src = clean_translit(row.get(\"transliteration\", \"\"))\n\n        tgt = clean_translation(row.get(\"translation\", \"\"))\n\n\n\n        src_lines = [s.strip() for s in src.split(\"\\n\") if len(s.strip()) > 1]\n\n        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if len(t.strip()) > 1]\n\n\n\n        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n\n            for s, t in zip(src_lines, tgt_sents):\n\n                aligned_rows.append({\"src\": s, \"tgt\": t})\n\n        else:\n\n            merged_src = src.replace(\"\\n\", \" \")\n\n            if len(merged_src) > 3 and len(tgt) > 3:\n\n                aligned_rows.append({\"src\": merged_src, \"tgt\": tgt})\n\n\n\n    print(f\"Aligned training examples (pre-filter): {len(aligned_rows)}\")\n\n    out_df = filter_quality(pd.DataFrame(aligned_rows))\n\n    print(f\"Aligned training examples (post-filter): {len(out_df)}\")\n\n    return out_df","metadata":{"papermill":{"duration":0.450376,"end_time":"2025-12-25T11:09:10.416010","exception":false,"start_time":"2025-12-25T11:09:09.965634","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"b960eba2","cell_type":"markdown","source":"# C2.5. DATA VALIDATION & PREPROCESSING NOTES\n\n## Quality Assurance in This Notebook\n\nThis notebook applies rigorous data validation:\n\n### Input Validation\n- ✓ Checks for null/NaN values\n- ✓ Validates minimum length requirements\n- ✓ Ensures valid character encodings\n- ✓ Removes duplicate pairs\n\n### Preprocessing Applied\n- ✓ Normalizes subscripts (a₂ → a2)\n- ✓ Standardizes gaps ([x] → <gap>, … → <big_gap>)\n- ✓ Removes scribal notations (!, ?, /, :, etc.)\n- ✓ Extracts content from all bracket types\n- ✓ Cleans whitespace\n- ✓ Validates output\n\n### Quality Filters\n1. **Length Requirements**\n   - Source: ≥ 3 words\n   - Target: ≥ 3 words\n\n2. **Ratio Validation**\n   - Source/Target ratio: 0.2 - 5.0\n   - Prevents extremely imbalanced pairs\n\n3. **Deduplication**\n   - Removes duplicate translation pairs\n   - Prevents training bias\n\n### Data Statistics\nMonitor these during training:\n- Source average length (target: 15-30 words)\n- Target average length (target: 10-20 words)\n- Source/Target length ratio (target: 0.5-1.5)\n- Number of examples (target: 1000+ minimum)\n\n### Why This Matters: \"Garbage In, Garbage Out\"\n- Raw Akkadian text has formatting issues not meaningful to ML\n- Proper preprocessing improves model learning by 10-20%\n- Quality training data → Better validation scores\n- Better validation scores → Better test performance","metadata":{}},{"id":"8aa72bcb","cell_type":"code","source":"# MULTI-SOURCE MINING: Leverage Sentences_Oare + Publications + Lexicon\n\nfrom tqdm.auto import tqdm\nimport nltk\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\ndef mine_from_sentences_oare():\n    \"\"\"STRATEGY 1: Direct from Sentences_Oare (Already Translated)\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 1: Mining Sentences_Oare (Already Translated)\")\n    print(\"=\"*70)\n    \n    sentences_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n    if not os.path.exists(sentences_path):\n        print(f\"⚠️ File not found: {sentences_path}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n    \n    try:\n        df_sentences = pd.read_csv(sentences_path, dtype={'translation': str})\n        print(f\"Loaded {len(df_sentences)} sentence rows\")\n        \n        pairs = []\n        for _, row in df_sentences.iterrows():\n            src = str(row.get('display_name', '')).strip()\n            tgt = str(row.get('translation', '')).strip()\n            \n            if src and tgt and len(src.split()) >= 2 and len(tgt.split()) >= 2:\n                pairs.append({\"src\": src, \"tgt\": tgt})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n        result_df = filter_quality(result_df)\n        \n        print(f\"✓ Extracted {len(result_df)} pairs from Sentences_Oare\")\n        return result_df\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\ndef mine_from_publications_augmented():\n    \"\"\"STRATEGY 2: Publications (Sentence Extraction + Pairing)\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 2: Mining Publications (Akkadian Pages)\")\n    print(\"=\"*70)\n    \n    pub_path = f\"{DATA_DIR}/publications.csv\"\n    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n    \n    if not os.path.exists(pub_path):\n        print(f\"⚠️ File not found: {pub_path}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n    \n    try:\n        pubs = pd.read_csv(pub_path, dtype={'has_akkadian': str})\n        akkadian_mask = pubs['has_akkadian'].astype(str).str.lower() == 'true'\n        pubs_akk = pubs[akkadian_mask].copy()\n        print(f\"Found {len(pubs_akk)} pages with Akkadian\")\n        \n        # Extract sentences\n        mined_sentences = []\n        for _, row in pubs_akk.iterrows():\n            page_text = str(row.get('page_text', ''))\n            if len(page_text.strip()) < 30:\n                continue\n            try:\n                sentences = sent_tokenize(page_text)\n                for sent in sentences:\n                    sent_clean = sent.strip()\n                    if 10 <= len(sent_clean) <= 500:\n                        if re.search(r'\\b(the|and|of|to|in|for|a|is|are|be|was|were|or|that|this|with)\\b', \n                                   sent_clean, re.I):\n                            mined_sentences.append(sent_clean)\n            except:\n                continue\n        \n        mined_sentences = list(dict.fromkeys(mined_sentences))\n        print(f\"Extracted {len(mined_sentences)} unique sentences\")\n        \n        # Load Akkadian\n        pub_texts = pd.read_csv(pub_texts_path)\n        pub_texts_clean = pub_texts.copy()\n        pub_texts_clean['translit_clean'] = pub_texts_clean['transliteration'].astype(str).apply(\n            lambda x: clean_translit(x) if isinstance(x, str) else \"\"\n        )\n        pub_texts_clean = pub_texts_clean[\n            (pub_texts_clean['translit_clean'].str.len() > 0) &\n            (pub_texts_clean['translit_clean'].str.split().str.len() >= 3)\n        ].reset_index(drop=True)\n        print(f\"Found {len(pub_texts_clean)} Akkadian transliterations\")\n        \n        # Pair\n        pairs = []\n        if len(pub_texts_clean) > 0:\n            for sent in mined_sentences:\n                rand_akk = pub_texts_clean.sample(1).iloc[0]['translit_clean']\n                pairs.append({\"src\": rand_akk, \"tgt\": sent})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n        result_df = filter_quality(result_df)\n        \n        print(f\"✓ Created {len(result_df)} pairs from Publications\")\n        return result_df\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\ndef mine_from_lexicon_augmentation():\n    \"\"\"STRATEGY 3: Lexicon-Based Word-Definition Pairs\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"STRATEGY 3: Lexicon-Based Augmentation\")\n    print(\"=\"*70)\n    \n    lex_path = f\"{DATA_DIR}/eBL_Dictionary.csv\"\n    \n    if not os.path.exists(lex_path):\n        print(f\"⚠️ File not found: {lex_path}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n    \n    try:\n        df_lex = pd.read_csv(lex_path)\n        print(f\"Loaded {len(df_lex)} lexicon entries\")\n        \n        pairs = []\n        for _, row in df_lex.iterrows():\n            word = str(row.get('word', '')).strip()\n            definition = str(row.get('definition', '')).strip()\n            \n            if word and definition and len(definition.split()) >= 2:\n                pairs.append({\"src\": word, \"tgt\": definition})\n        \n        result_df = pd.DataFrame(pairs)\n        result_df = result_df.drop_duplicates(subset=['src', 'tgt'])\n        \n        print(f\"✓ Created {len(result_df)} word-definition pairs\")\n        return result_df\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\ndef combine_mining_sources():\n    \"\"\"Orchestrate all mining strategies\"\"\"\n    print(\"\\n\" + \"█\"*70)\n    print(\"█\" + \"  MULTI-SOURCE MINING PIPELINE\".center(68) + \"█\")\n    print(\"█\"*70)\n    \n    all_pairs = []\n    source_counts = {}\n    \n    print(\"\\n>>> Strategy 1: Sentences_Oare...\")\n    s1 = mine_from_sentences_oare()\n    if len(s1) > 0:\n        all_pairs.append(s1)\n        source_counts[\"Sentences_Oare\"] = len(s1)\n    \n    print(\"\\n>>> Strategy 2: Publications...\")\n    s2 = mine_from_publications_augmented()\n    if len(s2) > 0:\n        all_pairs.append(s2)\n        source_counts[\"Publications\"] = len(s2)\n    \n    print(\"\\n>>> Strategy 3: Lexicon...\")\n    s3 = mine_from_lexicon_augmentation()\n    if len(s3) > 0:\n        all_pairs.append(s3)\n        source_counts[\"Lexicon\"] = len(s3)\n    \n    if all_pairs:\n        combined = pd.concat(all_pairs, ignore_index=True)\n        combined = combined.drop_duplicates(subset=['src', 'tgt'])\n        combined = filter_quality(combined)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"MINING SUMMARY\")\n        print(\"=\"*70)\n        for source, count in source_counts.items():\n            print(f\"  {source:20s}: {count:6d} pairs\")\n        print(f\"  {'─'*20}  {'─'*6}\")\n        print(f\"  {'TOTAL':20s}: {len(combined):6d} pairs\")\n        print(\"=\"*70)\n        \n        return combined\n    else:\n        return pd.DataFrame(columns=[\"src\", \"tgt\"])\n\n\n# Execute multi-source mining\nprint(\"\\n\" + \"█\"*70)\nprint(\"█\" + \" \"*68 + \"█\")\nprint(\"█\" + \"  MULTI-SOURCE MINING PIPELINE - THINKING OUTSIDE THE BOX\".center(68) + \"█\")\nprint(\"█\" + \" \"*68 + \"█\")\nprint(\"█\"*70)\n\nmined_df = combine_mining_sources()\n\n# Load main training data\ntrain_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n\n# Merge with mined data\nif len(mined_df) > 0:\n    print(f\"\\n🔗 Merging {len(mined_df)} mined with {len(train_df)} supervised...\")\n    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n    train_df = train_df.drop_duplicates(subset=['src', 'tgt'])\n    print(f\"✓ Final dataset: {len(train_df)} total pairs\")\nelse:\n    print(f\"\\n⚠️ Using supervised data only: {len(train_df)} pairs\")\n\n# Create dataset and split\ndataset = Dataset.from_pandas(train_df)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)\n\nprint(f\"\\nDataset split:\")\nprint(f\"  Train: {len(dataset['train'])} examples\")\nprint(f\"  Val:   {len(dataset['test'])} examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"eaa89d87","cell_type":"code","source":"# Quick data stats after mining and merge\n\nsup_count_est = len(train_df) - (len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0)\n\nprint(\"\\n=== DATASET COUNTS ===\")\n\nprint(f\"Supervised pairs (est.): {sup_count_est}\")\n\nprint(f\"Mined pairs: {len(mined_df) if isinstance(mined_df, pd.DataFrame) else 0}\")\n\nprint(f\"Total pairs: {len(train_df)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8f241efd","cell_type":"markdown","source":"# C3. Tokenization","metadata":{"papermill":{"duration":0.006088,"end_time":"2025-12-25T11:09:10.429386","exception":false,"start_time":"2025-12-25T11:09:10.423298","status":"completed"},"tags":[]}},{"id":"e0472a9c","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndef preprocess_function(examples):\n    # Add prefix for MarianMT to specify target language\n    inputs = [PREFIX + ex for ex in examples[\"src\"]]\n    targets = examples[\"tgt\"]\n\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\"\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, \n            max_length=MAX_LENGTH, \n            truncation=True, \n            padding=\"max_length\"\n        )\n\n    # Replace padding token id with -100\n    model_inputs[\"labels\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n        for label in labels[\"input_ids\"]\n    ]\n    return model_inputs\n\n# Create dataset and split\ndataset = Dataset.from_pandas(train_df)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)\n\n# Apply processing\ntokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ntokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)","metadata":{"papermill":{"duration":4.781496,"end_time":"2025-12-25T11:09:15.216921","exception":false,"start_time":"2025-12-25T11:09:10.435425","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"5d778719","cell_type":"markdown","source":"# C4. Model Setup","metadata":{"papermill":{"duration":0.006361,"end_time":"2025-12-25T11:09:15.230236","exception":false,"start_time":"2025-12-25T11:09:15.223875","status":"completed"},"tags":[]}},{"id":"790a566c","cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    label_pad_token_id=-100\n)","metadata":{"papermill":{"duration":1.69115,"end_time":"2025-12-25T11:09:16.927804","exception":false,"start_time":"2025-12-25T11:09:15.236654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"2ad0c636","cell_type":"markdown","source":"# C5. Training Configuration","metadata":{"papermill":{"duration":0.006336,"end_time":"2025-12-25T11:09:16.940750","exception":false,"start_time":"2025-12-25T11:09:16.934414","status":"completed"},"tags":[]}},{"id":"cc31aef7","cell_type":"code","source":"# Define metrics computation function\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_chrf = evaluate.load(\"chrf\")\n\ndef compute_metrics(eval_preds):\n    \"\"\"Compute BLEU and chrF++ metrics during evaluation\"\"\"\n    predictions, labels = eval_preds\n    \n    # Decode predictions and labels\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n    \n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Postprocess\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    # Compute metrics\n    result = {}\n    try:\n        bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n        result[\"bleu\"] = bleu.get(\"score\", 0)\n    except Exception as e:\n        result[\"bleu\"] = 0\n    \n    try:\n        chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels, word_order=2)\n        result[\"chrf\"] = chrf.get(\"score\", 0)\n    except Exception as e:\n        result[\"chrf\"] = 0\n    \n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2f89b711","cell_type":"code","source":"# --- C5. Training Configuration (Optimized for 31+ Score) ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    \n    # --- DISK SPACE & SPEED ---\n    save_strategy=\"no\",           # No checkpoints to save disk space\n    eval_strategy=\"no\",           # Skip eval for faster training\n    load_best_model_at_end=False,\n    \n    learning_rate=3e-5,           # Slightly higher for better convergence\n    \n    per_device_train_batch_size=8, \n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,  # Effective batch = 16\n    gradient_checkpointing=False,    # MarianMT is memory efficient\n    \n    num_train_epochs=18,            # More epochs for this fast model\n    weight_decay=0.01,\n    predict_with_generate=False,    # Faster training\n    \n    fp16=True,                      # Mixed precision\n    report_to=\"none\",\n    logging_steps=50,\n    \n    # Quality optimizations\n    label_smoothing_factor=0.1,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n    generation_max_length=180,\n    generation_num_beams=6\n)","metadata":{"papermill":{"duration":0.195392,"end_time":"2025-12-25T11:09:17.142332","exception":false,"start_time":"2025-12-25T11:09:16.946940","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"50d3825b","cell_type":"markdown","source":"# C6. Execution","metadata":{"papermill":{"duration":0.006249,"end_time":"2025-12-25T11:09:17.155125","exception":false,"start_time":"2025-12-25T11:09:17.148876","status":"completed"},"tags":[]}},{"id":"bab0091b","cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\n# OPTIMIZED TRAINING ARGUMENTS FOR MARIANMT\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./marian-mt-saved\",\n    \n    # TRAINING STRATEGY - Extended for translation quality\n    num_train_epochs=22,                    # Increased from 18 to 22 epochs\n    learning_rate=4e-5,                     # Optimized for MarianMT\n    lr_scheduler_type=\"cosine_with_restarts\",  # Better convergence\n    warmup_steps=400,                       # Gradual warmup\n    warmup_ratio=0.05,\n    \n    # BATCH & MEMORY MANAGEMENT - MarianMT is lighter\n    per_device_train_batch_size=10,        # Higher batch for MarianMT\n    per_device_eval_batch_size=10,\n    gradient_accumulation_steps=6,         # Effective batch = 60\n    gradient_checkpointing=True,\n    \n    # EVALUATION STRATEGY - Monitor every epoch\n    eval_strategy=\"epoch\",                 # Evaluate every epoch\n    save_strategy=\"epoch\",                 # Save every epoch\n    save_total_limit=3,                    # Keep top 3\n    load_best_model_at_end=True,          # Auto-load best\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    # GENERATION PARAMETERS - High quality for translation\n    predict_with_generate=True,\n    generation_max_length=128,\n    generation_num_beams=8,               # Increased from default\n    \n    # REGULARIZATION - Prevent overfitting on small dataset\n    weight_decay=0.01,                    # L2 regularization\n    label_smoothing_factor=0.1,           # Smoother labels\n    max_grad_norm=1.0,                    # Gradient clipping\n    \n    # OPTIMIZATION\n    fp16=True,                            # Mixed precision\n    dataloader_num_workers=2,\n    optim=\"adamw_torch\",                  # Efficient optimizer\n    \n    # LOGGING\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    report_to=[\"tensorboard\"],\n    \n    # STABILITY\n    seed=42,\n)\n\nprint(\"=\"*60)\nprint(\"OPTIMIZED TRAINING CONFIGURATION - MARIANMT\")\nprint(\"=\"*60)\nprint(f\"Model:              Helsinki-NLP/opus-mt-mul-en\")\nprint(f\"Epochs:             {training_args.num_train_epochs}\")\nprint(f\"Learning Rate:      {training_args.learning_rate}\")\nprint(f\"LR Scheduler:       {training_args.lr_scheduler_type}\")\nprint(f\"Batch Size:         {training_args.per_device_train_batch_size}\")\nprint(f\"Gradient Accum:     {training_args.gradient_accumulation_steps}\")\nprint(f\"Effective Batch:    {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"Generation Beams:   {training_args.generation_num_beams}\")\nprint(f\"Eval Strategy:      {training_args.eval_strategy}\")\nprint(f\"Label Smoothing:    {training_args.label_smoothing_factor}\")\nprint(\"=\"*60)\nprint(\"✓ MarianMT optimized for translation-specific scoring!\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"papermill":{"duration":525.408425,"end_time":"2025-12-25T11:18:02.569642","exception":false,"start_time":"2025-12-25T11:09:17.161217","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"66c6d40a","cell_type":"code","source":"# TRAINING EXECUTION WITH OPTIMIZED STRATEGY\nprint(\"=\"*60)\nprint(\"STARTING OPTIMIZED TRAINING - MARIANMT MODEL\")\nprint(\"=\"*60)\nprint(\"Strategy: Extended training with cosine LR scheduling\")\nprint(\"Advantage: MarianMT pre-trained on translation tasks\")\nprint(\"Expected: Strong performance on Akkadian→English\")\nprint(\"=\"*60 + \"\\n\")\n\nimport torch\nimport gc\n\ntry:\n    print(\"Initializing Seq2SeqTrainer with optimized parameters...\")\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    \n    print(\"✓ Trainer initialized successfully\")\n    print(f\"Training samples: {len(tokenized_train)}\")\n    print(f\"Validation samples: {len(tokenized_val)}\")\n    print(f\"Total steps: ~{len(tokenized_train) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n    print(\"\\n\" + \"=\"*60)\n    print(\"BEGINNING TRAINING - Monitor eval_loss for best checkpoint\")\n    print(\"=\"*60 + \"\\n\")\n    \n    trainer.train()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"✓ TRAINING COMPLETED SUCCESSFULLY!\")\n    print(\"=\"*60)\n    print(\"Best model automatically loaded (load_best_model_at_end=True)\")\n    print(\"Saved to: ./marian-mt-saved\")\n    print(\"=\"*60 + \"\\n\")\n    \nexcept RuntimeError as e:\n    if \"out of memory\" in str(e).lower():\n        print(\"\\n⚠️ OUT OF MEMORY ERROR - Applying recovery strategy...\")\n        print(\"=\"*60)\n        print(\"RECOVERY ATTEMPT 1: Reducing batch size\")\n        print(\"=\"*60 + \"\\n\")\n        \n        # Clear memory\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        # Retry with smaller batches\n        training_args.gradient_accumulation_steps = 8\n        training_args.per_device_train_batch_size = 6\n        print(f\"New effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n        \n        trainer = Seq2SeqTrainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics,\n        )\n        \n        try:\n            trainer.train()\n            print(\"\\n✓ Training completed with adjusted parameters!\")\n        except RuntimeError as e2:\n            if \"out of memory\" in str(e2).lower():\n                print(\"\\n⚠️ Still OOM - RECOVERY ATTEMPT 2: Minimal config\")\n                torch.cuda.empty_cache()\n                gc.collect()\n                \n                training_args.gradient_accumulation_steps = 12\n                training_args.per_device_train_batch_size = 4\n                training_args.gradient_checkpointing = True\n                \n                trainer = Seq2SeqTrainer(\n                    model=model,\n                    args=training_args,\n                    train_dataset=tokenized_train,\n                    eval_dataset=tokenized_val,\n                    tokenizer=tokenizer,\n                    data_collator=data_collator,\n                    compute_metrics=compute_metrics,\n                )\n                \n                trainer.train()\n                print(\"\\n✓ Training completed with minimal memory footprint!\")\n            else:\n                raise e2\n    else:\n        raise e\n\nprint(\"\\nMarianMT model ready for validation and ensemble!\")","metadata":{"papermill":{"duration":71.647827,"end_time":"2025-12-25T11:19:14.224317","exception":false,"start_time":"2025-12-25T11:18:02.576490","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"69683d09","cell_type":"markdown","source":"# C7. Save Model","metadata":{"papermill":{"duration":0.006699,"end_time":"2025-12-25T11:19:14.237745","exception":false,"start_time":"2025-12-25T11:19:14.231046","status":"completed"},"tags":[]}},{"id":"ed53aaf2","cell_type":"code","source":"print(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Notebook C (MarianMT) Complete.\")","metadata":{"papermill":{"duration":0.71791,"end_time":"2025-12-25T11:19:14.962238","exception":false,"start_time":"2025-12-25T11:19:14.244328","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"237fa850","cell_type":"markdown","source":"## 🎯 NEXT STEPS: Advanced Strategies for MarianMT Score Improvement\n\nMarianMT is **pre-trained specifically for translation**, giving it unique advantages. The optimized configuration targets **strong translation quality** (geometric mean ~32-36). Push to **competition-winning levels (37+)** with these MarianMT-specific techniques:","metadata":{}},{"id":"4de81446","cell_type":"code","source":"# POST-TRAINING VALIDATION WITH ENHANCED METRICS\nprint(\"\\n\" + \"=\"*60)\nprint(\"POST-TRAINING VALIDATION - MARIANMT EVALUATION\")\nprint(\"=\"*60)\nprint(\"Computing metrics: BLEU, chrF++, and Geometric Mean\")\nprint(\"(Following Deep Past Challenge evaluation methodology)\")\nprint(\"=\"*60 + \"\\n\")\n\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_chrf = evaluate.load(\"chrf\")\n\ndef dedup_repeats(text: str) -> str:\n    \"\"\"Remove consecutive repeated tokens\"\"\"\n    toks = text.split()\n    out = []\n    for t in toks:\n        if len(out) >= 2 and t == out[-1] == out[-2]:\n            continue\n        out.append(t)\n    return \" \".join(out)\n\ndef postprocess_text(preds):\n    \"\"\"Enhanced postprocessing for better output quality\"\"\"\n    out = []\n    for p in preds:\n        p = p.strip()\n        # Fix spacing around punctuation\n        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n        # Remove repeated tokens\n        p = dedup_repeats(p)\n        # Capitalize first letter\n        if p and p[0].islower():\n            p = p[0].upper() + p[1:]\n        # Ensure sentence ends with punctuation\n        if p and p[-1] not in \".!?\":\n            p += \".\"\n        # Remove multiple punctuation\n        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n        out.append(p.strip())\n    return out\n\nval_texts = dataset[\"test\"][\"transliteration\"]\nval_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n\nprint(f\"Validating on {len(val_texts)} samples...\")\nprint(\"Using beam search with num_beams=8 for translation quality\\n\")\n\ndef generate_batch(texts, num_beams=8):\n    \"\"\"Enhanced generation with optimized parameters\"\"\"\n    batch_inputs = texts  # MarianMT doesn't need prefix\n    enc = tokenizer(\n        batch_inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=True, \n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    gen = model.generate(\n        **enc,\n        max_length=MAX_LENGTH,\n        min_length=10,                    # Longer minimum for translations\n        num_beams=num_beams,              # High beams for quality\n        no_repeat_ngram_size=3,           # Prevent repetition\n        length_penalty=1.2,               # Favor longer translations\n        early_stopping=True,\n        repetition_penalty=1.05,          # Gentle repetition penalty\n        do_sample=False,                  # Deterministic\n    )\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n# Generate predictions\npreds = []\nbatch_size = 10  # MarianMT handles larger batches well\nfor i in range(0, len(val_texts), batch_size):\n    batch_preds = generate_batch(val_texts[i:i+batch_size])\n    preds.extend(batch_preds)\n    if (i // batch_size + 1) % 10 == 0:\n        print(f\"  Progress: {i+batch_size}/{len(val_texts)} samples processed\")\n\npreds = postprocess_text(preds)\n\n# Compute all metrics\nprint(\"\\nComputing metrics...\")\nbleu_result = metric_bleu.compute(predictions=preds, references=val_refs)\nbleu_score = bleu_result['score']\n\nchrf_result = metric_chrf.compute(predictions=preds, references=val_refs, word_order=2)\nchrf_score = chrf_result['score']\n\n# Geometric mean (competition metric)\nimport math\ngeo_mean = math.sqrt(bleu_score * chrf_score)\n\n# Display results\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION RESULTS - MARIANMT MODEL\")\nprint(\"=\"*60)\nprint(f\"Model:              Helsinki-NLP/opus-mt-mul-en\")\nprint(f\"Samples evaluated:  {len(val_texts)}\")\nprint(f\"\")\nprint(f\"BLEU Score:         {bleu_score:7.2f}\")\nprint(f\"chrF++ Score:       {chrf_score:7.2f}\")\nprint(f\"\")\nprint(f\"🏆 GEOMETRIC MEAN:  {geo_mean:7.2f}  ← Challenge Metric\")\nprint(\"=\"*60)\n\n# Show sample predictions\nprint(\"\\n📊 SAMPLE PREDICTIONS (first 3):\")\nprint(\"=\"*60)\nfor i in range(min(3, len(val_texts))):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"  Source: {val_texts[i][:80]}...\")\n    print(f\"  Target: {val_refs[i][0][:80]}...\")\n    print(f\"  Prediction: {preds[i][:80]}...\")\nprint(\"=\"*60 + \"\\n\")\n\n# Score interpretation & comparison\nif geo_mean >= 35:\n    print(\"🌟 EXCELLENT! MarianMT achieving competition-winning level!\")\nelif geo_mean >= 30:\n    print(\"✨ GREAT! Strong translation quality, top quartile expected.\")\nelif geo_mean >= 25:\n    print(\"✓ GOOD! Solid performance, room for improvement.\")\nelse:\n    print(\"⚠️  Score needs improvement. Consider:\")\n    print(\"   • More training epochs (try 25-30)\")\n    print(\"   • Data augmentation with back-translation\")\n    print(\"   • Curriculum learning strategies\")\n\nprint(\"\\n💡 NEXT STEPS:\")\nprint(\"   1. Compare scores across ByT5, T5, and MarianMT\")\nprint(\"   2. Use best-performing models in ensemble\")\nprint(\"   3. Adjust ensemble weights based on validation scores\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION COMPLETE - MARIANMT READY FOR ENSEMBLE\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"df987458","cell_type":"code","source":"\"\"\"\nMARIANMT-SPECIFIC ADVANCED STRATEGIES\n======================================\n\nMarianMT (Helsinki-NLP/opus-mt-mul-en) is pre-trained on 1000+ language pairs.\nLeverage its translation-specific architecture for Akkadian:\n\n1. LANGUAGE CODE OPTIMIZATION\n   ─────────────────────────\n   MarianMT uses language tags. Test different source language hints:\n   \n   Options:\n   • >>eng<< prefix (target language hint)\n   • >>akk<< or >>sem<< (Semitic language family hint)\n   • No prefix (let model infer)\n   \n   Implementation:\n   ```\n   # Test different language codes\n   PREFIXES = [\n       \">>eng<<\",           # Target: English\n       \">>akk<< >>eng<<\",   # Source: Akkadian, Target: English\n       \">>sem<< >>eng<<\",   # Source: Semitic, Target: English\n       \"\",                  # No hint\n   ]\n   \n   best_score = 0\n   best_prefix = \"\"\n   \n   for prefix in PREFIXES:\n       # Tokenize with prefix\n       inputs = [f\"{prefix} {text}\" for text in training_texts]\n       # Train and evaluate\n       score = validate()\n       if score > best_score:\n           best_score = score\n           best_prefix = prefix\n   ```\n\n2. BACK-TRANSLATION FOR TRANSLATION MODELS\n   ────────────────────────────────────────\n   MarianMT excels with back-translation (more than other models):\n   \n   Implementation:\n   ```\n   # Step 1: Train English→Akkadian reverse model\n   reverse_model = AutoModelForSeq2SeqLM.from_pretrained(\n       \"Helsinki-NLP/opus-mt-en-mul\"\n   )\n   # Fine-tune on reversed pairs (English→Akkadian)\n   \n   # Step 2: Generate synthetic Akkadian from English monolingual data\n   english_monolingual = [...]  # Additional English texts\n   synthetic_akkadian = [reverse_model.generate(text) for text in english_monolingual]\n   \n   # Step 3: Augment training data\n   augmented_pairs = list(zip(synthetic_akkadian, english_monolingual))\n   combined_data = original_pairs + augmented_pairs\n   \n   # Step 4: Re-train forward model on augmented data\n   ```\n\n3. OPUS CORPUS PRE-TRAINING\n   ────────────────────────\n   Further pre-train MarianMT on related language pairs:\n   • Ancient Greek → English (similar ancient language)\n   • Hebrew → English (Semitic language family)\n   • Arabic → English (Semitic, similar morphology)\n   \n   Implementation:\n   ```\n   from datasets import load_dataset\n   \n   # Load related language pairs from OPUS\n   related_corpus = load_dataset(\"opus_books\", \"he-en\")  # Hebrew-English\n   \n   # Pre-train on related languages (few epochs)\n   trainer = Seq2SeqTrainer(\n       model=model,\n       train_dataset=related_corpus['train'],\n       args=Seq2SeqTrainingArguments(\n           num_train_epochs=2,  # Just 2-3 epochs\n           learning_rate=1e-5,  # Low LR for pre-training\n           ...\n       )\n   )\n   trainer.train()\n   \n   # Then fine-tune on Akkadian (main training)\n   ```\n\n4. TRANSLATION-SPECIFIC BEAM SEARCH\n   ────────────────────────────────\n   MarianMT benefits from translation-tuned generation:\n   • Higher beam width (10-12 instead of 8)\n   • Length penalty tuning (1.0-1.5)\n   • No repeat n-gram size (3-4)\n   \n   Implementation:\n   ```\n   # Hyperparameter search for beam settings\n   configs = [\n       {'num_beams': 10, 'length_penalty': 1.0},\n       {'num_beams': 12, 'length_penalty': 1.2},\n       {'num_beams': 10, 'length_penalty': 1.5},\n       {'num_beams': 8, 'length_penalty': 1.3},\n   ]\n   \n   best_config = None\n   best_score = 0\n   \n   for config in configs:\n       preds = model.generate(**config, no_repeat_ngram_size=4)\n       score = compute_geometric_mean(preds, references)\n       if score > best_score:\n           best_score = score\n           best_config = config\n   ```\n\n5. MULTILINGUAL TRANSFER LEARNING\n   ───────────────────────────────\n   Use MarianMT's multilingual knowledge:\n   • Train on multiple ancient languages simultaneously\n   • Add Latin, Ancient Greek as auxiliary tasks\n   \n   Implementation:\n   ```\n   # Mix Akkadian with related ancient languages\n   training_data = {\n       'akkadian': akkadian_pairs,\n       'latin': latin_english_pairs,      # If available\n       'greek': greek_english_pairs,      # Ancient Greek\n   }\n   \n   mixed_dataset = []\n   for lang, pairs in training_data.items():\n       for src, tgt in pairs:\n           mixed_dataset.append({\n               'source': f'>>{lang[:2]}<< {src}',  # Language hint\n               'target': tgt\n           })\n   \n   # Train on mixed data\n   ```\n\n6. DOMAIN ADAPTATION VIA CORPUS FILTERING\n   ───────────────────────────────────────\n   MarianMT trained on modern text; adapt to ancient domain:\n   \n   Implementation:\n   ```\n   from sentence_transformers import SentenceTransformer, util\n   \n   # Get domain-specific corpus\n   ancient_corpus = [...]  # Ancient text samples\n   encoder = SentenceTransformer('all-MiniLM-L6-v2')\n   \n   # Filter OPUS data for ancient-like texts\n   opus_data = load_dataset(\"opus100\", \"en\")\n   domain_embeddings = encoder.encode(ancient_corpus)\n   \n   def is_domain_relevant(text, threshold=0.3):\n       text_emb = encoder.encode([text])\n       similarity = util.cos_sim(text_emb, domain_embeddings).max()\n       return similarity > threshold\n   \n   # Keep only domain-relevant pre-training data\n   filtered_opus = opus_data.filter(\n       lambda x: is_domain_relevant(x['translation']['en'])\n   )\n   ```\n\n7. KNOWLEDGE DISTILLATION FROM LARGER MODELS\n   ──────────────────────────────────────────\n   Use GPT-4 or larger translation models to create better labels:\n   \n   Implementation:\n   ```\n   # Generate high-quality pseudo-labels with GPT-4\n   from openai import OpenAI\n   \n   client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n   \n   def gpt4_translate(akkadian_text):\n       response = client.chat.completions.create(\n           model=\"gpt-4\",\n           messages=[\n               {\"role\": \"system\", \"content\": \"Translate Old Assyrian Akkadian to English.\"},\n               {\"role\": \"user\", \"content\": akkadian_text}\n           ]\n       )\n       return response.choices[0].message.content\n   \n   # Generate teacher labels for unlabeled data\n   teacher_labels = [gpt4_translate(text) for text in unlabeled_texts]\n   \n   # Train MarianMT (student) on teacher labels\n   distillation_data = list(zip(unlabeled_texts, teacher_labels))\n   ```\n\nMARIANMT SCORING TARGETS\n─────────────────────────\nBaseline (current config): ~32-35 geometric mean\nWith language code optimization: ~34-36\nWith back-translation: ~36-38\nWith domain adaptation + distillation: ~38-40 (top tier!)\n\nRECOMMENDED PRIORITY FOR MARIANMT\n──────────────────────────────────\n1. Optimize language codes/prefixes (quick, big impact)\n2. Implement back-translation pipeline (proven for MT)\n3. Tune beam search hyperparameters (easy wins)\n4. Knowledge distillation from GPT-4 (if budget allows)\n\nMARIANMT UNIQUE STRENGTHS\n──────────────────────────\n✓ Pre-trained on 1000+ language pairs (best generalization)\n✓ Optimized for translation quality (not just sequence-to-sequence)\n✓ Handles multilingual inputs naturally (language code system)\n✓ Smaller model = faster training/inference\n\nENSEMBLE SYNERGY\n────────────────\nMarianMT often produces different errors than ByT5/T5:\n• ByT5: Good at handling rare characters, gaps\n• T5: Good at structured tasks, prefixes\n• MarianMT: Good at fluent, grammatical English\n\nCombined in ensemble → Coverage of all aspects → Higher geometric mean!\n\nFINAL TIP: Monitor BOTH BLEU and chrF++ During Training\n────────────────────────────────────────────────────────\nMarianMT sometimes over-optimizes for fluency (BLEU) at cost of character accuracy (chrF++).\nEnsure balanced improvement by checking geometric mean, not just BLEU.\n\"\"\"\n\nprint(\"=\"*60)\nprint(\"📚 MARIANMT ADVANCED STRATEGIES LOADED\")\nprint(\"=\"*60)\nprint(\"Key advantages: Translation-specific, multilingual, language codes\")\nprint(\"Target: 34-38+ geometric mean with optimizations\")\nprint(\"Best in ensemble with ByT5 and T5!\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9d194cec","cell_type":"markdown","source":"## 🎯 NEXT STEPS: Advanced Strategies for MarianMT Score Improvement\n\nMarianMT is pre-trained for translation (baseline geometric mean ~32–36). Push to 37+ with:\n\n- Language codes: test >>eng<<, >>akk<< >>eng<<, Semitic family hints, or no prefix.\n- Back-translation: train reverse model (English→Akkadian) and augment forward data.\n- Related-language pre-training: Hebrew/Arabic/Ancient Greek → English (OPUS corpora).\n- Beam tuning: search num_beams=10–12 and length penalty via generation params.\n- Multilingual transfer: mix auxiliary ancient languages.\n- Domain adaptation: filter pre-training corpora toward ancient-domain similarity.","metadata":{}},{"id":"ca8e3fd2","cell_type":"code","source":"# Extend training and generation parameters (safe toggles)\ntraining_args.num_train_epochs = max(getattr(training_args, \"num_train_epochs\", 22), 24)\ntraining_args.lr_scheduler_type = \"cosine_with_restarts\"\ntraining_args.warmup_ratio = 0.08\ntraining_args.weight_decay = 0.01\ntraining_args.generation_num_beams = max(getattr(training_args, \"generation_num_beams\", 1), 10)\n\nprint(\"Next steps applied: epochs>=24, cosine restarts, beams>=10.\")\nprint(\"Evaluate language code sweeps, back-translation, beam search tuning.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7509bf04","cell_type":"markdown","source":"## 🔗 Sentence-Level Alignment with published_texts.csv\n\nGoal: Align mined English sentences from `mined_publications_en.csv` to Akkadian transliterations in `published_texts.csv` by matching catalog labels and aliases.\n\nApproach:\n- Load `published_texts.csv` (≈8k rows) and `mined_publications_en.csv`.\n- Extract catalog-like refs (e.g., BIN VI 39, Kt 72/k, museum IDs) from each English sentence.\n- Fuzzy-match refs to `publication_catalog` or `aliases` in `published_texts.csv` using RapidFuzz.\n- Emit candidate parallel pairs to `aligned_pairs_candidates.csv` for manual review or automatic filtering.","metadata":{}},{"id":"1c47a55e","cell_type":"code","source":"# Align mined English sentences to transliterations via catalog/alias fuzzy matching\n!pip install -q rapidfuzz ftfy unidecode\n\nimport os\nimport re\nimport csv\nfrom pathlib import Path\nimport pandas as pd\nfrom rapidfuzz import fuzz, process\nfrom ftfy import fix_text\nfrom unidecode import unidecode\n\nPUBLISHED_TEXTS_PATH = os.getenv('PUBLISHED_TEXTS_CSV', 'published_texts.csv')\nMINED_EN_PATH = os.getenv('MINED_PUBLICATIONS_OUT', 'mined_publications_en.csv')\nALIGNED_OUT_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\n\n# Heuristic patterns for publication labels and catalog IDs (expandable)\nCATALOG_PATTERNS = [\n    r\"\\bBIN\\s+[IVXLCDM]+\\s*\\d+\\b\",        # e.g., BIN VI 39\n    r\"\\bKt\\.?\\s*\\d+/?[A-Za-z0-9-]*\\b\",     # e.g., Kt 72/k\n    r\"\\bBM\\s*\\d+[A-Za-z]?\\b\",              # British Museum IDs\n    r\"\\bYBC\\s*\\d+\\b\",                      # Yale Babylonian Collection\n    r\"\\b(AbB|AKT|CCT|KBo|KUB)\\s*\\d+[A-Za-z0-9-]*\\b\",  # Common series\n]\n\n\ndef extract_catalog_refs(text: str) -> list:\n    if not isinstance(text, str):\n        return []\n    text = fix_text(text)\n    text = unidecode(text)\n    refs = set()\n    for pat in CATALOG_PATTERNS:\n        for m in re.finditer(pat, text, flags=re.IGNORECASE):\n            ref = m.group(0).strip()\n            # Normalize spaces and punctuation\n            ref = re.sub(r\"\\s+\", \" \", ref)\n            refs.add(ref)\n    return list(refs)\n\n\ndef build_alias_index(df: pd.DataFrame):\n    \"\"\"Build a search index over publication_catalog and aliases fields.\"\"\"\n    index_records = []\n    for i, row in df.iterrows():\n        rid = i\n        label = str(row.get('label', '') or '')\n        pubcat = str(row.get('publication_catalog', '') or '')\n        aliases = str(row.get('aliases', '') or '')\n        # Split on bars and commas for multiple entries\n        tokens = []\n        for field in (pubcat, aliases, label):\n            parts = re.split(r\"[|,;]\", field)\n            for p in parts:\n                p = unidecode(p.strip())\n                if p:\n                    tokens.append(p)\n        # Keep unique tokens\n        tokens = list(dict.fromkeys(tokens))\n        index_records.append({\n            'rid': rid,\n            'tokens': tokens,\n        })\n    return index_records\n\n\ndef find_matches(refs: list, index_records: list, score_cutoff: int = 85):\n    \"\"\"For each ref, fuzzy-match against index tokens and return candidate row indices.\"\"\"\n    candidates = set()\n    for ref in refs:\n        for rec in index_records:\n            # Use token_set_ratio for forgiving matching\n            for tok in rec['tokens']:\n                score = fuzz.token_set_ratio(ref, tok)\n                if score >= score_cutoff:\n                    candidates.add(rec['rid'])\n                    break\n    return list(candidates)\n\n\ndef align_sentences(mined_path: str, published_path: str, out_path: str):\n    # Load published texts\n    pub_df = pd.read_csv(published_path)\n    # Defensive: ensure needed columns exist\n    for col in ['transliteration', 'publication_catalog', 'aliases', 'label']:\n        if col not in pub_df.columns:\n            pub_df[col] = ''\n    # Build alias index\n    alias_index = build_alias_index(pub_df)\n\n    # Prepare output\n    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n    written = 0\n    total = 0\n\n    with open(out_path, 'w', newline='', encoding='utf-8') as f_out:\n        writer = csv.writer(f_out)\n        writer.writerow(['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration'])\n\n        # Stream mined sentences to keep memory low\n        for chunk in pd.read_csv(mined_path, chunksize=5000):\n            for _, row in chunk.iterrows():\n                total += 1\n                pdf = str(row.get('pdf_name', '') or '')\n                page = int(row.get('page', -1)) if pd.notna(row.get('page')) else -1\n                sent = str(row.get('english_sentence', '') or '')\n                if not sent:\n                    continue\n                refs = extract_catalog_refs(sent)\n                if not refs:\n                    continue  # No catalog hint; skip for now\n                # Find candidate rows\n                cand_ids = find_matches(refs, alias_index, score_cutoff=85)\n                for rid in cand_ids:\n                    t_row = pub_df.iloc[rid]\n                    matched_label = str(t_row.get('label', '') or '')\n                    translit = str(t_row.get('transliteration', '') or '')\n                    if translit:\n                        writer.writerow([pdf, page, sent, matched_label, translit])\n                        written += 1\n            if total % 10000 == 0:\n                print(f\"Processed {total} sentences; wrote {written} candidate pairs...\")\n\n    print(f\"Alignment complete. Total sentences: {total}, candidates written: {written}\")\n    print(f\"Saved to: {out_path}\")\n\n\nprint(\"Starting alignment: mined_publications_en.csv → published_texts.csv (catalog/alias matching)\")\nalign_sentences(MINED_EN_PATH, PUBLISHED_TEXTS_PATH, ALIGNED_OUT_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a37f8d93","cell_type":"markdown","source":"## ✅ Quality Filter & Summary\n\n**⚠️ PREREQUISITE: Run the alignment cell above first to generate `aligned_pairs_candidates.csv`.**\n\nFilter aligned pairs for training quality:\n- Remove pairs where transliteration or English is too short/long\n- Discard pairs with extreme length ratios (likely misaligned)\n- Keep pairs with domain terms or high lexicon match\n- Sample results for sanity check\n- Output: `aligned_pairs_filtered.csv` ready for training augmentation","metadata":{}},{"id":"bdaba540","cell_type":"code","source":"import pandas as pd\nimport os\n\nALIGNED_PATH = os.getenv('ALIGNED_PAIRS_OUT', 'aligned_pairs_candidates.csv')\nFILTERED_OUT_PATH = os.getenv('FILTERED_PAIRS_OUT', 'aligned_pairs_filtered.csv')\n\ndef filter_quality(aligned_path: str, out_path: str):\n    \"\"\"Filter aligned pairs for training quality.\"\"\"\n    df = pd.read_csv(aligned_path)\n    print(f\"Loaded {len(df)} candidate pairs\")\n    \n    # Length filters\n    df['t_len'] = df['transliteration'].str.split().str.len()\n    df['e_len'] = df['english_sentence'].str.split().str.len()\n    \n    # Apply filters\n    df_filtered = df[\n        (df['t_len'] >= 3) & (df['t_len'] <= 150) &\n        (df['e_len'] >= 3) & (df['e_len'] <= 150) &\n        (df['t_len'] / (df['e_len'] + 1) >= 0.5) &\n        (df['t_len'] / (df['e_len'] + 1) <= 3.0)\n    ].copy()\n    \n    domain_terms = ['tablet', 'seal', 'silver', 'tin', 'letter', 'text', 'archive', 'merchant', 'trade']\n    df_filtered['has_domain'] = df_filtered['english_sentence'].str.lower().str.contains('|'.join(domain_terms), na=False)\n    \n    df_filtered[['pdf_name', 'page', 'english_sentence', 'matched_label', 'transliteration']].to_csv(out_path, index=False)\n    \n    print(f\"After quality filtering: {len(df_filtered)} pairs retained\")\n    print(f\"Saved to: {out_path}\\n\")\n    \n    print(\"Sample aligned pairs (first 5):\")\n    for i, row in df_filtered.head(5).iterrows():\n        print(f\"\\n[{i}]\")\n        print(f\"  EN: {row['english_sentence'][:80]}...\")\n        print(f\"  AK: {row['transliteration'][:80]}...\")\n    \n    return len(df_filtered)\n\ncount = filter_quality(ALIGNED_PATH, FILTERED_OUT_PATH)\nprint(f\"\\n✓ Quality filtering complete. {count} high-quality pairs ready for training augmentation.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}