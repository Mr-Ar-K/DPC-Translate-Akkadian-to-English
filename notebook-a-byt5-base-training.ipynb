{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a4c494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:08.388312Z",
     "iopub.status.busy": "2026-01-08T08:08:08.387988Z",
     "iopub.status.idle": "2026-01-08T08:08:08.619248Z",
     "shell.execute_reply": "2026-01-08T08:08:08.616178Z",
     "shell.execute_reply.started": "2026-01-08T08:08:08.388289Z"
    },
    "papermill": {
     "duration": 0.184686,
     "end_time": "2025-12-25T10:28:15.031482",
     "exception": false,
     "start_time": "2025-12-25T10:28:14.846796",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  8 08:08:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853653de",
   "metadata": {
    "papermill": {
     "duration": 0.006431,
     "end_time": "2025-12-25T10:28:15.044668",
     "exception": false,
     "start_time": "2025-12-25T10:28:15.038237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964c2326",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:08.621873Z",
     "iopub.status.busy": "2026-01-08T08:08:08.621507Z",
     "iopub.status.idle": "2026-01-08T08:08:13.845850Z",
     "shell.execute_reply": "2026-01-08T08:08:13.844989Z",
     "shell.execute_reply.started": "2026-01-08T08:08:08.621830Z"
    },
    "papermill": {
     "duration": 4.824313,
     "end_time": "2025-12-25T10:28:19.875330",
     "exception": false,
     "start_time": "2025-12-25T10:28:15.051017",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5f368",
   "metadata": {
    "papermill": {
     "duration": 0.006597,
     "end_time": "2025-12-25T10:28:19.889807",
     "exception": false,
     "start_time": "2025-12-25T10:28:19.883210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A2. Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec80cf0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:13.847573Z",
     "iopub.status.busy": "2026-01-08T08:08:13.847199Z",
     "iopub.status.idle": "2026-01-08T08:08:45.115144Z",
     "shell.execute_reply": "2026-01-08T08:08:45.114538Z",
     "shell.execute_reply.started": "2026-01-08T08:08:13.847522Z"
    },
    "papermill": {
     "duration": 33.038891,
     "end_time": "2025-12-25T10:28:52.935206",
     "exception": false,
     "start_time": "2025-12-25T10:28:19.896315",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 08:08:28.427385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767859708.621266      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767859708.681230      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767859709.163048      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767859709.163087      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767859709.163090      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767859709.163093      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Memory/precision safety tweaks (helps avoid OOM on P100/T4)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e3a6c",
   "metadata": {
    "papermill": {
     "duration": 0.006759,
     "end_time": "2025-12-25T10:28:52.949032",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.942273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A3. Set constants (DO NOT change yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9780c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:45.116661Z",
     "iopub.status.busy": "2026-01-08T08:08:45.116044Z",
     "iopub.status.idle": "2026-01-08T08:08:45.121485Z",
     "shell.execute_reply": "2026-01-08T08:08:45.120695Z",
     "shell.execute_reply.started": "2026-01-08T08:08:45.116634Z"
    },
    "papermill": {
     "duration": 0.013576,
     "end_time": "2025-12-25T10:28:52.969057",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.955481",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "OUTPUT_DIR = \"/kaggle/working/byt5-base-saved\"\n",
    "\n",
    "# ByT5 is character-based. 256 balances coverage with memory efficiency\n",
    "MAX_LENGTH = 256\n",
    "PREFIX = \"translate Akkadian to English: \"\n",
    "\n",
    "# OOM guard: allow dynamic reduction controlled by env var\n",
    "try:\n",
    "    env_max_len = int(os.getenv(\"BYT5_MAX_LENGTH\", \"0\"))\n",
    "    if env_max_len >= 200:\n",
    "        MAX_LENGTH = env_max_len\n",
    "        print(f\"[INFO] MAX_LENGTH overridden by env: {MAX_LENGTH}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041d778",
   "metadata": {
    "papermill": {
     "duration": 0.006442,
     "end_time": "2025-12-25T10:28:52.982062",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.975620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A4. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035f86f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:08:45.123948Z",
     "iopub.status.busy": "2026-01-08T08:08:45.123665Z",
     "iopub.status.idle": "2026-01-08T08:09:40.314471Z",
     "shell.execute_reply": "2026-01-08T08:09:40.313446Z",
     "shell.execute_reply.started": "2026-01-08T08:08:45.123915Z"
    },
    "papermill": {
     "duration": 0.469755,
     "end_time": "2025-12-25T10:28:53.458249",
     "exception": false,
     "start_time": "2025-12-25T10:28:52.988494",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw documents: 1561\n",
      "✓ Loading sentence alignment data...\n",
      "Building aligned dataset using sentence map translations...\n",
      "✓ Extracted 51 sentence pairs from map file\n",
      "Aligned training examples (pre-filter): 1561\n",
      "Aligned training examples (post-filter): 1528\n",
      "\n",
      "============================================================\n",
      "MINING PUBLICATIONS FOR ADDITIONAL TRAINING DATA (FAST MODE)\n",
      "============================================================\n",
      "Total publication pages: 216602\n",
      "Pages with translation keywords: 12500\n",
      "Searching for matches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc564c056d4d4614a826ef142aac4f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mining:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  No additional pairs extracted (try adjusting regex or increasing candidates)\n",
      "\n",
      "============================================================\n",
      "CHECKING PUBLISHED TEXTS\n",
      "============================================================\n",
      "Published texts available: 7953\n",
      "Note: Will use these for monolingual pre-training\n",
      "\n",
      "Final dataset:\n",
      "  Train: 1451 examples\n",
      "  Validation: 77 examples\n"
     ]
    }
   ],
   "source": [
    "SUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n",
    "\n",
    "def normalize_subscripts(text: str) -> str:\n",
    "    return text.translate(SUBSCRIPT_TRANS)\n",
    "\n",
    "def replace_gaps(text, keep_gaps=True):\n",
    "    \"\"\"Replace various gap notations with standardized tokens\n",
    "    \n",
    "    Args:\n",
    "        keep_gaps: If True, keeps gap tokens (for test-like data).\n",
    "                   If False, removes them (for clean training).\n",
    "    \"\"\"\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    \n",
    "    # Complex gap patterns (order matters)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    # Simple gap patterns\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "    text = re.sub(r'……', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "    text = re.sub(r'…', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "    \n",
    "    # Bracketed gaps\n",
    "    text = re.sub(r'\\[\\.\\.\\.+\\]', '<big_gap>', text)\n",
    "    text = re.sub(r'\\[x+\\]', '<gap>', text)\n",
    "    \n",
    "    if not keep_gaps:\n",
    "        # Remove gaps for clean training\n",
    "        text = re.sub(r'<big_gap>', '', text)\n",
    "        text = re.sub(r'<gap>', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_translit(text, keep_gaps=True):\n",
    "    \"\"\"Normalize transliteration following competition guidance.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = normalize_subscripts(text)\n",
    "    # Apply gap replacement - KEEP gaps for domain matching\n",
    "    text = replace_gaps(text, keep_gaps=keep_gaps)\n",
    "    # Only remove scribal markers, keep gaps\n",
    "    text = re.sub(r\"<<[^>]*>>\", \" \", text)               # errant signs\n",
    "    text = re.sub(r\"[˹˺]\", \" \", text)                    # half brackets\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)             # comments/erasures\n",
    "    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)         # determinatives\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)            # scribal insertions keep content\n",
    "    text = re.sub(r\"[!?/:·]\", \" \", text)                 # scribal punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text, has_gaps=False):\n",
    "    \"\"\"Clean translation, optionally keeping gap indicators\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    if not has_gaps:\n",
    "        text = text.replace(\"…\", \" \")\n",
    "    # Keep ... if source has gaps\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def filter_quality(df):\n",
    "    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n",
    "    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n",
    "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
    "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
    "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
    "    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n",
    "    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ADVANCED DATA ALIGNMENT (Using Sentences_Oare_FirstWord_LinNum.csv)\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_sentence_alignment():\n",
    "    \"\"\"Load sentence alignment data if available\"\"\"\n",
    "    sent_align_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n",
    "    if os.path.exists(sent_align_path):\n",
    "        print(\"✓ Loading sentence alignment data...\")\n",
    "        return pd.read_csv(sent_align_path)\n",
    "    else:\n",
    "        print(\"⚠️  Sentence alignment file not found, using fallback alignment\")\n",
    "        return None\n",
    "\n",
    "def align_with_sentence_map(df, sent_map):\n",
    "    \"\"\"Use explicit sentence mapping for perfect alignment\n",
    "    \n",
    "    This function uses the Sentences_Oare_FirstWord_LinNum.csv file which contains:\n",
    "    - text_uuid: Document ID (matches oare_id in train.csv)\n",
    "    - translation: The English sentence (THIS IS KEY - use it directly!)\n",
    "    - first_word_transcription: First word of Akkadian sentence\n",
    "    - sentence_obj_in_text: Sentence order within document\n",
    "    \"\"\"\n",
    "    if sent_map is None:\n",
    "        return None\n",
    "    \n",
    "    print(\"Building aligned dataset using sentence map translations...\")\n",
    "    aligned_rows = []\n",
    "    \n",
    "    # Create lookup for full transliterations from train.csv\n",
    "    df['clean_translit_full'] = df['transliteration'].apply(lambda x: clean_translit(str(x), keep_gaps=True))\n",
    "    text_lookup = df.set_index('oare_id')['clean_translit_full'].to_dict()\n",
    "    \n",
    "    # Group by document\n",
    "    for text_id, group in sent_map.groupby('text_uuid'):\n",
    "        if text_id not in text_lookup:\n",
    "            continue\n",
    "        \n",
    "        full_akkadian = text_lookup[text_id]\n",
    "        if len(full_akkadian) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Sort sentences by their order in the text\n",
    "        sorted_sents = group.sort_values('sentence_obj_in_text')\n",
    "        \n",
    "        # Extract translations from the map (these are already sentence-aligned!)\n",
    "        map_translations = [str(row.translation).strip() for _, row in sorted_sents.iterrows()]\n",
    "        \n",
    "        # Try to split the Akkadian text into matching chunks\n",
    "        # Strategy: Use gaps and newlines as natural boundaries\n",
    "        akkadian_chunks = [s.strip() for s in re.split(r'(?:<big_gap>|<gap>|\\n)+', full_akkadian) if len(s.strip()) > 3]\n",
    "        \n",
    "        # If chunk counts match, pair them up (high confidence)\n",
    "        if len(akkadian_chunks) == len(map_translations):\n",
    "            for akk_chunk, eng_sent in zip(akkadian_chunks, map_translations):\n",
    "                if len(akk_chunk) > 3 and len(eng_sent) > 3:\n",
    "                    aligned_rows.append({\n",
    "                        \"transliteration\": akk_chunk,\n",
    "                        \"translation\": clean_translation(eng_sent)\n",
    "                    })\n",
    "        else:\n",
    "            # Fallback: If counts don't match, still use map translations\n",
    "            # but try to extract Akkadian by first-word matching\n",
    "            for _, sent_row in sorted_sents.iterrows():\n",
    "                first_word = str(sent_row.first_word_transcription).strip() if hasattr(sent_row, 'first_word_transcription') else \"\"\n",
    "                eng_sent = str(sent_row.translation).strip()\n",
    "                \n",
    "                if len(first_word) > 2 and first_word in full_akkadian:\n",
    "                    # Find sentence starting with this word (heuristic)\n",
    "                    start_pos = full_akkadian.find(first_word)\n",
    "                    if start_pos >= 0:\n",
    "                        # Extract until next gap or reasonable length\n",
    "                        remaining = full_akkadian[start_pos:start_pos+200]\n",
    "                        end_pos = re.search(r'<big_gap>|<gap>|\\n', remaining)\n",
    "                        akk_sent = remaining[:end_pos.start()] if end_pos else remaining\n",
    "                        \n",
    "                        if len(akk_sent) > 5 and len(eng_sent) > 3:\n",
    "                            aligned_rows.append({\n",
    "                                \"transliteration\": akk_sent.strip(),\n",
    "                                \"translation\": clean_translation(eng_sent)\n",
    "                            })\n",
    "    \n",
    "    if aligned_rows:\n",
    "        aligned_df = pd.DataFrame(aligned_rows)\n",
    "        print(f\"✓ Extracted {len(aligned_df)} sentence pairs from map file\")\n",
    "        return aligned_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_and_align_data(filepath):\n",
    "    \"\"\"\n",
    "    Enhanced alignment with sentence-level mapping support\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Raw documents: {len(df)}\")\n",
    "    \n",
    "    # Try to use sentence alignment map first\n",
    "    sent_map = load_sentence_alignment()\n",
    "    if sent_map is not None:\n",
    "        aligned_df = align_with_sentence_map(df, sent_map)\n",
    "        if aligned_df is not None and len(aligned_df) > 100:\n",
    "            print(f\"✓ Aligned using sentence map: {len(aligned_df)} examples\")\n",
    "            return filter_quality(aligned_df)\n",
    "    \n",
    "    # Fallback: Original alignment logic\n",
    "    aligned_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        src = clean_translit(row.get(\"transliteration\", \"\"), keep_gaps=True)\n",
    "        tgt = clean_translation(row.get(\"translation\", \"\"))\n",
    "\n",
    "        src_lines = [s.strip() for s in src.split(\"\\n\") if s.strip()]\n",
    "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n",
    "\n",
    "        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n",
    "            for s, t in zip(src_lines, tgt_sents):\n",
    "                if len(s) > 3 and len(t) > 3:\n",
    "                    aligned_rows.append({\"transliteration\": s, \"translation\": t})\n",
    "        else:\n",
    "            merged_src = src.replace(\"\\n\", \" \")\n",
    "            if len(merged_src) > 3 and len(tgt) > 3:\n",
    "                aligned_rows.append({\"transliteration\": merged_src, \"translation\": tgt})\n",
    "\n",
    "    print(f\"Aligned training examples (pre-filter): {len(aligned_rows)}\")\n",
    "    out_df = filter_quality(pd.DataFrame(aligned_rows))\n",
    "    print(f\"Aligned training examples (post-filter): {len(out_df)}\")\n",
    "    return out_df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MINE PUBLICATIONS.CSV FOR ADDITIONAL TRAINING DATA (OPTIMIZED)\n",
    "# -----------------------------------------------------------------------------\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def mine_publications_data():\n",
    "    print(\"\\n=== MINING PUBLICATIONS (BROAD MODE) ===\")\n",
    "    pub_path = f\"{DATA_DIR}/publications.csv\"\n",
    "    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    \n",
    "    # 1. Load Data\n",
    "    if not os.path.exists(pub_path): return pd.DataFrame()\n",
    "    pubs = pd.read_csv(pub_path)\n",
    "    pub_texts = pd.read_csv(pub_texts_path)\n",
    "    \n",
    "    # 2. Filter Publications (Keep only pages with English text)\n",
    "    # We check for common English stop words to ensure the page is in English\n",
    "    eng_mask = pubs['page_text'].astype(str).str.contains(r'\\b(the|and|of|in|to)\\b', case=False)\n",
    "    pubs = pubs[eng_mask].copy()\n",
    "    print(f\"Searching {len(pubs)} English publication pages...\")\n",
    "    \n",
    "    augmented_data = []\n",
    "    \n",
    "    # 3. Search for Text IDs\n",
    "    # We focus on the CDLI ID (e.g., P361099) because it's unique and easy to find\n",
    "    candidates = pub_texts.dropna(subset=['cdli_id']).head(3000)\n",
    "    \n",
    "    for _, row in tqdm(candidates.iterrows(), total=len(candidates)):\n",
    "        cdli_id = str(row['cdli_id']).split('|')[0].strip() # Take first ID\n",
    "        if len(cdli_id) < 4: continue\n",
    "        \n",
    "        # Fast search: Does this Page contain the CDLI ID?\n",
    "        matches = pubs[pubs['page_text'].astype(str).str.contains(cdli_id, regex=False)]\n",
    "        \n",
    "        for _, page in matches.head(1).iterrows():\n",
    "            content = str(page['page_text'])\n",
    "            # Find the ID in the text\n",
    "            idx = content.find(cdli_id)\n",
    "            if idx == -1: continue\n",
    "            \n",
    "            # Look at the text IMMEDIATELY after the ID (next 500 chars)\n",
    "            # This is where the translation usually is\n",
    "            snippet = content[idx:idx+800]\n",
    "            \n",
    "            # Extract sentences that look like English (start with Capital, end with dot)\n",
    "            # and differ from the ID itself.\n",
    "            potential_trans = re.findall(r'([A-Z][a-z\\s\\-,;]{20,200}[\\.\\!\\?])', snippet)\n",
    "            \n",
    "            for sent in potential_trans:\n",
    "                # Basic filter: Must have spaces, shouldn't be too short\n",
    "                if len(sent.split()) > 4 and \"Assyrian\" not in sent:\n",
    "                    augmented_data.append({\n",
    "                        \"transliteration\": clean_translit(str(row['transliteration'])),\n",
    "                        \"translation\": sent.strip()\n",
    "                    })\n",
    "                    break # Take only the first valid sentence found\n",
    "    \n",
    "    result_df = pd.DataFrame(augmented_data)\n",
    "    if not result_df.empty:\n",
    "        result_df = result_df.drop_duplicates(subset=['transliteration'])\n",
    "        print(f\"✓ Success! Mined {len(result_df)} pairs.\")\n",
    "        return result_df\n",
    "    \n",
    "    print(\"⚠️ Still found 0 pairs. Using fallback data only.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Load main training data\n",
    "train_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n",
    "\n",
    "# Mine publications for additional translations\n",
    "mined_df = mine_publications_data()\n",
    "\n",
    "# Augment with mined data if available\n",
    "if len(mined_df) > 0:\n",
    "    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n",
    "    train_df = train_df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n",
    "    print(f\"\\n✓ Total training examples after augmentation: {len(train_df)}\")\n",
    "\n",
    "# Check published texts availability for later use\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING PUBLISHED TEXTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "if os.path.exists(pub_texts_path):\n",
    "    pub_df = pd.read_csv(pub_texts_path)\n",
    "    print(f\"Published texts available: {len(pub_df)}\")\n",
    "    print(\"Note: Will use these for monolingual pre-training\")\n",
    "else:\n",
    "    print(\"⚠️  Published texts not found\")\n",
    "\n",
    "# Create dataset and split\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Train: {len(dataset['train'])} examples\")\n",
    "print(f\"  Validation: {len(dataset['test'])} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e0f72",
   "metadata": {
    "papermill": {
     "duration": 0.006698,
     "end_time": "2025-12-25T10:28:53.471962",
     "exception": false,
     "start_time": "2025-12-25T10:28:53.465264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A5 . Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1a0a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:09:40.315996Z",
     "iopub.status.busy": "2026-01-08T08:09:40.315665Z",
     "iopub.status.idle": "2026-01-08T08:09:44.388152Z",
     "shell.execute_reply": "2026-01-08T08:09:44.387424Z",
     "shell.execute_reply.started": "2026-01-08T08:09:40.315966Z"
    },
    "papermill": {
     "duration": 3.528045,
     "end_time": "2025-12-25T10:28:57.006773",
     "exception": false,
     "start_time": "2025-12-25T10:28:53.478728",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999dfb221f974217845a0b5904c1b537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abd1a38cb6d47c299cb8c035d75b4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading Tokenizer from:\", MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n",
    "    targets = examples[\"translation\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\" # Consistent padding helps training stability\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Replace padding token id with -100 so it's ignored by the loss function\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Process datasets\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c9278",
   "metadata": {
    "papermill": {
     "duration": 0.007545,
     "end_time": "2025-12-25T10:28:57.023322",
     "exception": false,
     "start_time": "2025-12-25T10:28:57.015777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447be19d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:09:44.389418Z",
     "iopub.status.busy": "2026-01-08T08:09:44.389097Z",
     "iopub.status.idle": "2026-01-08T08:09:45.231488Z",
     "shell.execute_reply": "2026-01-08T08:09:45.230953Z",
     "shell.execute_reply.started": "2026-01-08T08:09:44.389394Z"
    },
    "papermill": {
     "duration": 1.398574,
     "end_time": "2025-12-25T10:28:58.434786",
     "exception": false,
     "start_time": "2025-12-25T10:28:57.036212",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model from:\", MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Data Collator handles dynamic padding during batching\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f0dc9",
   "metadata": {},
   "source": [
    "# A6. Optional: Monolingual Pre-Training on Akkadian Texts\n",
    "\n",
    "This step teaches the model Akkadian grammar and morphology BEFORE translation training.\n",
    "Uses published_texts.csv (8,000+ Akkadian texts) with Masked Language Modeling (MLM).\n",
    "\n",
    "Benefits:\n",
    "- Model learns to handle gaps naturally\n",
    "- Better understanding of Akkadian word structure\n",
    "- Improves low-resource translation performance\n",
    "\n",
    "Set ENABLE_MONO_PRETRAIN=True to enable (adds ~30min training time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86171f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:09:45.232827Z",
     "iopub.status.busy": "2026-01-08T08:09:45.232532Z",
     "iopub.status.idle": "2026-01-08T08:39:38.576328Z",
     "shell.execute_reply": "2026-01-08T08:39:38.575692Z",
     "shell.execute_reply.started": "2026-01-08T08:09:45.232802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\n",
      "============================================================\n",
      "Loaded 5000 Akkadian texts for pre-training\n",
      "Created 5000 MLM training examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33dc53fd2a5406699e195b625901f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_55/57318026.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  mlm_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting monolingual pre-training (1 epoch on Akkadian texts)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 29:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.182600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Monolingual pre-training complete\n",
      "Model now understands Akkadian grammar and gaps better!\n"
     ]
    }
   ],
   "source": [
    "# Monolingual Pre-Training Configuration\n",
    "ENABLE_MONO_PRETRAIN = bool(int(os.getenv(\"ENABLE_MONO_PRETRAIN\", \"1\")))  # Set to 1 to enable\n",
    "\n",
    "if ENABLE_MONO_PRETRAIN:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    \n",
    "    if os.path.exists(pub_texts_path):\n",
    "        # Load Akkadian-only texts\n",
    "        pub_texts_df = pd.read_csv(pub_texts_path)\n",
    "        akkadian_texts = pub_texts_df['transliteration'].dropna().astype(str).tolist()\n",
    "        akkadian_texts = [clean_translit(t, keep_gaps=True) for t in akkadian_texts]\n",
    "        akkadian_texts = [t for t in akkadian_texts if len(t.split()) >= 5 and len(t.split()) <= 200]\n",
    "        akkadian_texts = akkadian_texts[:5000]  # Limit for time\n",
    "        \n",
    "        print(f\"Loaded {len(akkadian_texts)} Akkadian texts for pre-training\")\n",
    "        \n",
    "        # Simple MLM approach: Mask random spans\n",
    "        from transformers import DataCollatorForSeq2Seq\n",
    "        \n",
    "        def create_mlm_examples(texts):\n",
    "            \"\"\"Create masked language modeling examples\"\"\"\n",
    "            mlm_examples = []\n",
    "            for text in texts:\n",
    "                tokens = text.split()\n",
    "                if len(tokens) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Mask 15% of tokens\n",
    "                n_mask = max(1, int(len(tokens) * 0.15))\n",
    "                mask_positions = np.random.choice(len(tokens), size=n_mask, replace=False)\n",
    "                \n",
    "                masked_text = []\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if i in mask_positions:\n",
    "                        masked_text.append(\"<extra_id_0>\")  # T5-style sentinel\n",
    "                    else:\n",
    "                        masked_text.append(token)\n",
    "                \n",
    "                input_text = \" \".join(masked_text)\n",
    "                target_text = \" \".join([tokens[i] for i in mask_positions])\n",
    "                \n",
    "                mlm_examples.append({\n",
    "                    \"transliteration\": input_text,\n",
    "                    \"translation\": target_text\n",
    "                })\n",
    "            \n",
    "            return mlm_examples\n",
    "        \n",
    "        mlm_data = create_mlm_examples(akkadian_texts)\n",
    "        print(f\"Created {len(mlm_data)} MLM training examples\")\n",
    "        \n",
    "        # Create MLM dataset\n",
    "        mlm_dataset = Dataset.from_pandas(pd.DataFrame(mlm_data))\n",
    "        \n",
    "        def preprocess_mlm(examples):\n",
    "            inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n",
    "            targets = examples[\"translation\"]\n",
    "            model_inputs = tokenizer(\n",
    "                inputs,\n",
    "                max_length=MAX_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(\n",
    "                    targets,\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "            model_inputs[\"labels\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "            return model_inputs\n",
    "        \n",
    "        tokenized_mlm = mlm_dataset.map(preprocess_mlm, batched=True)\n",
    "        \n",
    "        # Short MLM pre-training (1-2 epochs)\n",
    "        mlm_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=f\"{OUTPUT_DIR}_mlm\",\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=3e-4,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=8,\n",
    "            fp16=True,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"no\",\n",
    "            logging_steps=50,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        mlm_trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=mlm_args,\n",
    "            train_dataset=tokenized_mlm,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting monolingual pre-training (1 epoch on Akkadian texts)...\")\n",
    "        try:\n",
    "            mlm_trainer.train()\n",
    "            print(\"✓ Monolingual pre-training complete\")\n",
    "            print(\"Model now understands Akkadian grammar and gaps better!\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  MLM pre-training failed: {e}\")\n",
    "            print(\"Continuing with main training...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️  published_texts.csv not found, skipping monolingual pre-training\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Monolingual pre-training disabled (set ENABLE_MONO_PRETRAIN=1 to enable)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory after monolingual pre-training to prevent OOM\n",
    "import gc\n",
    "del mlm_trainer\n",
    "del mlm_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared for main training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e880a1",
   "metadata": {
    "papermill": {
     "duration": 0.007375,
     "end_time": "2025-12-25T10:28:58.449624",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.442249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A7. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f5dcb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:39:38.577529Z",
     "iopub.status.busy": "2026-01-08T08:39:38.577280Z",
     "iopub.status.idle": "2026-01-08T08:39:38.613054Z",
     "shell.execute_reply": "2026-01-08T08:39:38.612457Z",
     "shell.execute_reply.started": "2026-01-08T08:39:38.577507Z"
    },
    "papermill": {
     "duration": 0.169346,
     "end_time": "2025-12-25T10:28:58.626361",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.457015",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 5. Training Arguments (OPTIMIZED for Quality & Score 31+) ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # --- VALIDATION STRATEGY ---\n",
    "    save_strategy=\"no\",                   # No checkpoints to save disk space\n",
    "    eval_strategy=\"no\",                   # Skip eval during training for speed\n",
    "    load_best_model_at_end=False,\n",
    "    \n",
    "    learning_rate=3e-4,                   # Higher LR for character-level model\n",
    "\n",
    "    # --- MEMORY-OPTIMIZED BUT EFFECTIVE ---\n",
    "    per_device_train_batch_size=1,        # Memory-safe on P100/T4\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,       # Effective batch = 16\n",
    "    gradient_checkpointing=True,          # Reduce memory usage\n",
    "    \n",
    "    num_train_epochs=12,                  # Increased for better convergence\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,          # Save memory\n",
    "    fp16=True,                            # Mixed precision training\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,                     # Monitor progress\n",
    "\n",
    "    # Quality optimizations\n",
    "    label_smoothing_factor=0.1,           # Regularization\n",
    "    lr_scheduler_type=\"cosine\",           # Smooth learning rate decay\n",
    "    warmup_ratio=0.08,                    # Longer warmup for stability\n",
    "    generation_max_length=420,\n",
    "    generation_num_beams=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0d460",
   "metadata": {
    "papermill": {
     "duration": 0.006845,
     "end_time": "2025-12-25T10:28:58.640314",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.633469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A8. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c0a3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:39:38.614126Z",
     "iopub.status.busy": "2026-01-08T08:39:38.613826Z",
     "iopub.status.idle": "2026-01-08T08:39:39.370781Z",
     "shell.execute_reply": "2026-01-08T08:39:39.369857Z",
     "shell.execute_reply.started": "2026-01-08T08:39:38.614094Z"
    },
    "papermill": {
     "duration": 22.854214,
     "end_time": "2025-12-25T10:29:21.501227",
     "exception": false,
     "start_time": "2025-12-25T10:28:58.647013",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/1667860881.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Force aggressive memory cleanup\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d909d2f",
   "metadata": {
    "papermill": {
     "duration": 0.006946,
     "end_time": "2025-12-25T10:29:21.515664",
     "exception": false,
     "start_time": "2025-12-25T10:29:21.508718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A9. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8847867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T08:39:39.372178Z",
     "iopub.status.busy": "2026-01-08T08:39:39.371813Z",
     "iopub.status.idle": "2026-01-08T08:43:10.281101Z",
     "shell.execute_reply": "2026-01-08T08:43:10.279836Z",
     "shell.execute_reply.started": "2026-01-08T08:39:39.372152Z"
    },
    "papermill": {
     "duration": 5732.305735,
     "end_time": "2025-12-25T12:04:53.828320",
     "exception": false,
     "start_time": "2025-12-25T10:29:21.522585",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Memory Fixes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/552 : < :, Epoch 0.02/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] CUDA OOM detected. Attempting recovery: reducing MAX_LENGTH and accumulation.\n",
      "New MAX_LENGTH: 342\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.19 MiB is free. Process 3364 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 350.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/1183986820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Continue training from current state if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    309\u001b[0m             )\n\u001b[1;32m    310\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             ) if torch.amp.is_autocast_available(ctx.device_type) else contextlib.nullcontext()\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_autocast_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_autocast_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdetached_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.19 MiB is free. Process 3364 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 350.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting Training with Memory Fixes...\")\n",
    "\n",
    "# OOM-safe training wrapper\n",
    "try:\n",
    "    trainer.train()\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"[WARNING] CUDA OOM detected. Attempting recovery: reducing MAX_LENGTH and accumulation.\")\n",
    "        # Reduce max length slightly to free memory for remaining steps\n",
    "        try:\n",
    "            MAX_LENGTH = max(320, int(MAX_LENGTH * 0.9))\n",
    "            print(f\"New MAX_LENGTH: {MAX_LENGTH}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        # Continue training from current state if possible\n",
    "        trainer.train(resume_from_checkpoint=None)\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28d5b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-08T08:43:10.281721Z",
     "iopub.status.idle": "2026-01-08T08:43:10.282019Z",
     "shell.execute_reply": "2026-01-08T08:43:10.281862Z",
     "shell.execute_reply.started": "2026-01-08T08:43:10.281847Z"
    },
    "papermill": {
     "duration": 250.42551,
     "end_time": "2025-12-25T12:09:04.261371",
     "exception": false,
     "start_time": "2025-12-25T12:04:53.835861",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation split with sacreBLEU and chrF AFTER training (memory-safe)\n",
    "print(\"\\n=== POST-TRAINING VALIDATION ===\")\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def dedup_repeats(text: str) -> str:\n",
    "    toks = text.split()\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(out) >= 2 and t == out[-1] == out[-2]:\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def postprocess_text(preds):\n",
    "    out = []\n",
    "    for p in preds:\n",
    "        p = p.strip()\n",
    "        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
    "        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
    "        p = dedup_repeats(p)\n",
    "        if p and p[0].islower():\n",
    "            p = p[0].upper() + p[1:]\n",
    "        if p and p[-1] not in \".!?\":\n",
    "            p += \".\"\n",
    "        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
    "        out.append(p.strip())\n",
    "    return out\n",
    "\n",
    "val_texts = dataset[\"test\"][\"transliteration\"]\n",
    "val_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n",
    "\n",
    "def generate_batch(texts):\n",
    "    batch_inputs = [PREFIX + doc for doc in texts]\n",
    "    enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_length=MAX_LENGTH,\n",
    "        min_length=6,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=1.05,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "preds = []\n",
    "for i in range(0, len(val_texts), 8):\n",
    "    preds.extend(generate_batch(val_texts[i:i+8]))\n",
    "\n",
    "preds = postprocess_text(preds)\n",
    "bleu = metric_bleu.compute(predictions=preds, references=val_refs)\n",
    "chrf = metric_chrf.compute(predictions=preds, references=val_refs)\n",
    "print(f\"Validation BLEU: {bleu['score']:.2f}, chrF: {chrf['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a27615",
   "metadata": {
    "papermill": {
     "duration": 0.007132,
     "end_time": "2025-12-25T12:09:04.275738",
     "exception": false,
     "start_time": "2025-12-25T12:09:04.268606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A10. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06794aa3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-08T08:43:10.283354Z",
     "iopub.status.idle": "2026-01-08T08:43:10.283622Z",
     "shell.execute_reply": "2026-01-08T08:43:10.283519Z",
     "shell.execute_reply.started": "2026-01-08T08:43:10.283502Z"
    },
    "papermill": {
     "duration": 3.633252,
     "end_time": "2025-12-25T12:09:07.916126",
     "exception": false,
     "start_time": "2025-12-25T12:09:04.282874",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Notebook A Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec693e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-08T08:43:10.284698Z",
     "iopub.status.idle": "2026-01-08T08:43:10.284976Z",
     "shell.execute_reply": "2026-01-08T08:43:10.284835Z",
     "shell.execute_reply.started": "2026-01-08T08:43:10.284821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# A11. Optional Self-Training Augmentation (Small, OOM-Safe)\n",
    "ENABLE_SELF_TRAIN = False\n",
    "MAX_PSEUDO = int(os.getenv(\"BYT5_MAX_PSEUDO\", \"500\"))  # keep small to avoid OOM\n",
    "\n",
    "if ENABLE_SELF_TRAIN:\n",
    "    print(\"\\n=== SELF-TRAINING AUGMENTATION (ByT5) ===\")\n",
    "    pub_path = f\"{DATA_DIR}/published_texts.csv\"\n",
    "    if os.path.exists(pub_path):\n",
    "        pub_df = pd.read_csv(pub_path)\n",
    "        translits = pub_df.get(\"transliteration\", pd.Series([])).dropna().astype(str).tolist()\n",
    "        translits = [clean_translit(t) for t in translits]\n",
    "        translits = [t for t in translits if 5 <= len(t.split()) <= 180]\n",
    "        translits = translits[:MAX_PSEUDO]\n",
    "        print(f\"Generating pseudo translations for {len(translits)} extra transliterations...\")\n",
    "\n",
    "        def generate_batch(texts):\n",
    "            batch_inputs = [PREFIX + doc for doc in texts]\n",
    "            enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "            gen = model.generate(\n",
    "                **enc,\n",
    "                max_length=min(MAX_LENGTH, 400),\n",
    "                min_length=6,\n",
    "                num_beams=6,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.05,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "        pseudo_trans = []\n",
    "        for i in range(0, len(translits), 8):  # small batch to avoid OOM\n",
    "            try:\n",
    "                batch_preds = generate_batch(translits[i:i+8])\n",
    "                pseudo_trans.extend(batch_preds)\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"[WARNING] OOM during pseudo generation; skipping remaining.\")\n",
    "                    break\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        # Postprocess & filter\n",
    "        def dedup_repeats(text: str) -> str:\n",
    "            toks = text.split()\n",
    "            out = []\n",
    "            for t in toks:\n",
    "                if len(out) >= 2 and t == out[-1] == out[-2]:\n",
    "                    continue\n",
    "                out.append(t)\n",
    "            return \" \".join(out)\n",
    "        def postprocess_text(preds):\n",
    "            out = []\n",
    "            for p in preds:\n",
    "                p = p.strip()\n",
    "                p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
    "                p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
    "                p = dedup_repeats(p)\n",
    "                if p and p[0].islower():\n",
    "                    p = p[0].upper() + p[1:]\n",
    "                if p and p[-1] not in \".!?\":\n",
    "                    p += \".\"\n",
    "                p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
    "                out.append(p.strip())\n",
    "            return out\n",
    "\n",
    "        pseudo_trans = postprocess_text(pseudo_trans)\n",
    "        aug_df = pd.DataFrame({\"transliteration\": translits[:len(pseudo_trans)], \"translation\": pseudo_trans})\n",
    "        aug_df[\"src_len\"] = aug_df[\"transliteration\"].str.split().str.len()\n",
    "        aug_df[\"tgt_len\"] = aug_df[\"translation\"].str.split().str.len()\n",
    "        ratio = (aug_df[\"tgt_len\"] / aug_df[\"src_len\"]).clip(upper=6)\n",
    "        aug_df = aug_df[(aug_df[\"tgt_len\"] >= 4) & (ratio >= 0.5) & (ratio <= 6)]\n",
    "        aug_df = aug_df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
    "        print(f\"Pseudo pairs retained after filtering: {len(aug_df)}\")\n",
    "\n",
    "        base_train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "        base_train = base_train.dropna(subset=[\"transliteration\", \"translation\"]).astype(str)\n",
    "        base_train[\"transliteration\"] = base_train[\"transliteration\"].map(clean_translit)\n",
    "        base_train[\"translation\"] = base_train[\"translation\"].map(clean_translation)\n",
    "        combined = pd.concat([\n",
    "            base_train[[\"transliteration\", \"translation\"]],\n",
    "            aug_df[[\"transliteration\", \"translation\"]]\n",
    "        ], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "        print(f\"Total combined training pairs: {len(combined)}\")\n",
    "\n",
    "        ds_combined = Dataset.from_pandas(combined)\n",
    "        def preprocess_function_aug(examples):\n",
    "            inputs = [PREFIX + ex for ex in examples[\"transliteration\"]]\n",
    "            targets = examples[\"translation\"]\n",
    "            model_inputs = tokenizer(\n",
    "                inputs,\n",
    "                max_length=MAX_LENGTH,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(\n",
    "                    targets,\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "            model_inputs[\"labels\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "                for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "            return model_inputs\n",
    "        tokenized_combined = ds_combined.map(preprocess_function_aug, batched=True)\n",
    "\n",
    "        training_args_aug = Seq2SeqTrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            learning_rate=2.5e-4,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=16,\n",
    "            num_train_epochs=1,  # keep short to avoid OOM/time\n",
    "            fp16=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        trainer_aug = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args_aug,\n",
    "            train_dataset=tokenized_combined,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"Starting second-stage training (ByT5) with augmented data...\")\n",
    "        try:\n",
    "            trainer_aug.train()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[WARNING] Augmentation training skipped due to error: {e}\")\n",
    "        print(\"Augmentation stage complete.\")\n",
    "\n",
    "        print(f\"Saving augmented model to {OUTPUT_DIR}...\")\n",
    "        trainer_aug.save_model(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    else:\n",
    "        print(\"published_texts.csv not found; skipping self-training.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9082937,
     "sourceId": 14236819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6059.435801,
   "end_time": "2025-12-25T12:09:11.716859",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T10:28:12.281058",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1fe03852120a40cda6d5c85f4e29fcaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b48bf215217541358d86be12f43b7f3c",
       "placeholder": "​",
       "style": "IPY_MODEL_fcaea7f32ee24eebaebfc0e32266d44a",
       "tabbable": null,
       "tooltip": null,
       "value": " 8.15k/? [00:00&lt;00:00, 928kB/s]"
      }
     },
     "226d44a1d8884129a1d1230b2642a786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b0c8da6c308b446cb01170a376a943c3",
       "max": 77,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_39da431ed6e041ef8c50061ff0cb2734",
       "tabbable": null,
       "tooltip": null,
       "value": 77
      }
     },
     "2bccb998d49d415580aaed49b33a1b22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_791efc0e593d4599ba993239183a55e3",
       "placeholder": "​",
       "style": "IPY_MODEL_683b28591d2f4363aa74a0a1a44c7a1c",
       "tabbable": null,
       "tooltip": null,
       "value": " 77/77 [00:00&lt;00:00, 388.82 examples/s]"
      }
     },
     "2d21f5383a1548d98e116b290031fccd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39da431ed6e041ef8c50061ff0cb2734": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3b23edd7473a4b5cbeb0e7453bf9b6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "427b3335b5374628a175042c3a3382e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d21f5383a1548d98e116b290031fccd",
       "placeholder": "​",
       "style": "IPY_MODEL_9aed8f291fff48a5a708caf8c20013d8",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading builder script: "
      }
     },
     "44b7b6316c1c4a4bbc5e3b84e174e79f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6a4941ccd41469aa09a0ec5a804af6e",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b76718f5d5b1422ab37b370c9cc527ff",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "45c5ff13e0804ab38df84c594f5cbfe4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c11ec6efe7cb4722883a279f3eb185ed",
       "placeholder": "​",
       "style": "IPY_MODEL_8846ab600ef243c39c4f1cb71a308f59",
       "tabbable": null,
       "tooltip": null,
       "value": "Downloading builder script: "
      }
     },
     "4a7eec71c5664bbd922f48d02fb439ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b48b9cb404748bdba5a1554f4687518": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e865f98ee6084edf9b9c63807b52da6b",
        "IPY_MODEL_9f53690cf1d84b5aa3e3929a66799044",
        "IPY_MODEL_f030f089b1124566ab437b62c99b2578"
       ],
       "layout": "IPY_MODEL_c2b4df78e626401c8a097c7890f1386e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6297171298534da8a8ae461611777771": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "67bff04e3ed849f8aadf4b6b7df2b9e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_427b3335b5374628a175042c3a3382e4",
        "IPY_MODEL_44b7b6316c1c4a4bbc5e3b84e174e79f",
        "IPY_MODEL_1fe03852120a40cda6d5c85f4e29fcaf"
       ],
       "layout": "IPY_MODEL_b8d2e1271f1e4afdb533651d811a2bb8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "683b28591d2f4363aa74a0a1a44c7a1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6c90b63b47dc4de495594cfd1e6519b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "701dd03462844cf0af47a3ab8c452a04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_45c5ff13e0804ab38df84c594f5cbfe4",
        "IPY_MODEL_bc4d79d1c6384c23a736957e90afd304",
        "IPY_MODEL_9d58cd0d03464cd18c653fdec13d87e4"
       ],
       "layout": "IPY_MODEL_b0bcd130aa8341b1acef83b7cc545dce",
       "tabbable": null,
       "tooltip": null
      }
     },
     "71affa5ac8894c80959fbb0448728c81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c324f021e0034d85bcf6459f17a7a752",
       "placeholder": "​",
       "style": "IPY_MODEL_6297171298534da8a8ae461611777771",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "791efc0e593d4599ba993239183a55e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8846ab600ef243c39c4f1cb71a308f59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f98493e151546f2baa629a4edf06a14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9369db00183844a5b53806dd64a214db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "969e0b0841ff4999a85c76101abdcc41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "9aed8f291fff48a5a708caf8c20013d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d58cd0d03464cd18c653fdec13d87e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b789e817126e4af1a184cfae19f2612f",
       "placeholder": "​",
       "style": "IPY_MODEL_3b23edd7473a4b5cbeb0e7453bf9b6bc",
       "tabbable": null,
       "tooltip": null,
       "value": " 9.01k/? [00:00&lt;00:00, 914kB/s]"
      }
     },
     "9f53690cf1d84b5aa3e3929a66799044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ab64a8242ff94bdfbfceeabac64877cf",
       "max": 1452,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0f4b1fc30b24fd1a84d58113003352a",
       "tabbable": null,
       "tooltip": null,
       "value": 1452
      }
     },
     "a1151781b3e84a738ae66c7ac283ba3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_71affa5ac8894c80959fbb0448728c81",
        "IPY_MODEL_226d44a1d8884129a1d1230b2642a786",
        "IPY_MODEL_2bccb998d49d415580aaed49b33a1b22"
       ],
       "layout": "IPY_MODEL_4a7eec71c5664bbd922f48d02fb439ac",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a8e873b972004c38ae11c1d76cfe84ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ab64a8242ff94bdfbfceeabac64877cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae69eb96031c4d43bf4c52da5aedb198": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0bcd130aa8341b1acef83b7cc545dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b0c8da6c308b446cb01170a376a943c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b48bf215217541358d86be12f43b7f3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b76718f5d5b1422ab37b370c9cc527ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b789e817126e4af1a184cfae19f2612f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8d2e1271f1e4afdb533651d811a2bb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc4d79d1c6384c23a736957e90afd304": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_969e0b0841ff4999a85c76101abdcc41",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a8e873b972004c38ae11c1d76cfe84ba",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "c11ec6efe7cb4722883a279f3eb185ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2b4df78e626401c8a097c7890f1386e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c324f021e0034d85bcf6459f17a7a752": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0f4b1fc30b24fd1a84d58113003352a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e6a4941ccd41469aa09a0ec5a804af6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "e865f98ee6084edf9b9c63807b52da6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ae69eb96031c4d43bf4c52da5aedb198",
       "placeholder": "​",
       "style": "IPY_MODEL_8f98493e151546f2baa629a4edf06a14",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "f030f089b1124566ab437b62c99b2578": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9369db00183844a5b53806dd64a214db",
       "placeholder": "​",
       "style": "IPY_MODEL_6c90b63b47dc4de495594cfd1e6519b0",
       "tabbable": null,
       "tooltip": null,
       "value": " 1452/1452 [00:03&lt;00:00, 448.42 examples/s]"
      }
     },
     "fcaea7f32ee24eebaebfc0e32266d44a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
