{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121150,"databundleVersionId":14976537,"sourceType":"competition"},{"sourceId":14238328,"sourceType":"datasetVersion","datasetId":9084005},{"sourceId":14238329,"sourceType":"datasetVersion","datasetId":9084006},{"sourceId":14238668,"sourceType":"datasetVersion","datasetId":9084224}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n# ==================================================================================\n#   DEEP PAST CHALLENGE - FINAL ROBUST ENSEMBLE\n#   ------------------------------------------------------------------------------\n#   Logic: ByT5 (Best Morphology) + T5 (Best Grammar) + Marian (Fluency)\n#   Metric: GeoMean(BLEU, chrF++) optimized via weighted voting & keyword validation.\n# ==================================================================================\n\nimport os\nimport gc\nimport sys\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom tqdm.auto import tqdm\nimport difflib\n\n# ==================================================================================\n# 1. CONFIGURATION (!!! UPDATE THESE PATHS !!!)\n# ==================================================================================\n# Check the \"Data\" tab in Kaggle to find the exact paths to your saved models.\n# They will look something like \"/kaggle/input/your-notebook-name/byt5-base-saved\"\n\nMODEL_PATHS = {\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",       # Update this\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\",       # Update this\n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"   # Update this\n}\n\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\n\n# ==================================================================================\n# 2. ADVANCED CLEANING & UTILS\n# ==================================================================================\n\ndef clean_prediction(text):\n    \"\"\"Post-processing to fix common NMT artifacts.\"\"\"\n    if not isinstance(text, str): return \"\"\n    text = text.strip()\n    \n    # 1. Fix punctuation spacing (e.g., \"city .\" -> \"city.\")\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    \n    # 2. Capitalize first letter\n    if text:\n        text = text[0].upper() + text[1:]\n        \n    # 3. Ensure sentence ending punctuation (if missing)\n    if text and text[-1] not in \".!?\":\n        text += \".\"\n        \n    return text\n\ndef is_garbage(text, source_text=\"\"):\n    \"\"\"Returns True if the prediction is likely a hallucination or failure.\"\"\"\n    if len(text) < 3: return True\n    \n    # Check for repetition loops (e.g., \"the silver the silver the silver\")\n    if len(text) > 20 and len(set(text.split())) < 3:\n        return True\n        \n    # Check if model just copied the input (common failure mode)\n    ratio = difflib.SequenceMatcher(None, text.lower(), source_text.lower()).ratio()\n    if ratio > 0.8: # If translation is 80% identical to source transliteration\n        return True\n        \n    return False\n\n# ==================================================================================\n# 3. EFFICIENT INFERENCE ENGINE\n# ==================================================================================\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\n        self.texts = [prefix + str(t) for t in texts]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx], \n            truncation=True, \n            padding=\"max_length\", \n            max_length=self.max_len, \n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0), \n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n        }\n\ndef generate_predictions(model_name, model_path, inputs):\n    \"\"\"Loads model, predicts, then UNLOADS model to save RAM.\"\"\"\n    print(f\"\\n[INFO] Processing with {model_name.upper()}...\")\n    \n    if not os.path.exists(model_path):\n        print(f\"[WARNING] Path not found: {model_path}. Skipping.\")\n        return [\"\"] * len(inputs)\n\n    # 1. Config based on model type\n    if \"byt5\" in model_name:\n        max_len = 512\n        prefix = \"translate Akkadian to English: \"\n    elif \"t5\" in model_name:\n        max_len = 256\n        prefix = \"translate Akkadian to English: \"\n    else: # Marian\n        max_len = 128\n        prefix = \"\"\n\n    # 2. Load\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n    except Exception as e:\n        print(f\"[ERROR] Failed to load {model_name}: {e}\")\n        return [\"\"] * len(inputs)\n\n    # 3. Predict\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    preds = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            mask = batch[\"attention_mask\"].to(DEVICE)\n            \n            # Beam search with penalties for repetition (Crucial for score)\n            gen_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=mask,\n                max_length=max_len,\n                num_beams=5,               # Higher beam = better quality\n                no_repeat_ngram_size=3,    # Prevent \"and the and the\"\n                repetition_penalty=1.2,    # Penalty for loops\n                early_stopping=True\n            )\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n            preds.extend([clean_prediction(d) for d in decoded])\n\n    # 4. Cleanup (CRITICAL for 16GB GPU)\n    del model, tokenizer, dataset, loader\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return preds\n\n# ==================================================================================\n# 4. DOMAIN-AWARE ENSEMBLE STRATEGY\n# ==================================================================================\n\n# Dictionary of high-value Akkadian keywords\nKEYWORDS = {\n    \"qi-bi-ma\": [\"speak\", \"tell\", \"say\"],\n    \"um-ma\": [\"thus\"],\n    \"a-na\": [\"to\", \"for\"],\n    \"ku-babbar\": [\"silver\", \"money\"],\n    \"ku-gi\": [\"gold\"],\n    \"a-lim\": [\"city\"],\n    \"e-gal\": [\"palace\"],\n    \"dam-qar\": [\"merchant\", \"agent\"],\n    \"tup-pi\": [\"tablet\", \"letter\", \"document\", \"record\"],\n    \"be-li\": [\"lord\", \"master\", \"boss\"],\n    \"a-hi\": [\"brother\", \"partner\"],\n    \"i-na\": [\"in\", \"from\", \"on\"],\n    \"su-be2-el\": [\"send\"],\n    \"u2-bi-il\": [\"brought\", \"carried\"],\n    \"li-bi-shi\": [\"should be\", \"let it be\"],\n    \"mi3-ma\": [\"anything\", \"something\", \"property\"]\n}\n\ndef ensemble_vote(inputs, predictions_dict):\n    \"\"\"\n    Selects the best translation based on:\n    1. Model Trust (ByT5 > T5 > Marian)\n    2. Keyword Coverage (Did it translate 'silver' correctly?)\n    3. Garbage Detection\n    \"\"\"\n    final_translations = []\n    \n    # Trust weights based on your training logs (ByT5 was superior)\n    MODEL_WEIGHTS = {\"byt5\": 3.0, \"t5\": 2.0, \"marian\": 1.0}\n    \n    print(\"\\n[INFO] Running Weighted Ensemble Voting...\")\n    \n    for i in tqdm(range(len(inputs))):\n        src = inputs[i].lower()\n        candidates = {\n            k: predictions_dict[k][i] \n            for k in predictions_dict \n            if predictions_dict[k][i] # Only consider if prediction exists\n        }\n        \n        if not candidates:\n            final_translations.append(\"Broken text.\")\n            continue\n\n        best_score = -1\n        best_text = \"\"\n        \n        for model_name, text in candidates.items():\n            # A. Base Score (Model Confidence)\n            score = MODEL_WEIGHTS.get(model_name, 1.0)\n            \n            # B. Filter Garbage\n            if is_garbage(text, src):\n                score -= 10 # Heavily penalize garbage\n                \n            # C. Keyword Bonus (The Secret Sauce)\n            text_lower = text.lower()\n            for akk, eng_list in KEYWORDS.items():\n                if akk in src:\n                    # If the source has 'silver', and translation has 'silver', BOOST IT\n                    if any(eng in text_lower for eng in eng_list):\n                        score += 2.0\n            \n            # D. Length Penalty (Too short is usually bad, unless input is short)\n            if len(text) < 10 and len(src) > 20:\n                score -= 1.0\n                \n            if score > best_score:\n                best_score = score\n                best_text = text\n        \n        # Fallback if all scores are negative (all garbage)\n        if best_score < 0:\n            # Prefer T5 or ByT5 output even if garbage, better than empty\n            best_text = candidates.get(\"byt5\", candidates.get(\"t5\", \"Broken text.\"))\n\n        final_translations.append(best_text)\n        \n    return final_translations\n\n# ==================================================================================\n# 5. EXECUTION PIPELINE\n# ==================================================================================\n\ndef main():\n    print(\"=== STARTING INFERENCE PIPELINE ===\")\n    \n    # 1. Load Test Data\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\n    ids = test_df[\"id\"].tolist()\n    \n    print(f\"Loaded {len(inputs)} test sentences.\")\n    \n    # 2. Run Inference (Sequential to save memory)\n    all_preds = {}\n    \n    # Run ByT5 (The Specialist)\n    all_preds[\"byt5\"] = generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], inputs)\n    \n    # Run T5 (The Generalist)\n    all_preds[\"t5\"] = generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], inputs)\n    \n    # Run Marian (The Fluency Expert)\n    all_preds[\"marian\"] = generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], inputs)\n    \n    # 3. Ensemble\n    final_preds = ensemble_vote(inputs, all_preds)\n    \n    # 4. Save Submission\n    sub = pd.DataFrame({\"id\": ids, \"translation\": final_preds})\n    \n    # Final check for empty strings\n    sub[\"translation\"] = sub[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Broken text.\")\n    \n    sub.to_csv(\"submission.csv\", index=False)\n    print(\"\\n[SUCCESS] submission.csv generated successfully.\")\n    print(sub.head())\n\nif __name__ == \"__main__\":\n    main()\n\n''' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-21T12:18:33.672157Z","iopub.execute_input":"2025-12-21T12:18:33.672428Z","iopub.status.idle":"2025-12-21T12:18:33.686263Z","shell.execute_reply.started":"2025-12-21T12:18:33.672384Z","shell.execute_reply":"2025-12-21T12:18:33.685656Z"}},"outputs":[{"name":"stderr","text":"<>:47: SyntaxWarning: invalid escape sequence '\\s'\n<>:47: SyntaxWarning: invalid escape sequence '\\s'\n/tmp/ipykernel_55/2929549046.py:47: SyntaxWarning: invalid escape sequence '\\s'\n  text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n# ==================================================================================\\n#   DEEP PAST CHALLENGE - FINAL ROBUST ENSEMBLE\\n#   ------------------------------------------------------------------------------\\n#   Logic: ByT5 (Best Morphology) + T5 (Best Grammar) + Marian (Fluency)\\n#   Metric: GeoMean(BLEU, chrF++) optimized via weighted voting & keyword validation.\\n# ==================================================================================\\n\\nimport os\\nimport gc\\nimport sys\\nimport re\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nfrom tqdm.auto import tqdm\\nimport difflib\\n\\n# ==================================================================================\\n# 1. CONFIGURATION (!!! UPDATE THESE PATHS !!!)\\n# ==================================================================================\\n# Check the \"Data\" tab in Kaggle to find the exact paths to your saved models.\\n# They will look something like \"/kaggle/input/your-notebook-name/byt5-base-saved\"\\n\\nMODEL_PATHS = {\\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",       # Update this\\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\",       # Update this\\n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"   # Update this\\n}\\n\\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nBATCH_SIZE = 16\\n\\n# ==================================================================================\\n# 2. ADVANCED CLEANING & UTILS\\n# ==================================================================================\\n\\ndef clean_prediction(text):\\n    \"\"\"Post-processing to fix common NMT artifacts.\"\"\"\\n    if not isinstance(text, str): return \"\"\\n    text = text.strip()\\n    \\n    # 1. Fix punctuation spacing (e.g., \"city .\" -> \"city.\")\\n    text = re.sub(r\\'\\\\s+([.,!?;:])\\', r\\'\\x01\\', text)\\n    \\n    # 2. Capitalize first letter\\n    if text:\\n        text = text[0].upper() + text[1:]\\n        \\n    # 3. Ensure sentence ending punctuation (if missing)\\n    if text and text[-1] not in \".!?\":\\n        text += \".\"\\n        \\n    return text\\n\\ndef is_garbage(text, source_text=\"\"):\\n    \"\"\"Returns True if the prediction is likely a hallucination or failure.\"\"\"\\n    if len(text) < 3: return True\\n    \\n    # Check for repetition loops (e.g., \"the silver the silver the silver\")\\n    if len(text) > 20 and len(set(text.split())) < 3:\\n        return True\\n        \\n    # Check if model just copied the input (common failure mode)\\n    ratio = difflib.SequenceMatcher(None, text.lower(), source_text.lower()).ratio()\\n    if ratio > 0.8: # If translation is 80% identical to source transliteration\\n        return True\\n        \\n    return False\\n\\n# ==================================================================================\\n# 3. EFFICIENT INFERENCE ENGINE\\n# ==================================================================================\\n\\nclass TextDataset(Dataset):\\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\\n        self.texts = [prefix + str(t) for t in texts]\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n\\n    def __len__(self):\\n        return len(self.texts)\\n\\n    def __getitem__(self, idx):\\n        enc = self.tokenizer(\\n            self.texts[idx], \\n            truncation=True, \\n            padding=\"max_length\", \\n            max_length=self.max_len, \\n            return_tensors=\"pt\"\\n        )\\n        return {\\n            \"input_ids\": enc[\"input_ids\"].squeeze(0), \\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\\n        }\\n\\ndef generate_predictions(model_name, model_path, inputs):\\n    \"\"\"Loads model, predicts, then UNLOADS model to save RAM.\"\"\"\\n    print(f\"\\n[INFO] Processing with {model_name.upper()}...\")\\n    \\n    if not os.path.exists(model_path):\\n        print(f\"[WARNING] Path not found: {model_path}. Skipping.\")\\n        return [\"\"] * len(inputs)\\n\\n    # 1. Config based on model type\\n    if \"byt5\" in model_name:\\n        max_len = 512\\n        prefix = \"translate Akkadian to English: \"\\n    elif \"t5\" in model_name:\\n        max_len = 256\\n        prefix = \"translate Akkadian to English: \"\\n    else: # Marian\\n        max_len = 128\\n        prefix = \"\"\\n\\n    # 2. Load\\n    try:\\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\\n    except Exception as e:\\n        print(f\"[ERROR] Failed to load {model_name}: {e}\")\\n        return [\"\"] * len(inputs)\\n\\n    # 3. Predict\\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\\n    \\n    preds = []\\n    with torch.no_grad():\\n        for batch in tqdm(loader, desc=f\"Inference {model_name}\"):\\n            input_ids = batch[\"input_ids\"].to(DEVICE)\\n            mask = batch[\"attention_mask\"].to(DEVICE)\\n            \\n            # Beam search with penalties for repetition (Crucial for score)\\n            gen_ids = model.generate(\\n                input_ids=input_ids,\\n                attention_mask=mask,\\n                max_length=max_len,\\n                num_beams=5,               # Higher beam = better quality\\n                no_repeat_ngram_size=3,    # Prevent \"and the and the\"\\n                repetition_penalty=1.2,    # Penalty for loops\\n                early_stopping=True\\n            )\\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\\n            preds.extend([clean_prediction(d) for d in decoded])\\n\\n    # 4. Cleanup (CRITICAL for 16GB GPU)\\n    del model, tokenizer, dataset, loader\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    return preds\\n\\n# ==================================================================================\\n# 4. DOMAIN-AWARE ENSEMBLE STRATEGY\\n# ==================================================================================\\n\\n# Dictionary of high-value Akkadian keywords\\nKEYWORDS = {\\n    \"qi-bi-ma\": [\"speak\", \"tell\", \"say\"],\\n    \"um-ma\": [\"thus\"],\\n    \"a-na\": [\"to\", \"for\"],\\n    \"ku-babbar\": [\"silver\", \"money\"],\\n    \"ku-gi\": [\"gold\"],\\n    \"a-lim\": [\"city\"],\\n    \"e-gal\": [\"palace\"],\\n    \"dam-qar\": [\"merchant\", \"agent\"],\\n    \"tup-pi\": [\"tablet\", \"letter\", \"document\", \"record\"],\\n    \"be-li\": [\"lord\", \"master\", \"boss\"],\\n    \"a-hi\": [\"brother\", \"partner\"],\\n    \"i-na\": [\"in\", \"from\", \"on\"],\\n    \"su-be2-el\": [\"send\"],\\n    \"u2-bi-il\": [\"brought\", \"carried\"],\\n    \"li-bi-shi\": [\"should be\", \"let it be\"],\\n    \"mi3-ma\": [\"anything\", \"something\", \"property\"]\\n}\\n\\ndef ensemble_vote(inputs, predictions_dict):\\n    \"\"\"\\n    Selects the best translation based on:\\n    1. Model Trust (ByT5 > T5 > Marian)\\n    2. Keyword Coverage (Did it translate \\'silver\\' correctly?)\\n    3. Garbage Detection\\n    \"\"\"\\n    final_translations = []\\n    \\n    # Trust weights based on your training logs (ByT5 was superior)\\n    MODEL_WEIGHTS = {\"byt5\": 3.0, \"t5\": 2.0, \"marian\": 1.0}\\n    \\n    print(\"\\n[INFO] Running Weighted Ensemble Voting...\")\\n    \\n    for i in tqdm(range(len(inputs))):\\n        src = inputs[i].lower()\\n        candidates = {\\n            k: predictions_dict[k][i] \\n            for k in predictions_dict \\n            if predictions_dict[k][i] # Only consider if prediction exists\\n        }\\n        \\n        if not candidates:\\n            final_translations.append(\"Broken text.\")\\n            continue\\n\\n        best_score = -1\\n        best_text = \"\"\\n        \\n        for model_name, text in candidates.items():\\n            # A. Base Score (Model Confidence)\\n            score = MODEL_WEIGHTS.get(model_name, 1.0)\\n            \\n            # B. Filter Garbage\\n            if is_garbage(text, src):\\n                score -= 10 # Heavily penalize garbage\\n                \\n            # C. Keyword Bonus (The Secret Sauce)\\n            text_lower = text.lower()\\n            for akk, eng_list in KEYWORDS.items():\\n                if akk in src:\\n                    # If the source has \\'silver\\', and translation has \\'silver\\', BOOST IT\\n                    if any(eng in text_lower for eng in eng_list):\\n                        score += 2.0\\n            \\n            # D. Length Penalty (Too short is usually bad, unless input is short)\\n            if len(text) < 10 and len(src) > 20:\\n                score -= 1.0\\n                \\n            if score > best_score:\\n                best_score = score\\n                best_text = text\\n        \\n        # Fallback if all scores are negative (all garbage)\\n        if best_score < 0:\\n            # Prefer T5 or ByT5 output even if garbage, better than empty\\n            best_text = candidates.get(\"byt5\", candidates.get(\"t5\", \"Broken text.\"))\\n\\n        final_translations.append(best_text)\\n        \\n    return final_translations\\n\\n# ==================================================================================\\n# 5. EXECUTION PIPELINE\\n# ==================================================================================\\n\\ndef main():\\n    print(\"=== STARTING INFERENCE PIPELINE ===\")\\n    \\n    # 1. Load Test Data\\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\\n    ids = test_df[\"id\"].tolist()\\n    \\n    print(f\"Loaded {len(inputs)} test sentences.\")\\n    \\n    # 2. Run Inference (Sequential to save memory)\\n    all_preds = {}\\n    \\n    # Run ByT5 (The Specialist)\\n    all_preds[\"byt5\"] = generate_predictions(\"byt5\", MODEL_PATHS[\"byt5\"], inputs)\\n    \\n    # Run T5 (The Generalist)\\n    all_preds[\"t5\"] = generate_predictions(\"t5\", MODEL_PATHS[\"t5\"], inputs)\\n    \\n    # Run Marian (The Fluency Expert)\\n    all_preds[\"marian\"] = generate_predictions(\"marian\", MODEL_PATHS[\"marian\"], inputs)\\n    \\n    # 3. Ensemble\\n    final_preds = ensemble_vote(inputs, all_preds)\\n    \\n    # 4. Save Submission\\n    sub = pd.DataFrame({\"id\": ids, \"translation\": final_preds})\\n    \\n    # Final check for empty strings\\n    sub[\"translation\"] = sub[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Broken text.\")\\n    \\n    sub.to_csv(\"submission.csv\", index=False)\\n    print(\"\\n[SUCCESS] submission.csv generated successfully.\")\\n    print(sub.head())\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"'''\n# ==================================================================================\n#   DEEP PAST CHALLENGE - HYBRID SOLUTION (Retrieval + Neural Ensemble)\n#   ------------------------------------------------------------------------------\n#   Strategy:\n#   1. CHECK: Is this input >70% similar to something we already have?\n#      -> YES: Use the existing human translation (Perfect accuracy)\n#      -> NO:  Use the Neural Ensemble (ByT5 + T5 + Marian)\n# ==================================================================================\n\nimport os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm.auto import tqdm\nimport difflib\n\n# ==================================================================================\n# 1. CONFIGURATION\n# ==================================================================================\nMODEL_PATHS = {\n    # UPDATE THESE EXACT PATHS based on your Kaggle Input names\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\", \n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"\n}\n\n# Where the competition data lives (for Retrieval Database)\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\n\n# SIMILARITY THRESHOLD\n# If an input is > 75% similar to a training sentence, we trust the training data\n# over the AI.\nRETRIEVAL_THRESHOLD = 0.75 \n\n# ==================================================================================\n# 2. RETRIEVAL ENGINE (The \"User 2\" Logic)\n# ==================================================================================\nclass RetrievalEngine:\n    def __init__(self, train_csv_path):\n        print(\"[INFO] Building Retrieval Database...\")\n        self.df = pd.read_csv(train_csv_path)\n        \n        # 1. Vectorize the Training Data (Character n-grams capture spelling)\n        self.vectorizer = TfidfVectorizer(\n            analyzer='char_wb', \n            ngram_range=(2, 6),\n            min_df=1\n        )\n        self.train_vectors = self.vectorizer.fit_transform(self.df['transliteration'].astype(str))\n        print(f\"[INFO] Indexed {self.train_vectors.shape[0]} training documents.\")\n\n    def find_match(self, input_text):\n        \"\"\"\n        Returns: (best_translation, similarity_score)\n        \"\"\"\n        # Vectorize input\n        input_vec = self.vectorizer.transform([input_text])\n        \n        # Calculate similarity against ALL training data\n        similarities = cosine_similarity(input_vec, self.train_vectors).flatten()\n        \n        # Get best match\n        best_idx = np.argmax(similarities)\n        best_score = similarities[best_idx]\n        \n        if best_score > 0:\n            return self.df.iloc[best_idx]['translation'], best_score\n        return None, 0.0\n\n# ==================================================================================\n# 3. NEURAL ENGINE (The \"User 1\" Logic - Your Models)\n# ==================================================================================\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\n        self.texts = [prefix + str(t) for t in texts]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx], \n            truncation=True, \n            padding=\"max_length\", \n            max_length=self.max_len, \n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0), \n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n        }\n\ndef clean_prediction(text):\n    if not isinstance(text, str): return \"\"\n    text = text.strip()\n    # Fix spacing around punctuation\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n    if text and text[0].islower(): text = text[0].upper() + text[1:]\n    return text\n\ndef run_neural_inference(model_name, model_path, inputs):\n    print(f\"\\n[INFO] Neural Inference: {model_name.upper()}...\")\n    \n    if not os.path.exists(model_path):\n        print(f\"[WARNING] Path not found: {model_path}\")\n        return [\"\"] * len(inputs)\n\n    # Config\n    if \"byt5\" in model_name:\n        max_len = 400; prefix = \"translate Akkadian to English: \"\n    elif \"t5\" in model_name:\n        max_len = 256; prefix = \"translate Akkadian to English: \"\n    else: \n        max_len = 160; prefix = \"\"\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n    except Exception as e:\n        print(f\"[ERROR] Could not load {model_name}: {e}\")\n        return [\"\"] * len(inputs)\n\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\n    \n    # FIXED: num_workers=0 prevents the \"semaphore\" error in Kaggle\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    preds = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=model_name):\n            gen_ids = model.generate(\n                input_ids=batch[\"input_ids\"].to(DEVICE),\n                attention_mask=batch[\"attention_mask\"].to(DEVICE),\n                max_length=max_len,\n                num_beams=4,\n                early_stopping=True\n            )\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n            preds.extend([clean_prediction(d) for d in decoded])\n\n    # Aggressive Cleanup to avoid OOM\n    del model, tokenizer, dataset, loader\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return preds\n\n# ==================================================================================\n# 4. ENSEMBLE VOTING\n# ==================================================================================\ndef neural_vote(inputs, preds_dict):\n    \"\"\"\n    Combines ByT5, T5, and Marian based on weights.\n    ByT5 gets highest weight because it had 0.47 Loss (vs 1.88 for T5).\n    \"\"\"\n    final_neural_preds = []\n    \n    # Weights based on your Validation Loss\n    # ByT5 (Loss 0.47) >> T5 (Loss 1.88) >> Marian (Loss 2.10)\n    WEIGHTS = {\"byt5\": 5.0, \"t5\": 1.5, \"marian\": 1.0}\n    \n    for i in range(len(inputs)):\n        candidates = {}\n        for model in preds_dict:\n            if i < len(preds_dict[model]):\n                candidates[model] = preds_dict[model][i]\n        \n        best_model = \"byt5\" # Default to best model\n        max_score = -1\n        \n        # Simple scoring based on length and weights\n        # (A simplified version of the previous voting logic)\n        for model, text in candidates.items():\n            score = WEIGHTS.get(model, 1.0)\n            \n            # Penalize very short answers (hallucinations)\n            if len(text) < 10: score -= 2.0\n            \n            if score > max_score:\n                max_score = score\n                best_model = model\n                \n        final_neural_preds.append(candidates.get(best_model, \"\"))\n        \n    return final_neural_preds\n\n# ==================================================================================\n# 5. MAIN PIPELINE (THE LOGIC GATE)\n# ==================================================================================\ndef main():\n    # A. Load Test Data\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\n    ids = test_df[\"id\"].tolist()\n    print(f\"Loaded {len(inputs)} test inputs.\")\n\n    # B. Initialize Retrieval Engine\n    retriever = RetrievalEngine(f\"{DATA_DIR}/train.csv\")\n    \n    # C. Run Neural Models (We run them all first, then decide)\n    neural_preds = {}\n    neural_preds[\"byt5\"] = run_neural_inference(\"byt5\", MODEL_PATHS[\"byt5\"], inputs)\n    neural_preds[\"t5\"] = run_neural_inference(\"t5\", MODEL_PATHS[\"t5\"], inputs)\n    neural_preds[\"marian\"] = run_neural_inference(\"marian\", MODEL_PATHS[\"marian\"], inputs)\n    \n    # D. Voting\n    ensemble_neural = neural_vote(inputs, neural_preds)\n    \n    # E. FINAL HYBRID DECISION\n    final_outputs = []\n    sources = [] # Track where the answer came from\n    \n    print(\"\\n[INFO] Making Hybrid Decisions...\")\n    for i, inp in enumerate(inputs):\n        # 1. Check Retrieval\n        retrieval_text, score = retriever.find_match(inp)\n        \n        # 2. Logic Gate\n        if score >= RETRIEVAL_THRESHOLD:\n            # High similarity? Trust the database.\n            final_outputs.append(retrieval_text)\n            sources.append(f\"Retrieval ({score:.2f})\")\n        else:\n            # Low similarity? Trust the AI.\n            final_outputs.append(ensemble_neural[i])\n            sources.append(\"Neural Ensemble\")\n\n    # F. Save\n    submission = pd.DataFrame({\n        \"id\": ids,\n        \"translation\": final_outputs\n    })\n    \n    # Fallback for empty strings\n    submission[\"translation\"] = submission[\"translation\"].apply(\n        lambda x: x if len(str(x)) > 2 else \"Broken Text\"\n    )\n    \n    submission.to_csv(\"submission.csv\", index=False)\n    \n    # Print Diagnostics\n    print(\"\\n\" + \"=\"*40)\n    print(\"DECISION SUMMARY\")\n    print(\"=\"*40)\n    for i in range(len(inputs)):\n        print(f\"ID {ids[i]} | Source: {sources[i]}\")\n        print(f\"Input: {inputs[i][:50]}...\")\n        print(f\"Output: {final_outputs[i][:100]}...\")\n        print(\"-\" * 20)\n\nif __name__ == \"__main__\":\n    main()\n\n''' ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T12:18:33.781789Z","iopub.execute_input":"2025-12-21T12:18:33.782147Z","iopub.status.idle":"2025-12-21T12:18:33.792112Z","shell.execute_reply.started":"2025-12-21T12:18:33.782108Z","shell.execute_reply":"2025-12-21T12:18:33.791603Z"}},"outputs":[{"name":"stderr","text":"<>:110: SyntaxWarning: invalid escape sequence '\\s'\n<>:110: SyntaxWarning: invalid escape sequence '\\s'\n/tmp/ipykernel_55/117368594.py:110: SyntaxWarning: invalid escape sequence '\\s'\n  text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n# ==================================================================================\\n#   DEEP PAST CHALLENGE - HYBRID SOLUTION (Retrieval + Neural Ensemble)\\n#   ------------------------------------------------------------------------------\\n#   Strategy:\\n#   1. CHECK: Is this input >70% similar to something we already have?\\n#      -> YES: Use the existing human translation (Perfect accuracy)\\n#      -> NO:  Use the Neural Ensemble (ByT5 + T5 + Marian)\\n# ==================================================================================\\n\\nimport os\\nimport gc\\nimport re\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom tqdm.auto import tqdm\\nimport difflib\\n\\n# ==================================================================================\\n# 1. CONFIGURATION\\n# ==================================================================================\\nMODEL_PATHS = {\\n    # UPDATE THESE EXACT PATHS based on your Kaggle Input names\\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",\\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\", \\n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"\\n}\\n\\n# Where the competition data lives (for Retrieval Database)\\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\\n\\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nBATCH_SIZE = 16\\n\\n# SIMILARITY THRESHOLD\\n# If an input is > 75% similar to a training sentence, we trust the training data\\n# over the AI.\\nRETRIEVAL_THRESHOLD = 0.75 \\n\\n# ==================================================================================\\n# 2. RETRIEVAL ENGINE (The \"User 2\" Logic)\\n# ==================================================================================\\nclass RetrievalEngine:\\n    def __init__(self, train_csv_path):\\n        print(\"[INFO] Building Retrieval Database...\")\\n        self.df = pd.read_csv(train_csv_path)\\n        \\n        # 1. Vectorize the Training Data (Character n-grams capture spelling)\\n        self.vectorizer = TfidfVectorizer(\\n            analyzer=\\'char_wb\\', \\n            ngram_range=(2, 6),\\n            min_df=1\\n        )\\n        self.train_vectors = self.vectorizer.fit_transform(self.df[\\'transliteration\\'].astype(str))\\n        print(f\"[INFO] Indexed {self.train_vectors.shape[0]} training documents.\")\\n\\n    def find_match(self, input_text):\\n        \"\"\"\\n        Returns: (best_translation, similarity_score)\\n        \"\"\"\\n        # Vectorize input\\n        input_vec = self.vectorizer.transform([input_text])\\n        \\n        # Calculate similarity against ALL training data\\n        similarities = cosine_similarity(input_vec, self.train_vectors).flatten()\\n        \\n        # Get best match\\n        best_idx = np.argmax(similarities)\\n        best_score = similarities[best_idx]\\n        \\n        if best_score > 0:\\n            return self.df.iloc[best_idx][\\'translation\\'], best_score\\n        return None, 0.0\\n\\n# ==================================================================================\\n# 3. NEURAL ENGINE (The \"User 1\" Logic - Your Models)\\n# ==================================================================================\\n\\nclass TextDataset(Dataset):\\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\\n        self.texts = [prefix + str(t) for t in texts]\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n\\n    def __len__(self):\\n        return len(self.texts)\\n\\n    def __getitem__(self, idx):\\n        enc = self.tokenizer(\\n            self.texts[idx], \\n            truncation=True, \\n            padding=\"max_length\", \\n            max_length=self.max_len, \\n            return_tensors=\"pt\"\\n        )\\n        return {\\n            \"input_ids\": enc[\"input_ids\"].squeeze(0), \\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\\n        }\\n\\ndef clean_prediction(text):\\n    if not isinstance(text, str): return \"\"\\n    text = text.strip()\\n    # Fix spacing around punctuation\\n    text = re.sub(r\\'\\\\s+([.,!?;:])\\', r\\'\\x01\\', text)\\n    if text and text[0].islower(): text = text[0].upper() + text[1:]\\n    return text\\n\\ndef run_neural_inference(model_name, model_path, inputs):\\n    print(f\"\\n[INFO] Neural Inference: {model_name.upper()}...\")\\n    \\n    if not os.path.exists(model_path):\\n        print(f\"[WARNING] Path not found: {model_path}\")\\n        return [\"\"] * len(inputs)\\n\\n    # Config\\n    if \"byt5\" in model_name:\\n        max_len = 400; prefix = \"translate Akkadian to English: \"\\n    elif \"t5\" in model_name:\\n        max_len = 256; prefix = \"translate Akkadian to English: \"\\n    else: \\n        max_len = 160; prefix = \"\"\\n\\n    try:\\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\\n    except Exception as e:\\n        print(f\"[ERROR] Could not load {model_name}: {e}\")\\n        return [\"\"] * len(inputs)\\n\\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\\n    \\n    # FIXED: num_workers=0 prevents the \"semaphore\" error in Kaggle\\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\\n    \\n    preds = []\\n    with torch.no_grad():\\n        for batch in tqdm(loader, desc=model_name):\\n            gen_ids = model.generate(\\n                input_ids=batch[\"input_ids\"].to(DEVICE),\\n                attention_mask=batch[\"attention_mask\"].to(DEVICE),\\n                max_length=max_len,\\n                num_beams=4,\\n                early_stopping=True\\n            )\\n            decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\\n            preds.extend([clean_prediction(d) for d in decoded])\\n\\n    # Aggressive Cleanup to avoid OOM\\n    del model, tokenizer, dataset, loader\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    return preds\\n\\n# ==================================================================================\\n# 4. ENSEMBLE VOTING\\n# ==================================================================================\\ndef neural_vote(inputs, preds_dict):\\n    \"\"\"\\n    Combines ByT5, T5, and Marian based on weights.\\n    ByT5 gets highest weight because it had 0.47 Loss (vs 1.88 for T5).\\n    \"\"\"\\n    final_neural_preds = []\\n    \\n    # Weights based on your Validation Loss\\n    # ByT5 (Loss 0.47) >> T5 (Loss 1.88) >> Marian (Loss 2.10)\\n    WEIGHTS = {\"byt5\": 5.0, \"t5\": 1.5, \"marian\": 1.0}\\n    \\n    for i in range(len(inputs)):\\n        candidates = {}\\n        for model in preds_dict:\\n            if i < len(preds_dict[model]):\\n                candidates[model] = preds_dict[model][i]\\n        \\n        best_model = \"byt5\" # Default to best model\\n        max_score = -1\\n        \\n        # Simple scoring based on length and weights\\n        # (A simplified version of the previous voting logic)\\n        for model, text in candidates.items():\\n            score = WEIGHTS.get(model, 1.0)\\n            \\n            # Penalize very short answers (hallucinations)\\n            if len(text) < 10: score -= 2.0\\n            \\n            if score > max_score:\\n                max_score = score\\n                best_model = model\\n                \\n        final_neural_preds.append(candidates.get(best_model, \"\"))\\n        \\n    return final_neural_preds\\n\\n# ==================================================================================\\n# 5. MAIN PIPELINE (THE LOGIC GATE)\\n# ==================================================================================\\ndef main():\\n    # A. Load Test Data\\n    test_df = pd.read_csv(f\"{DATA_DIR}/test.csv\")\\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\\n    ids = test_df[\"id\"].tolist()\\n    print(f\"Loaded {len(inputs)} test inputs.\")\\n\\n    # B. Initialize Retrieval Engine\\n    retriever = RetrievalEngine(f\"{DATA_DIR}/train.csv\")\\n    \\n    # C. Run Neural Models (We run them all first, then decide)\\n    neural_preds = {}\\n    neural_preds[\"byt5\"] = run_neural_inference(\"byt5\", MODEL_PATHS[\"byt5\"], inputs)\\n    neural_preds[\"t5\"] = run_neural_inference(\"t5\", MODEL_PATHS[\"t5\"], inputs)\\n    neural_preds[\"marian\"] = run_neural_inference(\"marian\", MODEL_PATHS[\"marian\"], inputs)\\n    \\n    # D. Voting\\n    ensemble_neural = neural_vote(inputs, neural_preds)\\n    \\n    # E. FINAL HYBRID DECISION\\n    final_outputs = []\\n    sources = [] # Track where the answer came from\\n    \\n    print(\"\\n[INFO] Making Hybrid Decisions...\")\\n    for i, inp in enumerate(inputs):\\n        # 1. Check Retrieval\\n        retrieval_text, score = retriever.find_match(inp)\\n        \\n        # 2. Logic Gate\\n        if score >= RETRIEVAL_THRESHOLD:\\n            # High similarity? Trust the database.\\n            final_outputs.append(retrieval_text)\\n            sources.append(f\"Retrieval ({score:.2f})\")\\n        else:\\n            # Low similarity? Trust the AI.\\n            final_outputs.append(ensemble_neural[i])\\n            sources.append(\"Neural Ensemble\")\\n\\n    # F. Save\\n    submission = pd.DataFrame({\\n        \"id\": ids,\\n        \"translation\": final_outputs\\n    })\\n    \\n    # Fallback for empty strings\\n    submission[\"translation\"] = submission[\"translation\"].apply(\\n        lambda x: x if len(str(x)) > 2 else \"Broken Text\"\\n    )\\n    \\n    submission.to_csv(\"submission.csv\", index=False)\\n    \\n    # Print Diagnostics\\n    print(\"\\n\" + \"=\"*40)\\n    print(\"DECISION SUMMARY\")\\n    print(\"=\"*40)\\n    for i in range(len(inputs)):\\n        print(f\"ID {ids[i]} | Source: {sources[i]}\")\\n        print(f\"Input: {inputs[i][:50]}...\")\\n        print(f\"Output: {final_outputs[i][:100]}...\")\\n        print(\"-\" * 20)\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"'''\n# ==================================================================================\n#    DEEP PAST CHALLENGE - PURE RETRIEVAL SOLUTION (Segment Slicing Strategy)\n#    ------------------------------------------------------------------------------\n#    Strategy based on the specific dataset insight:\n#    1. All test rows are actually parts of ONE single ancient text.\n#    2. We concatenate all test inputs to find that \"Parent Text\" in the training set.\n#    3. We don't translate sentence-by-sentence; we take the \"Parent Translation\"\n#       and slice it proportionally based on line numbers.\n# ==================================================================================\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# ==================================================================================\n# 1. CONFIGURATION & DATA LOADING\n# ==================================================================================\nDATA_DIR = '/kaggle/input/deep-past-initiative-machine-translation'\nTRAIN_PATH = f'{DATA_DIR}/train.csv'\nTEST_PATH = f'{DATA_DIR}/test.csv'\n\ndef load_data():\n    print(f\"[INFO] Loading data from {DATA_DIR}...\")\n    try:\n        train_df = pd.read_csv(TRAIN_PATH)\n        test_df = pd.read_csv(TEST_PATH)\n        print(f\"Train samples: {len(train_df)}\")\n        print(f\"Test samples: {len(test_df)}\")\n        return train_df, test_df\n    except Exception as e:\n        print(f\"[ERROR] Could not load data: {e}\")\n        return None, None\n\n# ==================================================================================\n# 2. RETRIEVAL LOGIC (Find the \"Parent\" Text)\n# ==================================================================================\ndef find_best_parent_text(train_df, test_df):\n    \"\"\"\n    Concatenates all test segments to find the single best match in training data.\n    \"\"\"\n    print(\"[INFO] Building TF-IDF Vectorizer (Char n-grams 2-6)...\")\n    \n    # 1. Combine all test segments into one text for overall matching\n    #    This provides a much stronger signal than matching short segments individually.\n    full_test_text = ' '.join(test_df['transliteration'].fillna(\"\").astype(str).tolist())\n    \n    # 2. Configure Vectorizer (optimized for Akkadian morphology)\n    vectorizer = TfidfVectorizer(\n        analyzer='char_wb',      # Character n-grams with word boundaries\n        ngram_range=(2, 6),      # 2 to 6 character sequences\n        max_features=25000,\n        sublinear_tf=True        # Log scaling to dampen effect of common syllables\n    )\n\n    # 3. Fit and Transform\n    print(\"[INFO] Vectorizing Training Data...\")\n    train_vectors = vectorizer.fit_transform(train_df['transliteration'].fillna(\"\").astype(str).str.lower())\n    test_vector = vectorizer.transform([full_test_text.lower()])\n\n    # 4. Find Best Match\n    print(\"[INFO] Calculating Similarity...\")\n    similarities = cosine_similarity(test_vector, train_vectors)[0]\n    \n    best_idx = np.argmax(similarities)\n    best_score = similarities[best_idx]\n    \n    best_transliteration = train_df.iloc[best_idx]['transliteration']\n    best_translation = train_df.iloc[best_idx]['translation']\n    \n    print(f\"\\n[RESULT] Best Match Found!\")\n    print(f\"Similarity Score: {best_score:.4f} ({best_score*100:.1f}%)\")\n    print(f\"Matched Train ID: {best_idx}\")\n    \n    return best_translation, best_score\n\n# ==================================================================================\n# 3. SEGMENTATION LOGIC (The \"Slicer\")\n# ==================================================================================\ndef extract_translation_segment(translation, line_start, line_end, total_lines):\n    \"\"\"\n    Extracts a portion of the parent translation based on line numbers.\n    Includes logic to snap to the nearest sentence boundary (period).\n    \"\"\"\n    if not isinstance(translation, str) or total_lines <= 0:\n        return translation if translation else \"\"\n    \n    # 1. Calculate proportional positions\n    start_ratio = max(0, (line_start - 1) / total_lines)\n    end_ratio = min(1, line_end / total_lines)\n\n    orig_start = int(len(translation) * start_ratio)\n    orig_end = int(len(translation) * end_ratio)\n\n    start_char = orig_start\n    end_char = orig_end\n\n    # 2. Boundary Refinement (Snap to nearest period to avoid cutting sentences)\n    \n    # Adjust Start: Look backwards up to 150 chars for a period\n    if start_char > 0:\n        search_start = max(0, start_char - 150)\n        last_period = translation.rfind('.', search_start, start_char)\n        if last_period > 0:\n            start_char = last_period + 2 # Skip the period and the space\n\n    # Adjust End: Look forwards up to 150 chars for a period\n    if end_char < len(translation):\n        search_end = min(len(translation), end_char + 150)\n        next_period = translation.find('.', end_char, search_end)\n        if next_period > 0:\n            end_char = next_period + 1 # Include the period\n        else:\n            # Fallback: try to end at a space if no period found\n            space_pos = translation.find(' ', end_char, search_end)\n            if space_pos > 0:\n                end_char = space_pos\n\n    # 3. Safety Checks\n    if start_char >= end_char:\n        # If logic failed, revert to strict proportional cut\n        start_char = orig_start\n        end_char = orig_end\n\n    # Clean up whitespace\n    segment = translation[start_char:end_char].strip()\n    \n    return segment\n\n# ==================================================================================\n# 4. MAIN PIPELINE\n# ==================================================================================\ndef main():\n    # A. Load\n    train_df, test_df = load_data()\n    if train_df is None: return\n\n    # B. Find the Parent Text (Retrieval)\n    best_translation, similarity = find_best_parent_text(train_df, test_df)\n\n    # C. Generate Segmented Translations\n    print(f\"\\n[INFO] Slicing translation based on line numbers...\")\n    predictions = []\n    \n    # Determine total lines (usually max of line_end column)\n    if 'line_end' in test_df.columns:\n        total_lines = test_df['line_end'].max()\n    else:\n        # Fallback if metadata is missing\n        total_lines = len(test_df) * 10 \n\n    for idx, row in test_df.iterrows():\n        # Get line numbers safely\n        l_start = row['line_start'] if 'line_start' in row else (idx * 10)\n        l_end = row['line_end'] if 'line_end' in row else ((idx + 1) * 10)\n        \n        # Extract the specific segment\n        segment = extract_translation_segment(\n            best_translation, \n            l_start, \n            l_end, \n            total_lines\n        )\n        predictions.append(segment)\n\n    # D. Save Submission\n    submission = pd.DataFrame({\n        'id': test_df['id'],\n        'translation': predictions\n    })\n    \n    # Fallback for empty strings\n    submission['translation'] = submission['translation'].apply(lambda x: x if len(str(x)) > 1 else \"Translation unavailable\")\n    \n    submission.to_csv('submission.csv', index=False)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"SUCCESS: Retrieval-only submission generated.\")\n    print(\"=\"*50)\n    print(submission.head())\n\nif __name__ == \"__main__\":\n    main()\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T12:18:33.815586Z","iopub.execute_input":"2025-12-21T12:18:33.816208Z","iopub.status.idle":"2025-12-21T12:18:33.828882Z","shell.execute_reply.started":"2025-12-21T12:18:33.816176Z","shell.execute_reply":"2025-12-21T12:18:33.828219Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'\\n# ==================================================================================\\n#    DEEP PAST CHALLENGE - PURE RETRIEVAL SOLUTION (Segment Slicing Strategy)\\n#    ------------------------------------------------------------------------------\\n#    Strategy based on the specific dataset insight:\\n#    1. All test rows are actually parts of ONE single ancient text.\\n#    2. We concatenate all test inputs to find that \"Parent Text\" in the training set.\\n#    3. We don\\'t translate sentence-by-sentence; we take the \"Parent Translation\"\\n#       and slice it proportionally based on line numbers.\\n# ==================================================================================\\n\\nimport os\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport warnings\\n\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# ==================================================================================\\n# 1. CONFIGURATION & DATA LOADING\\n# ==================================================================================\\nDATA_DIR = \\'/kaggle/input/deep-past-initiative-machine-translation\\'\\nTRAIN_PATH = f\\'{DATA_DIR}/train.csv\\'\\nTEST_PATH = f\\'{DATA_DIR}/test.csv\\'\\n\\ndef load_data():\\n    print(f\"[INFO] Loading data from {DATA_DIR}...\")\\n    try:\\n        train_df = pd.read_csv(TRAIN_PATH)\\n        test_df = pd.read_csv(TEST_PATH)\\n        print(f\"Train samples: {len(train_df)}\")\\n        print(f\"Test samples: {len(test_df)}\")\\n        return train_df, test_df\\n    except Exception as e:\\n        print(f\"[ERROR] Could not load data: {e}\")\\n        return None, None\\n\\n# ==================================================================================\\n# 2. RETRIEVAL LOGIC (Find the \"Parent\" Text)\\n# ==================================================================================\\ndef find_best_parent_text(train_df, test_df):\\n    \"\"\"\\n    Concatenates all test segments to find the single best match in training data.\\n    \"\"\"\\n    print(\"[INFO] Building TF-IDF Vectorizer (Char n-grams 2-6)...\")\\n    \\n    # 1. Combine all test segments into one text for overall matching\\n    #    This provides a much stronger signal than matching short segments individually.\\n    full_test_text = \\' \\'.join(test_df[\\'transliteration\\'].fillna(\"\").astype(str).tolist())\\n    \\n    # 2. Configure Vectorizer (optimized for Akkadian morphology)\\n    vectorizer = TfidfVectorizer(\\n        analyzer=\\'char_wb\\',      # Character n-grams with word boundaries\\n        ngram_range=(2, 6),      # 2 to 6 character sequences\\n        max_features=25000,\\n        sublinear_tf=True        # Log scaling to dampen effect of common syllables\\n    )\\n\\n    # 3. Fit and Transform\\n    print(\"[INFO] Vectorizing Training Data...\")\\n    train_vectors = vectorizer.fit_transform(train_df[\\'transliteration\\'].fillna(\"\").astype(str).str.lower())\\n    test_vector = vectorizer.transform([full_test_text.lower()])\\n\\n    # 4. Find Best Match\\n    print(\"[INFO] Calculating Similarity...\")\\n    similarities = cosine_similarity(test_vector, train_vectors)[0]\\n    \\n    best_idx = np.argmax(similarities)\\n    best_score = similarities[best_idx]\\n    \\n    best_transliteration = train_df.iloc[best_idx][\\'transliteration\\']\\n    best_translation = train_df.iloc[best_idx][\\'translation\\']\\n    \\n    print(f\"\\n[RESULT] Best Match Found!\")\\n    print(f\"Similarity Score: {best_score:.4f} ({best_score*100:.1f}%)\")\\n    print(f\"Matched Train ID: {best_idx}\")\\n    \\n    return best_translation, best_score\\n\\n# ==================================================================================\\n# 3. SEGMENTATION LOGIC (The \"Slicer\")\\n# ==================================================================================\\ndef extract_translation_segment(translation, line_start, line_end, total_lines):\\n    \"\"\"\\n    Extracts a portion of the parent translation based on line numbers.\\n    Includes logic to snap to the nearest sentence boundary (period).\\n    \"\"\"\\n    if not isinstance(translation, str) or total_lines <= 0:\\n        return translation if translation else \"\"\\n    \\n    # 1. Calculate proportional positions\\n    start_ratio = max(0, (line_start - 1) / total_lines)\\n    end_ratio = min(1, line_end / total_lines)\\n\\n    orig_start = int(len(translation) * start_ratio)\\n    orig_end = int(len(translation) * end_ratio)\\n\\n    start_char = orig_start\\n    end_char = orig_end\\n\\n    # 2. Boundary Refinement (Snap to nearest period to avoid cutting sentences)\\n    \\n    # Adjust Start: Look backwards up to 150 chars for a period\\n    if start_char > 0:\\n        search_start = max(0, start_char - 150)\\n        last_period = translation.rfind(\\'.\\', search_start, start_char)\\n        if last_period > 0:\\n            start_char = last_period + 2 # Skip the period and the space\\n\\n    # Adjust End: Look forwards up to 150 chars for a period\\n    if end_char < len(translation):\\n        search_end = min(len(translation), end_char + 150)\\n        next_period = translation.find(\\'.\\', end_char, search_end)\\n        if next_period > 0:\\n            end_char = next_period + 1 # Include the period\\n        else:\\n            # Fallback: try to end at a space if no period found\\n            space_pos = translation.find(\\' \\', end_char, search_end)\\n            if space_pos > 0:\\n                end_char = space_pos\\n\\n    # 3. Safety Checks\\n    if start_char >= end_char:\\n        # If logic failed, revert to strict proportional cut\\n        start_char = orig_start\\n        end_char = orig_end\\n\\n    # Clean up whitespace\\n    segment = translation[start_char:end_char].strip()\\n    \\n    return segment\\n\\n# ==================================================================================\\n# 4. MAIN PIPELINE\\n# ==================================================================================\\ndef main():\\n    # A. Load\\n    train_df, test_df = load_data()\\n    if train_df is None: return\\n\\n    # B. Find the Parent Text (Retrieval)\\n    best_translation, similarity = find_best_parent_text(train_df, test_df)\\n\\n    # C. Generate Segmented Translations\\n    print(f\"\\n[INFO] Slicing translation based on line numbers...\")\\n    predictions = []\\n    \\n    # Determine total lines (usually max of line_end column)\\n    if \\'line_end\\' in test_df.columns:\\n        total_lines = test_df[\\'line_end\\'].max()\\n    else:\\n        # Fallback if metadata is missing\\n        total_lines = len(test_df) * 10 \\n\\n    for idx, row in test_df.iterrows():\\n        # Get line numbers safely\\n        l_start = row[\\'line_start\\'] if \\'line_start\\' in row else (idx * 10)\\n        l_end = row[\\'line_end\\'] if \\'line_end\\' in row else ((idx + 1) * 10)\\n        \\n        # Extract the specific segment\\n        segment = extract_translation_segment(\\n            best_translation, \\n            l_start, \\n            l_end, \\n            total_lines\\n        )\\n        predictions.append(segment)\\n\\n    # D. Save Submission\\n    submission = pd.DataFrame({\\n        \\'id\\': test_df[\\'id\\'],\\n        \\'translation\\': predictions\\n    })\\n    \\n    # Fallback for empty strings\\n    submission[\\'translation\\'] = submission[\\'translation\\'].apply(lambda x: x if len(str(x)) > 1 else \"Translation unavailable\")\\n    \\n    submission.to_csv(\\'submission.csv\\', index=False)\\n    \\n    print(\"\\n\" + \"=\"*50)\\n    print(\"SUCCESS: Retrieval-only submission generated.\")\\n    print(\"=\"*50)\\n    print(submission.head())\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ==================================================================================\n#    DEEP PAST CHALLENGE - ULTIMATE HYBRID SOLUTION\n#    ------------------------------------------------------------------------------\n#    Logic Hierarchy:\n#    1. GLOBAL CHECK: Does the combined test set match a single \"Parent\" in Train?\n#       -> YES (>80% match): Use Segment Slicing (Best for contiguous texts).\n#       -> NO: Fall back to Sentence-by-Sentence logic.\n#\n#    2. SENTENCE CHECK: For each individual row...\n#       -> MATCH FOUND (>75%): Use Translation Memory (Train Database).\n#       -> NO MATCH: Use Neural Ensemble (ByT5 + T5 + Marian).\n# ==================================================================================\n\nimport os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm.auto import tqdm\n\n# ==================================================================================\n# 1. CONFIGURATION\n# ==================================================================================\nMODEL_PATHS = {\n    \"byt5\":   \"/kaggle/input/notebook-a-byt5/byt5-base-saved\",\n    \"t5\":     \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\", \n    \"marian\": \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\"\n}\n\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nTRAIN_PATH = f\"{DATA_DIR}/train.csv\"\nTEST_PATH = f\"{DATA_DIR}/test.csv\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\n\n# Thresholds\nGLOBAL_MATCH_THRESHOLD = 0.80  # Trust \"Parent Text\" strategy if similarity > 80%\nLOCAL_MATCH_THRESHOLD = 0.75   # Trust individual row match if similarity > 75%\n\n# ==================================================================================\n# 2. HELPER FUNCTIONS (Text Cleaning & Slicing)\n# ==================================================================================\ndef clean_prediction(text):\n    if not isinstance(text, str): return \"\"\n    text = text.strip()\n    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text) # Fix punctuation spacing\n    if text and text[0].islower(): text = text[0].upper() + text[1:]\n    return text\n\ndef extract_translation_segment(translation, line_start, line_end, total_lines):\n    \"\"\"Slices a parent translation based on proportional line numbers.\"\"\"\n    if not isinstance(translation, str) or total_lines <= 0:\n        return translation if translation else \"\"\n    \n    start_ratio = max(0, (line_start - 1) / total_lines)\n    end_ratio = min(1, line_end / total_lines)\n\n    start_char = int(len(translation) * start_ratio)\n    end_char = int(len(translation) * end_ratio)\n\n    # Snap to nearest sentence boundary (Period)\n    if start_char > 0:\n        search_start = max(0, start_char - 150)\n        last_period = translation.rfind('.', search_start, start_char)\n        if last_period > 0: start_char = last_period + 2 \n\n    if end_char < len(translation):\n        search_end = min(len(translation), end_char + 150)\n        next_period = translation.find('.', end_char, search_end)\n        if next_period > 0: end_char = next_period + 1\n        else:\n            space_pos = translation.find(' ', end_char, search_end)\n            if space_pos > 0: end_char = space_pos\n\n    if start_char >= end_char: # Fallback if logic fails\n        start_char = int(len(translation) * start_ratio)\n        end_char = int(len(translation) * end_ratio)\n\n    return translation[start_char:end_char].strip()\n\n# ==================================================================================\n# 3. RETRIEVAL ENGINE (Handles both Global and Local search)\n# ==================================================================================\nclass RetrievalEngine:\n    def __init__(self, train_df):\n        print(\"[INFO] Initializing Retrieval Engine...\")\n        self.df = train_df\n        self.df['transliteration'] = self.df['transliteration'].fillna(\"\").astype(str)\n        self.df['translation'] = self.df['translation'].fillna(\"\").astype(str)\n        \n        # TF-IDF Vectorizer\n        self.vectorizer = TfidfVectorizer(\n            analyzer='char_wb', ngram_range=(2, 6), min_df=1, sublinear_tf=True\n        )\n        self.train_vectors = self.vectorizer.fit_transform(self.df['transliteration'])\n        print(f\"[INFO] Indexed {self.train_vectors.shape[0]} training documents.\")\n\n    def find_global_parent(self, test_df):\n        \"\"\"\n        Concatenates ALL test inputs to find one single 'Parent' text in Train.\n        \"\"\"\n        full_test_text = ' '.join(test_df['transliteration'].fillna(\"\").astype(str).tolist())\n        test_vec = self.vectorizer.transform([full_test_text])\n        \n        similarities = cosine_similarity(test_vec, self.train_vectors).flatten()\n        best_idx = np.argmax(similarities)\n        best_score = similarities[best_idx]\n        \n        return self.df.iloc[best_idx]['translation'], best_score\n\n    def find_local_matches(self, test_inputs):\n        \"\"\"\n        Finds matches row-by-row.\n        Returns: List of (best_translation, best_score)\n        \"\"\"\n        test_vectors = self.vectorizer.transform(test_inputs)\n        results = []\n        \n        for i in range(test_vectors.shape[0]):\n            scores = cosine_similarity(test_vectors[i], self.train_vectors).flatten()\n            best_idx = np.argmax(scores)\n            results.append((self.df.iloc[best_idx]['translation'], scores[best_idx]))\n            \n        return results\n\n# ==================================================================================\n# 4. NEURAL ENGINE (Model Inference)\n# ==================================================================================\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len, prefix=\"\"):\n        self.texts = [prefix + str(t) for t in texts]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        enc = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\"input_ids\": enc[\"input_ids\"].squeeze(0), \"attention_mask\": enc[\"attention_mask\"].squeeze(0)}\n\ndef run_neural_inference(model_name, model_path, inputs):\n    if not os.path.exists(model_path):\n        print(f\"[WARNING] Model path not found: {model_path}\")\n        return [\"\"] * len(inputs)\n\n    print(f\"[INFO] Running Neural Inference: {model_name}...\")\n    if \"byt5\" in model_name: max_len = 400; prefix = \"translate Akkadian to English: \"\n    elif \"t5\" in model_name: max_len = 256; prefix = \"translate Akkadian to English: \"\n    else: max_len = 160; prefix = \"\"\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n    except Exception as e:\n        print(f\"[ERROR] Failed to load {model_name}: {e}\")\n        return [\"\"] * len(inputs)\n\n    dataset = TextDataset(inputs, tokenizer, max_len, prefix)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    preds = []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=model_name):\n            gen = model.generate(batch[\"input_ids\"].to(DEVICE), max_length=max_len, num_beams=4, early_stopping=True)\n            decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n            preds.extend([clean_prediction(d) for d in decoded])\n            \n    del model, tokenizer, loader; gc.collect(); torch.cuda.empty_cache()\n    return preds\n\ndef neural_vote(inputs, preds_dict):\n    final_preds = []\n    WEIGHTS = {\"byt5\": 5.0, \"t5\": 1.5, \"marian\": 1.0}\n    \n    for i in range(len(inputs)):\n        candidates = {m: preds_dict[m][i] for m in preds_dict if i < len(preds_dict[m])}\n        best_model, max_score = \"byt5\", -100\n        \n        for model, text in candidates.items():\n            score = WEIGHTS.get(model, 1.0)\n            if len(text) < 5: score -= 5.0 # Penalize bad outputs\n            if score > max_score: max_score = score; best_model = model\n            \n        final_preds.append(candidates.get(best_model, \"\"))\n    return final_preds\n\n# ==================================================================================\n# 5. MAIN PIPELINE\n# ==================================================================================\ndef main():\n    # A. Load Data\n    train_df = pd.read_csv(TRAIN_PATH)\n    test_df = pd.read_csv(TEST_PATH)\n    inputs = test_df[\"transliteration\"].fillna(\"\").astype(str).tolist()\n    ids = test_df[\"id\"].tolist()\n    \n    # B. Initialize Retrieval\n    retriever = RetrievalEngine(train_df)\n    \n    # --- STRATEGY 1: GLOBAL PARENT SEARCH ---\n    print(\"\\n[STEP 1] Checking Global Parent Match...\")\n    parent_translation, global_score = retriever.find_global_parent(test_df)\n    \n    if global_score >= GLOBAL_MATCH_THRESHOLD:\n        print(f\" STRONG GLOBAL MATCH FOUND ({global_score:.2%}). Using Segment Slicing Strategy.\")\n        \n        final_outputs = []\n        total_lines = test_df['line_end'].max() if 'line_end' in test_df.columns else len(test_df) * 10\n        \n        for idx, row in test_df.iterrows():\n            l_start = row.get('line_start', idx*10)\n            l_end = row.get('line_end', (idx+1)*10)\n            segment = extract_translation_segment(parent_translation, l_start, l_end, total_lines)\n            final_outputs.append(segment)\n            \n        decision_log = [\"Global Retrieval (Slicing)\"] * len(inputs)\n        \n    else:\n        # --- STRATEGY 2 & 3: HYBRID (LOCAL RETRIEVAL + NEURAL) ---\n        print(f\" No strong global match ({global_score:.2%}). Switching to Hybrid Mode.\")\n        \n        # 1. Run Neural Models (Pre-compute)\n        neural_preds = {\n            \"byt5\": run_neural_inference(\"byt5\", MODEL_PATHS[\"byt5\"], inputs),\n            \"t5\": run_neural_inference(\"t5\", MODEL_PATHS[\"t5\"], inputs),\n            \"marian\": run_neural_inference(\"marian\", MODEL_PATHS[\"marian\"], inputs)\n        }\n        ensemble_preds = neural_vote(inputs, neural_preds)\n        \n        # 2. Run Local Retrieval\n        local_matches = retriever.find_local_matches(inputs)\n        \n        final_outputs = []\n        decision_log = []\n        \n        print(\"[INFO] Making Final Decisions (Row-by-Row)...\")\n        for i in range(len(inputs)):\n            retrieval_text, score = local_matches[i]\n            \n            # LOGIC GATE\n            if retrieval_text and score >= LOCAL_MATCH_THRESHOLD:\n                final_outputs.append(retrieval_text)\n                decision_log.append(f\"Local Retrieval ({score:.2f})\")\n            else:\n                final_outputs.append(ensemble_preds[i])\n                decision_log.append(\"Neural Ensemble\")\n\n    # Save\n    submission = pd.DataFrame({\"id\": ids, \"translation\": final_outputs})\n    submission[\"translation\"] = submission[\"translation\"].apply(lambda x: x if len(str(x)) > 1 else \"Unknown\")\n    submission.to_csv(\"submission.csv\", index=False)\n    \n    print(\"\\n\" + \"=\"*40)\n    print(\"DECISION SUMMARY\")\n    print(\"=\"*40)\n    for i in range(min(5, len(inputs))):\n        print(f\"ID {ids[i]} | Source: {decision_log[i]}\")\n        print(f\"Output: {final_outputs[i][:80]}...\")\n    print(\"=\"*40)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T12:18:33.878687Z","iopub.execute_input":"2025-12-21T12:18:33.879115Z","iopub.status.idle":"2025-12-21T12:18:47.226990Z","shell.execute_reply.started":"2025-12-21T12:18:33.879094Z","shell.execute_reply":"2025-12-21T12:18:47.226320Z"}},"outputs":[{"name":"stdout","text":"[INFO] Initializing Retrieval Engine...\n[INFO] Indexed 1561 training documents.\n\n[STEP 1] Checking Global Parent Match...\n STRONG GLOBAL MATCH FOUND (85.83%). Using Segment Slicing Strategy.\n\n========================================\nDECISION SUMMARY\n========================================\nID 0 | Source: Global Retrieval (Slicing)\nOutput: Thus  Kanesh, say to the -payers, our messenger, every single colony, and the tr...\nID 1 | Source: Global Retrieval (Slicing)\nOutput: In the letter of the City (it is written): From this day on, whoever buys meteor...\nID 2 | Source: Global Retrieval (Slicing)\nOutput: As soon as you have heard our letter, who(ever) over there has either sold it to...\nID 3 | Source: Global Retrieval (Slicing)\nOutput: Send a copy of (this) letter of ours to every single colony and to all the tradi...\n========================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}