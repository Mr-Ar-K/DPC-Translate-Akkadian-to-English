{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":14976537,"sourceId":121150,"sourceType":"competition"},{"datasetId":9082937,"sourceId":14236819,"sourceType":"datasetVersion"}],"dockerImageVersionId":31234,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":6059.435801,"end_time":"2025-12-25T12:09:11.716859","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-25T10:28:12.281058","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1fe03852120a40cda6d5c85f4e29fcaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b48bf215217541358d86be12f43b7f3c","placeholder":"​","style":"IPY_MODEL_fcaea7f32ee24eebaebfc0e32266d44a","tabbable":null,"tooltip":null,"value":" 8.15k/? [00:00&lt;00:00, 928kB/s]"}},"226d44a1d8884129a1d1230b2642a786":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_b0c8da6c308b446cb01170a376a943c3","max":77,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39da431ed6e041ef8c50061ff0cb2734","tabbable":null,"tooltip":null,"value":77}},"2bccb998d49d415580aaed49b33a1b22":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_791efc0e593d4599ba993239183a55e3","placeholder":"​","style":"IPY_MODEL_683b28591d2f4363aa74a0a1a44c7a1c","tabbable":null,"tooltip":null,"value":" 77/77 [00:00&lt;00:00, 388.82 examples/s]"}},"2d21f5383a1548d98e116b290031fccd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39da431ed6e041ef8c50061ff0cb2734":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b23edd7473a4b5cbeb0e7453bf9b6bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"427b3335b5374628a175042c3a3382e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2d21f5383a1548d98e116b290031fccd","placeholder":"​","style":"IPY_MODEL_9aed8f291fff48a5a708caf8c20013d8","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"44b7b6316c1c4a4bbc5e3b84e174e79f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_e6a4941ccd41469aa09a0ec5a804af6e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b76718f5d5b1422ab37b370c9cc527ff","tabbable":null,"tooltip":null,"value":1}},"45c5ff13e0804ab38df84c594f5cbfe4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c11ec6efe7cb4722883a279f3eb185ed","placeholder":"​","style":"IPY_MODEL_8846ab600ef243c39c4f1cb71a308f59","tabbable":null,"tooltip":null,"value":"Downloading builder script: "}},"4a7eec71c5664bbd922f48d02fb439ac":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b48b9cb404748bdba5a1554f4687518":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e865f98ee6084edf9b9c63807b52da6b","IPY_MODEL_9f53690cf1d84b5aa3e3929a66799044","IPY_MODEL_f030f089b1124566ab437b62c99b2578"],"layout":"IPY_MODEL_c2b4df78e626401c8a097c7890f1386e","tabbable":null,"tooltip":null}},"6297171298534da8a8ae461611777771":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"67bff04e3ed849f8aadf4b6b7df2b9e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_427b3335b5374628a175042c3a3382e4","IPY_MODEL_44b7b6316c1c4a4bbc5e3b84e174e79f","IPY_MODEL_1fe03852120a40cda6d5c85f4e29fcaf"],"layout":"IPY_MODEL_b8d2e1271f1e4afdb533651d811a2bb8","tabbable":null,"tooltip":null}},"683b28591d2f4363aa74a0a1a44c7a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6c90b63b47dc4de495594cfd1e6519b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"701dd03462844cf0af47a3ab8c452a04":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45c5ff13e0804ab38df84c594f5cbfe4","IPY_MODEL_bc4d79d1c6384c23a736957e90afd304","IPY_MODEL_9d58cd0d03464cd18c653fdec13d87e4"],"layout":"IPY_MODEL_b0bcd130aa8341b1acef83b7cc545dce","tabbable":null,"tooltip":null}},"71affa5ac8894c80959fbb0448728c81":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c324f021e0034d85bcf6459f17a7a752","placeholder":"​","style":"IPY_MODEL_6297171298534da8a8ae461611777771","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"791efc0e593d4599ba993239183a55e3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8846ab600ef243c39c4f1cb71a308f59":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8f98493e151546f2baa629a4edf06a14":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9369db00183844a5b53806dd64a214db":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"969e0b0841ff4999a85c76101abdcc41":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9aed8f291fff48a5a708caf8c20013d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9d58cd0d03464cd18c653fdec13d87e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b789e817126e4af1a184cfae19f2612f","placeholder":"​","style":"IPY_MODEL_3b23edd7473a4b5cbeb0e7453bf9b6bc","tabbable":null,"tooltip":null,"value":" 9.01k/? [00:00&lt;00:00, 914kB/s]"}},"9f53690cf1d84b5aa3e3929a66799044":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_ab64a8242ff94bdfbfceeabac64877cf","max":1452,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0f4b1fc30b24fd1a84d58113003352a","tabbable":null,"tooltip":null,"value":1452}},"a1151781b3e84a738ae66c7ac283ba3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71affa5ac8894c80959fbb0448728c81","IPY_MODEL_226d44a1d8884129a1d1230b2642a786","IPY_MODEL_2bccb998d49d415580aaed49b33a1b22"],"layout":"IPY_MODEL_4a7eec71c5664bbd922f48d02fb439ac","tabbable":null,"tooltip":null}},"a8e873b972004c38ae11c1d76cfe84ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab64a8242ff94bdfbfceeabac64877cf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae69eb96031c4d43bf4c52da5aedb198":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0bcd130aa8341b1acef83b7cc545dce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0c8da6c308b446cb01170a376a943c3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b48bf215217541358d86be12f43b7f3c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b76718f5d5b1422ab37b370c9cc527ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b789e817126e4af1a184cfae19f2612f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8d2e1271f1e4afdb533651d811a2bb8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4d79d1c6384c23a736957e90afd304":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_969e0b0841ff4999a85c76101abdcc41","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8e873b972004c38ae11c1d76cfe84ba","tabbable":null,"tooltip":null,"value":1}},"c11ec6efe7cb4722883a279f3eb185ed":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b4df78e626401c8a097c7890f1386e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c324f021e0034d85bcf6459f17a7a752":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0f4b1fc30b24fd1a84d58113003352a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a4941ccd41469aa09a0ec5a804af6e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e865f98ee6084edf9b9c63807b52da6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ae69eb96031c4d43bf4c52da5aedb198","placeholder":"​","style":"IPY_MODEL_8f98493e151546f2baa629a4edf06a14","tabbable":null,"tooltip":null,"value":"Map: 100%"}},"f030f089b1124566ab437b62c99b2578":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9369db00183844a5b53806dd64a214db","placeholder":"​","style":"IPY_MODEL_6c90b63b47dc4de495594cfd1e6519b0","tabbable":null,"tooltip":null,"value":" 1452/1452 [00:03&lt;00:00, 448.42 examples/s]"}},"fcaea7f32ee24eebaebfc0e32266d44a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"76a4c494","cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:08:08.387988Z","iopub.execute_input":"2026-01-08T08:08:08.388312Z","iopub.status.idle":"2026-01-08T08:08:08.619248Z","shell.execute_reply.started":"2026-01-08T08:08:08.388289Z","shell.execute_reply":"2026-01-08T08:08:08.616178Z"},"papermill":{"duration":0.184686,"end_time":"2025-12-25T10:28:15.031482","exception":false,"start_time":"2025-12-25T10:28:14.846796","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Thu Jan  8 08:08:08 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"id":"853653de","cell_type":"markdown","source":"# A1. Install required libraries","metadata":{"papermill":{"duration":0.006431,"end_time":"2025-12-25T10:28:15.044668","exception":false,"start_time":"2025-12-25T10:28:15.038237","status":"completed"},"tags":[]}},{"id":"964c2326","cell_type":"code","source":"!pip install -q evaluate sacrebleu","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2026-01-08T08:08:08.621507Z","iopub.execute_input":"2026-01-08T08:08:08.621873Z","iopub.status.idle":"2026-01-08T08:08:13.845850Z","shell.execute_reply.started":"2026-01-08T08:08:08.621830Z","shell.execute_reply":"2026-01-08T08:08:13.844989Z"},"papermill":{"duration":4.824313,"end_time":"2025-12-25T10:28:19.875330","exception":false,"start_time":"2025-12-25T10:28:15.051017","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"id":"5dd5f368","cell_type":"markdown","source":"# A2. Imports & config","metadata":{"papermill":{"duration":0.006597,"end_time":"2025-12-25T10:28:19.889807","exception":false,"start_time":"2025-12-25T10:28:19.883210","status":"completed"},"tags":[]}},{"id":"ec80cf0e","cell_type":"code","source":"import os\nimport gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed\n)\nimport evaluate\n\n# Memory/precision safety tweaks (helps avoid OOM on P100/T4)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntry:\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.benchmark = False\n    torch.set_float32_matmul_precision(\"medium\")\nexcept Exception:\n    pass\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:08:13.847199Z","iopub.execute_input":"2026-01-08T08:08:13.847573Z","iopub.status.idle":"2026-01-08T08:08:45.115144Z","shell.execute_reply.started":"2026-01-08T08:08:13.847522Z","shell.execute_reply":"2026-01-08T08:08:45.114538Z"},"papermill":{"duration":33.038891,"end_time":"2025-12-25T10:28:52.935206","exception":false,"start_time":"2025-12-25T10:28:19.896315","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2026-01-08 08:08:28.427385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767859708.621266      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767859708.681230      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767859709.163048      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767859709.163087      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767859709.163090      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767859709.163093      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"id":"b38e3a6c","cell_type":"markdown","source":"# A3. Set constants (DO NOT change yet)","metadata":{"papermill":{"duration":0.006759,"end_time":"2025-12-25T10:28:52.949032","exception":false,"start_time":"2025-12-25T10:28:52.942273","status":"completed"},"tags":[]}},{"id":"cbc9780c","cell_type":"code","source":"MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\nDATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\nOUTPUT_DIR = \"/kaggle/working/byt5-base-saved\"\n\n# ByT5 is character-based. 360-400 provides good coverage without excessive memory\nMAX_LENGTH = 380\nPREFIX = \"translate Akkadian to English: \"\n\n# OOM guard: allow dynamic reduction controlled by env var\ntry:\n    env_max_len = int(os.getenv(\"BYT5_MAX_LENGTH\", \"0\"))\n    if env_max_len >= 280:\n        MAX_LENGTH = env_max_len\n        print(f\"[INFO] MAX_LENGTH overridden by env: {MAX_LENGTH}\")\nexcept Exception:\n    pass","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:08:45.116044Z","iopub.execute_input":"2026-01-08T08:08:45.116661Z","iopub.status.idle":"2026-01-08T08:08:45.121485Z","shell.execute_reply.started":"2026-01-08T08:08:45.116634Z","shell.execute_reply":"2026-01-08T08:08:45.120695Z"},"papermill":{"duration":0.013576,"end_time":"2025-12-25T10:28:52.969057","exception":false,"start_time":"2025-12-25T10:28:52.955481","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"9041d778","cell_type":"markdown","source":"# A4. Data Loading & Cleaning","metadata":{"papermill":{"duration":0.006442,"end_time":"2025-12-25T10:28:52.982062","exception":false,"start_time":"2025-12-25T10:28:52.975620","status":"completed"},"tags":[]}},{"id":"9035f86f","cell_type":"code","source":"SUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n\ndef normalize_subscripts(text: str) -> str:\n    return text.translate(SUBSCRIPT_TRANS)\n\ndef replace_gaps(text, keep_gaps=True):\n    \"\"\"Replace various gap notations with standardized tokens\n    \n    Args:\n        keep_gaps: If True, keeps gap tokens (for test-like data).\n                   If False, removes them (for clean training).\n    \"\"\"\n    if pd.isna(text): \n        return text\n    \n    # Complex gap patterns (order matters)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n\n    # Simple gap patterns\n    text = re.sub(r'xx', '<gap>', text)\n    text = re.sub(r' x ', ' <gap> ', text)\n    text = re.sub(r'……', '<big_gap>', text)\n    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n    text = re.sub(r'…', '<big_gap>', text)\n    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n    \n    # Bracketed gaps\n    text = re.sub(r'\\[\\.\\.\\.+\\]', '<big_gap>', text)\n    text = re.sub(r'\\[x+\\]', '<gap>', text)\n    \n    if not keep_gaps:\n        # Remove gaps for clean training\n        text = re.sub(r'<big_gap>', '', text)\n        text = re.sub(r'<gap>', '', text)\n\n    return text\n\ndef clean_translit(text, keep_gaps=True):\n    \"\"\"Normalize transliteration following competition guidance.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = normalize_subscripts(text)\n    # Apply gap replacement - KEEP gaps for domain matching\n    text = replace_gaps(text, keep_gaps=keep_gaps)\n    # Only remove scribal markers, keep gaps\n    text = re.sub(r\"<<[^>]*>>\", \" \", text)               # errant signs\n    text = re.sub(r\"[˹˺]\", \" \", text)                    # half brackets\n    text = re.sub(r\"\\([^)]*\\)\", \" \", text)             # comments/erasures\n    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)         # determinatives\n    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)            # scribal insertions keep content\n    text = re.sub(r\"[!?/:·]\", \" \", text)                 # scribal punctuation\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef clean_translation(text, has_gaps=False):\n    \"\"\"Clean translation, optionally keeping gap indicators\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    if not has_gaps:\n        text = text.replace(\"…\", \" \")\n    # Keep ... if source has gaps\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\ndef filter_quality(df):\n    df[\"src_len\"] = df[\"transliteration\"].str.split().str.len()\n    df[\"tgt_len\"] = df[\"translation\"].str.split().str.len()\n    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n    df = df[(ratio >= 0.2) & (ratio <= 5)]\n    df = df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n\n# -----------------------------------------------------------------------------\n# ADVANCED DATA ALIGNMENT (Using Sentences_Oare_FirstWord_LinNum.csv)\n# -----------------------------------------------------------------------------\ndef load_sentence_alignment():\n    \"\"\"Load sentence alignment data if available\"\"\"\n    sent_align_path = f\"{DATA_DIR}/Sentences_Oare_FirstWord_LinNum.csv\"\n    if os.path.exists(sent_align_path):\n        print(\"✓ Loading sentence alignment data...\")\n        return pd.read_csv(sent_align_path)\n    else:\n        print(\"⚠️  Sentence alignment file not found, using fallback alignment\")\n        return None\n\ndef align_with_sentence_map(df, sent_map):\n    \"\"\"Use explicit sentence mapping for perfect alignment\n    \n    This function uses the Sentences_Oare_FirstWord_LinNum.csv file which contains:\n    - text_uuid: Document ID (matches oare_id in train.csv)\n    - translation: The English sentence (THIS IS KEY - use it directly!)\n    - first_word_transcription: First word of Akkadian sentence\n    - sentence_obj_in_text: Sentence order within document\n    \"\"\"\n    if sent_map is None:\n        return None\n    \n    print(\"Building aligned dataset using sentence map translations...\")\n    aligned_rows = []\n    \n    # Create lookup for full transliterations from train.csv\n    df['clean_translit_full'] = df['transliteration'].apply(lambda x: clean_translit(str(x), keep_gaps=True))\n    text_lookup = df.set_index('oare_id')['clean_translit_full'].to_dict()\n    \n    # Group by document\n    for text_id, group in sent_map.groupby('text_uuid'):\n        if text_id not in text_lookup:\n            continue\n        \n        full_akkadian = text_lookup[text_id]\n        if len(full_akkadian) < 10:\n            continue\n        \n        # Sort sentences by their order in the text\n        sorted_sents = group.sort_values('sentence_obj_in_text')\n        \n        # Extract translations from the map (these are already sentence-aligned!)\n        map_translations = [str(row.translation).strip() for _, row in sorted_sents.iterrows()]\n        \n        # Try to split the Akkadian text into matching chunks\n        # Strategy: Use gaps and newlines as natural boundaries\n        akkadian_chunks = [s.strip() for s in re.split(r'(?:<big_gap>|<gap>|\\n)+', full_akkadian) if len(s.strip()) > 3]\n        \n        # If chunk counts match, pair them up (high confidence)\n        if len(akkadian_chunks) == len(map_translations):\n            for akk_chunk, eng_sent in zip(akkadian_chunks, map_translations):\n                if len(akk_chunk) > 3 and len(eng_sent) > 3:\n                    aligned_rows.append({\n                        \"transliteration\": akk_chunk,\n                        \"translation\": clean_translation(eng_sent)\n                    })\n        else:\n            # Fallback: If counts don't match, still use map translations\n            # but try to extract Akkadian by first-word matching\n            for _, sent_row in sorted_sents.iterrows():\n                first_word = str(sent_row.first_word_transcription).strip() if hasattr(sent_row, 'first_word_transcription') else \"\"\n                eng_sent = str(sent_row.translation).strip()\n                \n                if len(first_word) > 2 and first_word in full_akkadian:\n                    # Find sentence starting with this word (heuristic)\n                    start_pos = full_akkadian.find(first_word)\n                    if start_pos >= 0:\n                        # Extract until next gap or reasonable length\n                        remaining = full_akkadian[start_pos:start_pos+200]\n                        end_pos = re.search(r'<big_gap>|<gap>|\\n', remaining)\n                        akk_sent = remaining[:end_pos.start()] if end_pos else remaining\n                        \n                        if len(akk_sent) > 5 and len(eng_sent) > 3:\n                            aligned_rows.append({\n                                \"transliteration\": akk_sent.strip(),\n                                \"translation\": clean_translation(eng_sent)\n                            })\n    \n    if aligned_rows:\n        aligned_df = pd.DataFrame(aligned_rows)\n        print(f\"✓ Extracted {len(aligned_df)} sentence pairs from map file\")\n        return aligned_df\n    \n    return None\n\ndef load_and_align_data(filepath):\n    \"\"\"\n    Enhanced alignment with sentence-level mapping support\n    \"\"\"\n    df = pd.read_csv(filepath)\n    print(f\"Raw documents: {len(df)}\")\n    \n    # Try to use sentence alignment map first\n    sent_map = load_sentence_alignment()\n    if sent_map is not None:\n        aligned_df = align_with_sentence_map(df, sent_map)\n        if aligned_df is not None and len(aligned_df) > 100:\n            print(f\"✓ Aligned using sentence map: {len(aligned_df)} examples\")\n            return filter_quality(aligned_df)\n    \n    # Fallback: Original alignment logic\n    aligned_rows = []\n\n    for _, row in df.iterrows():\n        src = clean_translit(row.get(\"transliteration\", \"\"), keep_gaps=True)\n        tgt = clean_translation(row.get(\"translation\", \"\"))\n\n        src_lines = [s.strip() for s in src.split(\"\\n\") if s.strip()]\n        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n\n        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n            for s, t in zip(src_lines, tgt_sents):\n                if len(s) > 3 and len(t) > 3:\n                    aligned_rows.append({\"transliteration\": s, \"translation\": t})\n        else:\n            merged_src = src.replace(\"\\n\", \" \")\n            if len(merged_src) > 3 and len(tgt) > 3:\n                aligned_rows.append({\"transliteration\": merged_src, \"translation\": tgt})\n\n    print(f\"Aligned training examples (pre-filter): {len(aligned_rows)}\")\n    out_df = filter_quality(pd.DataFrame(aligned_rows))\n    print(f\"Aligned training examples (post-filter): {len(out_df)}\")\n    return out_df\n\n# -----------------------------------------------------------------------------\n# MINE PUBLICATIONS.CSV FOR ADDITIONAL TRAINING DATA (OPTIMIZED)\n# -----------------------------------------------------------------------------\nfrom tqdm.auto import tqdm\n\ndef mine_publications_data():\n    \"\"\"Extract translations from publications.csv to augment training data (Optimized)\"\"\"\n    pub_path = f\"{DATA_DIR}/publications.csv\"\n    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n    \n    if not os.path.exists(pub_path) or not os.path.exists(pub_texts_path):\n        print(\"⚠️  Publications data not found, skipping augmentation\")\n        return pd.DataFrame()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"MINING PUBLICATIONS FOR ADDITIONAL TRAINING DATA (FAST MODE)\")\n    print(\"=\"*60)\n    \n    # Load data\n    pubs = pd.read_csv(pub_path)\n    pub_texts = pd.read_csv(pub_texts_path)\n    \n    print(f\"Total publication pages: {len(pubs)}\")\n    \n    # OPTIMIZATION 1: Pre-filter pages that contain keywords\n    # We only care about pages that explicitly mention 'translation' or 'English'\n    # This reduces search space from potentially 216k -> ~10-20k relevant pages\n    # Check which column contains the text content\n    text_col = None\n    for col in ['page_text', 'text', 'content', 'ocr_text']:\n        if col in pubs.columns:\n            text_col = col\n            break\n    \n    if text_col is None:\n        print(\"⚠️  Could not find text column in publications.csv\")\n        return pd.DataFrame()\n    \n    relevant_mask = pubs[text_col].astype(str).str.contains(r'translation|English', case=False, regex=True)\n    pubs_filtered = pubs[relevant_mask].copy()\n    print(f\"Pages with translation keywords: {len(pubs_filtered)}\")\n    \n    if len(pubs_filtered) == 0:\n        print(\"⚠️  No pages contain translation keywords\")\n        return pd.DataFrame()\n    \n    augmented_data = []\n    \n    # OPTIMIZATION 2: Limit texts and use progress bar\n    # We check the top 1500 candidate texts against the filtered pages\n    candidates = pub_texts.head(1500)\n    \n    print(\"Searching for matches...\")\n    for _, pub_text in tqdm(candidates.iterrows(), total=len(candidates), desc=\"Mining\"):\n        text_id = pub_text.get(\"oare_id\") or pub_text.get(\"cdli_id\")\n        translit = pub_text.get(\"transliteration\", \"\")\n        \n        if pd.isna(text_id) or pd.isna(translit) or len(str(translit)) < 10:\n            continue\n        \n        text_id_str = str(text_id)\n        \n        # OPTIMIZATION 3: Vectorized search on specific column only\n        # Much faster than row.astype(str)\n        matches = pubs_filtered[pubs_filtered[text_col].astype(str).str.contains(text_id_str, regex=False)]\n        \n        if not matches.empty:\n            # Take the first match\n            pub_content = str(matches.iloc[0].get(text_col, ''))\n            \n            # Extract translation using regex\n            # Looks for \"Translation: [English Text]\" pattern\n            trans_match = re.search(r'(?:translation|English|translates?)[:\\s]+([A-Z][^.]{20,300}[.!?])', \n                                   pub_content, re.IGNORECASE)\n            \n            if trans_match:\n                translation = trans_match.group(1).strip()\n                augmented_data.append({\n                    \"transliteration\": clean_translit(str(translit), keep_gaps=True),\n                    \"translation\": clean_translation(translation, has_gaps='<gap>' in str(translit))\n                })\n    \n    aug_df = pd.DataFrame(augmented_data)\n    if len(aug_df) > 0:\n        aug_df = filter_quality(aug_df)\n        print(f\"✓ Mined {len(aug_df)} additional training pairs from publications\")\n    else:\n        print(\"⚠️  No additional pairs extracted (try adjusting regex or increasing candidates)\")\n    \n    return aug_df\n\n# Load main training data\ntrain_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n\n# Mine publications for additional translations\nmined_df = mine_publications_data()\n\n# Augment with mined data if available\nif len(mined_df) > 0:\n    train_df = pd.concat([train_df, mined_df], ignore_index=True)\n    train_df = train_df.drop_duplicates(subset=[\"transliteration\", \"translation\"])\n    print(f\"\\n✓ Total training examples after augmentation: {len(train_df)}\")\n\n# Check published texts availability for later use\nprint(\"\\n\" + \"=\"*60)\nprint(\"CHECKING PUBLISHED TEXTS\")\nprint(\"=\"*60)\n\npub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\nif os.path.exists(pub_texts_path):\n    pub_df = pd.read_csv(pub_texts_path)\n    print(f\"Published texts available: {len(pub_df)}\")\n    print(\"Note: Will use these for monolingual pre-training\")\nelse:\n    print(\"⚠️  Published texts not found\")\n\n# Create dataset and split\ndataset = Dataset.from_pandas(train_df)\ndataset = dataset.train_test_split(test_size=0.05, seed=42)\n\nprint(f\"\\nFinal dataset:\")\nprint(f\"  Train: {len(dataset['train'])} examples\")\nprint(f\"  Validation: {len(dataset['test'])} examples\")\n","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:08:45.123665Z","iopub.execute_input":"2026-01-08T08:08:45.123948Z","iopub.status.idle":"2026-01-08T08:09:40.314471Z","shell.execute_reply.started":"2026-01-08T08:08:45.123915Z","shell.execute_reply":"2026-01-08T08:09:40.313446Z"},"papermill":{"duration":0.469755,"end_time":"2025-12-25T10:28:53.458249","exception":false,"start_time":"2025-12-25T10:28:52.988494","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Raw documents: 1561\n✓ Loading sentence alignment data...\nBuilding aligned dataset using sentence map translations...\n✓ Extracted 51 sentence pairs from map file\nAligned training examples (pre-filter): 1561\nAligned training examples (post-filter): 1528\n\n============================================================\nMINING PUBLICATIONS FOR ADDITIONAL TRAINING DATA (FAST MODE)\n============================================================\nTotal publication pages: 216602\nPages with translation keywords: 12500\nSearching for matches...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Mining:   0%|          | 0/1500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc564c056d4d4614a826ef142aac4f31"}},"metadata":{}},{"name":"stdout","text":"⚠️  No additional pairs extracted (try adjusting regex or increasing candidates)\n\n============================================================\nCHECKING PUBLISHED TEXTS\n============================================================\nPublished texts available: 7953\nNote: Will use these for monolingual pre-training\n\nFinal dataset:\n  Train: 1451 examples\n  Validation: 77 examples\n","output_type":"stream"}],"execution_count":5},{"id":"373e0f72","cell_type":"markdown","source":"# A5 . Tokenization","metadata":{"papermill":{"duration":0.006698,"end_time":"2025-12-25T10:28:53.471962","exception":false,"start_time":"2025-12-25T10:28:53.465264","status":"completed"},"tags":[]}},{"id":"af1a0a55","cell_type":"code","source":"print(\"Loading Tokenizer from:\", MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndef preprocess_function(examples):\n    inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n    targets = examples[\"translation\"]\n\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\" # Consistent padding helps training stability\n    )\n    \n    labels = tokenizer(\n        targets, \n        max_length=MAX_LENGTH, \n        truncation=True, \n        padding=\"max_length\"\n    )\n\n    # Replace padding token id with -100 so it's ignored by the loss function\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Process datasets\ntokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ntokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:09:40.315665Z","iopub.execute_input":"2026-01-08T08:09:40.315996Z","iopub.status.idle":"2026-01-08T08:09:44.388152Z","shell.execute_reply.started":"2026-01-08T08:09:40.315966Z","shell.execute_reply":"2026-01-08T08:09:44.387424Z"},"papermill":{"duration":3.528045,"end_time":"2025-12-25T10:28:57.006773","exception":false,"start_time":"2025-12-25T10:28:53.478728","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading Tokenizer from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"999dfb221f974217845a0b5904c1b537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/77 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9abd1a38cb6d47c299cb8c035d75b4aa"}},"metadata":{}}],"execution_count":6},{"id":"680c9278","cell_type":"markdown","source":"# A6. Model Setup","metadata":{"papermill":{"duration":0.007545,"end_time":"2025-12-25T10:28:57.023322","exception":false,"start_time":"2025-12-25T10:28:57.015777","status":"completed"},"tags":[]}},{"id":"447be19d","cell_type":"code","source":"print(\"Loading Model from:\", MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n\n# Data Collator handles dynamic padding during batching\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    label_pad_token_id=-100\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:09:44.389097Z","iopub.execute_input":"2026-01-08T08:09:44.389418Z","iopub.status.idle":"2026-01-08T08:09:45.231488Z","shell.execute_reply.started":"2026-01-08T08:09:44.389394Z","shell.execute_reply":"2026-01-08T08:09:45.230953Z"},"papermill":{"duration":1.398574,"end_time":"2025-12-25T10:28:58.434786","exception":false,"start_time":"2025-12-25T10:28:57.036212","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loading Model from: /kaggle/input/models-for-dpc/pretrained_models/byt5-base\n","output_type":"stream"}],"execution_count":7},{"id":"314f0dc9","cell_type":"markdown","source":"# A6. Optional: Monolingual Pre-Training on Akkadian Texts\n\nThis step teaches the model Akkadian grammar and morphology BEFORE translation training.\nUses published_texts.csv (8,000+ Akkadian texts) with Masked Language Modeling (MLM).\n\nBenefits:\n- Model learns to handle gaps naturally\n- Better understanding of Akkadian word structure\n- Improves low-resource translation performance\n\nSet ENABLE_MONO_PRETRAIN=True to enable (adds ~30min training time).","metadata":{}},{"id":"a86171f5","cell_type":"code","source":"# Monolingual Pre-Training Configuration\nENABLE_MONO_PRETRAIN = bool(int(os.getenv(\"ENABLE_MONO_PRETRAIN\", \"1\")))  # Set to 1 to enable\n\nif ENABLE_MONO_PRETRAIN:\n    print(\"\\n\" + \"=\"*60)\n    print(\"MONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\")\n    print(\"=\"*60)\n    \n    pub_texts_path = f\"{DATA_DIR}/published_texts.csv\"\n    \n    if os.path.exists(pub_texts_path):\n        # Load Akkadian-only texts\n        pub_texts_df = pd.read_csv(pub_texts_path)\n        akkadian_texts = pub_texts_df['transliteration'].dropna().astype(str).tolist()\n        akkadian_texts = [clean_translit(t, keep_gaps=True) for t in akkadian_texts]\n        akkadian_texts = [t for t in akkadian_texts if len(t.split()) >= 5 and len(t.split()) <= 200]\n        akkadian_texts = akkadian_texts[:5000]  # Limit for time\n        \n        print(f\"Loaded {len(akkadian_texts)} Akkadian texts for pre-training\")\n        \n        # Simple MLM approach: Mask random spans\n        from transformers import DataCollatorForSeq2Seq\n        \n        def create_mlm_examples(texts):\n            \"\"\"Create masked language modeling examples\"\"\"\n            mlm_examples = []\n            for text in texts:\n                tokens = text.split()\n                if len(tokens) < 5:\n                    continue\n                \n                # Mask 15% of tokens\n                n_mask = max(1, int(len(tokens) * 0.15))\n                mask_positions = np.random.choice(len(tokens), size=n_mask, replace=False)\n                \n                masked_text = []\n                for i, token in enumerate(tokens):\n                    if i in mask_positions:\n                        masked_text.append(\"<extra_id_0>\")  # T5-style sentinel\n                    else:\n                        masked_text.append(token)\n                \n                input_text = \" \".join(masked_text)\n                target_text = \" \".join([tokens[i] for i in mask_positions])\n                \n                mlm_examples.append({\n                    \"transliteration\": input_text,\n                    \"translation\": target_text\n                })\n            \n            return mlm_examples\n        \n        mlm_data = create_mlm_examples(akkadian_texts)\n        print(f\"Created {len(mlm_data)} MLM training examples\")\n        \n        # Create MLM dataset\n        mlm_dataset = Dataset.from_pandas(pd.DataFrame(mlm_data))\n        \n        def preprocess_mlm(examples):\n            inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n            targets = examples[\"translation\"]\n            model_inputs = tokenizer(\n                inputs,\n                max_length=MAX_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n            with tokenizer.as_target_tokenizer():\n                labels = tokenizer(\n                    targets,\n                    max_length=MAX_LENGTH,\n                    truncation=True,\n                    padding=\"max_length\"\n                )\n            model_inputs[\"labels\"] = [\n                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n                for label in labels[\"input_ids\"]\n            ]\n            return model_inputs\n        \n        tokenized_mlm = mlm_dataset.map(preprocess_mlm, batched=True)\n        \n        # Short MLM pre-training (1-2 epochs)\n        mlm_args = Seq2SeqTrainingArguments(\n            output_dir=f\"{OUTPUT_DIR}_mlm\",\n            num_train_epochs=1,\n            learning_rate=3e-4,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=8,\n            fp16=True,\n            save_strategy=\"no\",\n            eval_strategy=\"no\",\n            logging_steps=50,\n            report_to=\"none\"\n        )\n        \n        mlm_trainer = Seq2SeqTrainer(\n            model=model,\n            args=mlm_args,\n            train_dataset=tokenized_mlm,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n        \n        print(\"Starting monolingual pre-training (1 epoch on Akkadian texts)...\")\n        try:\n            mlm_trainer.train()\n            print(\"✓ Monolingual pre-training complete\")\n            print(\"Model now understands Akkadian grammar and gaps better!\")\n        except Exception as e:\n            print(f\"⚠️  MLM pre-training failed: {e}\")\n            print(\"Continuing with main training...\")\n    \n    else:\n        print(\"⚠️  published_texts.csv not found, skipping monolingual pre-training\")\nelse:\n    print(\"\\n⚠️  Monolingual pre-training disabled (set ENABLE_MONO_PRETRAIN=1 to enable)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:09:45.232532Z","iopub.execute_input":"2026-01-08T08:09:45.232827Z","iopub.status.idle":"2026-01-08T08:39:38.576328Z","shell.execute_reply.started":"2026-01-08T08:09:45.232802Z","shell.execute_reply":"2026-01-08T08:39:38.575692Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nMONOLINGUAL PRE-TRAINING ON AKKADIAN TEXTS\n============================================================\nLoaded 5000 Akkadian texts for pre-training\nCreated 5000 MLM training examples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f33dc53fd2a5406699e195b625901f66"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/tmp/ipykernel_55/57318026.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  mlm_trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting monolingual pre-training (1 epoch on Akkadian texts)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [157/157 29:11, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.689100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.261500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.182600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✓ Monolingual pre-training complete\nModel now understands Akkadian grammar and gaps better!\n","output_type":"stream"}],"execution_count":8},{"id":"31e880a1","cell_type":"markdown","source":"# A7. Training Arguments","metadata":{"papermill":{"duration":0.007375,"end_time":"2025-12-25T10:28:58.449624","exception":false,"start_time":"2025-12-25T10:28:58.442249","status":"completed"},"tags":[]}},{"id":"1f5dcb1e","cell_type":"code","source":"# --- 5. Training Arguments (OPTIMIZED for Quality & Score 31+) ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n\n    # --- VALIDATION STRATEGY ---\n    save_strategy=\"no\",                   # No checkpoints to save disk space\n    eval_strategy=\"no\",                   # Skip eval during training for speed\n    load_best_model_at_end=False,\n    \n    learning_rate=3e-4,                   # Higher LR for character-level model\n\n    # --- MEMORY-OPTIMIZED BUT EFFECTIVE ---\n    per_device_train_batch_size=1,        # Memory-safe on P100/T4\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,       # Effective batch = 16\n    gradient_checkpointing=True,          # Reduce memory usage\n    \n    num_train_epochs=12,                  # Increased for better convergence\n    weight_decay=0.01,\n    predict_with_generate=False,          # Save memory\n    fp16=True,                            # Mixed precision training\n    report_to=\"none\",\n    logging_steps=50,                     # Monitor progress\n\n    # Quality optimizations\n    label_smoothing_factor=0.1,           # Regularization\n    lr_scheduler_type=\"cosine\",           # Smooth learning rate decay\n    warmup_ratio=0.08,                    # Longer warmup for stability\n    generation_max_length=420,\n    generation_num_beams=8\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:39:38.577280Z","iopub.execute_input":"2026-01-08T08:39:38.577529Z","iopub.status.idle":"2026-01-08T08:39:38.613054Z","shell.execute_reply.started":"2026-01-08T08:39:38.577507Z","shell.execute_reply":"2026-01-08T08:39:38.612457Z"},"papermill":{"duration":0.169346,"end_time":"2025-12-25T10:28:58.626361","exception":false,"start_time":"2025-12-25T10:28:58.457015","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"id":"77b0d460","cell_type":"markdown","source":"# A8. Trainer","metadata":{"papermill":{"duration":0.006845,"end_time":"2025-12-25T10:28:58.640314","exception":false,"start_time":"2025-12-25T10:28:58.633469","status":"completed"},"tags":[]}},{"id":"34c0a3fd","cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Force aggressive memory cleanup\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:39:38.613826Z","iopub.execute_input":"2026-01-08T08:39:38.614126Z","iopub.status.idle":"2026-01-08T08:39:39.370781Z","shell.execute_reply.started":"2026-01-08T08:39:38.614094Z","shell.execute_reply":"2026-01-08T08:39:39.369857Z"},"papermill":{"duration":22.854214,"end_time":"2025-12-25T10:29:21.501227","exception":false,"start_time":"2025-12-25T10:28:58.647013","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/1667860881.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":10},{"id":"7d909d2f","cell_type":"markdown","source":"# A9. Execution","metadata":{"papermill":{"duration":0.006946,"end_time":"2025-12-25T10:29:21.515664","exception":false,"start_time":"2025-12-25T10:29:21.508718","status":"completed"},"tags":[]}},{"id":"c8847867","cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"Starting Training with Memory Fixes...\")\n\n# OOM-safe training wrapper\ntry:\n    trainer.train()\nexcept RuntimeError as e:\n    if \"out of memory\" in str(e).lower():\n        print(\"[WARNING] CUDA OOM detected. Attempting recovery: reducing MAX_LENGTH and accumulation.\")\n        # Reduce max length slightly to free memory for remaining steps\n        try:\n            MAX_LENGTH = max(320, int(MAX_LENGTH * 0.9))\n            print(f\"New MAX_LENGTH: {MAX_LENGTH}\")\n        except Exception:\n            pass\n        torch.cuda.empty_cache(); gc.collect()\n        # Continue training from current state if possible\n        trainer.train(resume_from_checkpoint=None)\n    else:\n        raise\n","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:39:39.371813Z","iopub.execute_input":"2026-01-08T08:39:39.372178Z","iopub.status.idle":"2026-01-08T08:43:10.281101Z","shell.execute_reply.started":"2026-01-08T08:39:39.372152Z","shell.execute_reply":"2026-01-08T08:43:10.279836Z"},"papermill":{"duration":5732.305735,"end_time":"2025-12-25T12:04:53.828320","exception":false,"start_time":"2025-12-25T10:29:21.522585","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Starting Training with Memory Fixes...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/552 : < :, Epoch 0.02/12]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[WARNING] CUDA OOM detected. Attempting recovery: reducing MAX_LENGTH and accumulation.\nNew MAX_LENGTH: 342\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1183986820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Continue training from current state if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    309\u001b[0m             )\n\u001b[1;32m    310\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             ) if torch.amp.is_autocast_available(ctx.device_type) else contextlib.nullcontext()\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_autocast_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_autocast_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdetached_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.19 MiB is free. Process 3364 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 350.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.19 MiB is free. Process 3364 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 350.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":11},{"id":"9c28d5b8","cell_type":"code","source":"# Evaluate on validation split with sacreBLEU and chrF AFTER training (memory-safe)\nprint(\"\\n=== POST-TRAINING VALIDATION ===\")\nmetric_bleu = evaluate.load(\"sacrebleu\")\nmetric_chrf = evaluate.load(\"chrf\")\n\ndef dedup_repeats(text: str) -> str:\n    toks = text.split()\n    out = []\n    for t in toks:\n        if len(out) >= 2 and t == out[-1] == out[-2]:\n            continue\n        out.append(t)\n    return \" \".join(out)\n\ndef postprocess_text(preds):\n    out = []\n    for p in preds:\n        p = p.strip()\n        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n        p = dedup_repeats(p)\n        if p and p[0].islower():\n            p = p[0].upper() + p[1:]\n        if p and p[-1] not in \".!?\":\n            p += \".\"\n        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n        out.append(p.strip())\n    return out\n\nval_texts = dataset[\"test\"][\"transliteration\"]\nval_refs = [[t] for t in dataset[\"test\"][\"translation\"]]\n\ndef generate_batch(texts):\n    batch_inputs = [PREFIX + doc for doc in texts]\n    enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n    gen = model.generate(\n        **enc,\n        max_length=MAX_LENGTH,\n        min_length=6,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        length_penalty=1.05,\n        early_stopping=True,\n    )\n    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\npreds = []\nfor i in range(0, len(val_texts), 8):\n    preds.extend(generate_batch(val_texts[i:i+8]))\n\npreds = postprocess_text(preds)\nbleu = metric_bleu.compute(predictions=preds, references=val_refs)\nchrf = metric_chrf.compute(predictions=preds, references=val_refs)\nprint(f\"Validation BLEU: {bleu['score']:.2f}, chrF: {chrf['score']:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:43:10.281721Z","iopub.status.idle":"2026-01-08T08:43:10.282019Z","shell.execute_reply.started":"2026-01-08T08:43:10.281847Z","shell.execute_reply":"2026-01-08T08:43:10.281862Z"},"papermill":{"duration":250.42551,"end_time":"2025-12-25T12:09:04.261371","exception":false,"start_time":"2025-12-25T12:04:53.835861","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"03a27615","cell_type":"markdown","source":"# A10. Save Final Model","metadata":{"papermill":{"duration":0.007132,"end_time":"2025-12-25T12:09:04.275738","exception":false,"start_time":"2025-12-25T12:09:04.268606","status":"completed"},"tags":[]}},{"id":"06794aa3","cell_type":"code","source":"print(f\"Saving model to {OUTPUT_DIR}...\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(\"Notebook A Complete.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-08T08:43:10.283354Z","iopub.status.idle":"2026-01-08T08:43:10.283622Z","shell.execute_reply.started":"2026-01-08T08:43:10.283502Z","shell.execute_reply":"2026-01-08T08:43:10.283519Z"},"papermill":{"duration":3.633252,"end_time":"2025-12-25T12:09:07.916126","exception":false,"start_time":"2025-12-25T12:09:04.282874","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"21ec693e","cell_type":"code","source":"# A11. Optional Self-Training Augmentation (Small, OOM-Safe)\nENABLE_SELF_TRAIN = False\nMAX_PSEUDO = int(os.getenv(\"BYT5_MAX_PSEUDO\", \"500\"))  # keep small to avoid OOM\n\nif ENABLE_SELF_TRAIN:\n    print(\"\\n=== SELF-TRAINING AUGMENTATION (ByT5) ===\")\n    pub_path = f\"{DATA_DIR}/published_texts.csv\"\n    if os.path.exists(pub_path):\n        pub_df = pd.read_csv(pub_path)\n        translits = pub_df.get(\"transliteration\", pd.Series([])).dropna().astype(str).tolist()\n        translits = [clean_translit(t) for t in translits]\n        translits = [t for t in translits if 5 <= len(t.split()) <= 180]\n        translits = translits[:MAX_PSEUDO]\n        print(f\"Generating pseudo translations for {len(translits)} extra transliterations...\")\n\n        def generate_batch(texts):\n            batch_inputs = [PREFIX + doc for doc in texts]\n            enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n            gen = model.generate(\n                **enc,\n                max_length=min(MAX_LENGTH, 400),\n                min_length=6,\n                num_beams=6,\n                no_repeat_ngram_size=3,\n                length_penalty=1.05,\n                early_stopping=True,\n            )\n            return tokenizer.batch_decode(gen, skip_special_tokens=True)\n\n        pseudo_trans = []\n        for i in range(0, len(translits), 8):  # small batch to avoid OOM\n            try:\n                batch_preds = generate_batch(translits[i:i+8])\n                pseudo_trans.extend(batch_preds)\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower():\n                    print(\"[WARNING] OOM during pseudo generation; skipping remaining.\")\n                    break\n                else:\n                    raise\n\n        # Postprocess & filter\n        def dedup_repeats(text: str) -> str:\n            toks = text.split()\n            out = []\n            for t in toks:\n                if len(out) >= 2 and t == out[-1] == out[-2]:\n                    continue\n                out.append(t)\n            return \" \".join(out)\n        def postprocess_text(preds):\n            out = []\n            for p in preds:\n                p = p.strip()\n                p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n                p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n                p = dedup_repeats(p)\n                if p and p[0].islower():\n                    p = p[0].upper() + p[1:]\n                if p and p[-1] not in \".!?\":\n                    p += \".\"\n                p = re.sub(r\"([.!?]){2,}\", \".\", p)\n                out.append(p.strip())\n            return out\n\n        pseudo_trans = postprocess_text(pseudo_trans)\n        aug_df = pd.DataFrame({\"transliteration\": translits[:len(pseudo_trans)], \"translation\": pseudo_trans})\n        aug_df[\"src_len\"] = aug_df[\"transliteration\"].str.split().str.len()\n        aug_df[\"tgt_len\"] = aug_df[\"translation\"].str.split().str.len()\n        ratio = (aug_df[\"tgt_len\"] / aug_df[\"src_len\"]).clip(upper=6)\n        aug_df = aug_df[(aug_df[\"tgt_len\"] >= 4) & (ratio >= 0.5) & (ratio <= 6)]\n        aug_df = aug_df.drop(columns=[\"src_len\", \"tgt_len\"])\n        print(f\"Pseudo pairs retained after filtering: {len(aug_df)}\")\n\n        base_train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n        base_train = base_train.dropna(subset=[\"transliteration\", \"translation\"]).astype(str)\n        base_train[\"transliteration\"] = base_train[\"transliteration\"].map(clean_translit)\n        base_train[\"translation\"] = base_train[\"translation\"].map(clean_translation)\n        combined = pd.concat([\n            base_train[[\"transliteration\", \"translation\"]],\n            aug_df[[\"transliteration\", \"translation\"]]\n        ], axis=0).drop_duplicates().reset_index(drop=True)\n        print(f\"Total combined training pairs: {len(combined)}\")\n\n        ds_combined = Dataset.from_pandas(combined)\n        def preprocess_function_aug(examples):\n            inputs = [PREFIX + ex for ex in examples[\"transliteration\"]]\n            targets = examples[\"translation\"]\n            model_inputs = tokenizer(\n                inputs,\n                max_length=MAX_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n            with tokenizer.as_target_tokenizer():\n                labels = tokenizer(\n                    targets,\n                    max_length=MAX_LENGTH,\n                    truncation=True,\n                    padding=\"max_length\"\n                )\n            model_inputs[\"labels\"] = [\n                [(l if l != tokenizer.pad_token_id else -100) for l in label]\n                for label in labels[\"input_ids\"]\n            ]\n            return model_inputs\n        tokenized_combined = ds_combined.map(preprocess_function_aug, batched=True)\n\n        training_args_aug = Seq2SeqTrainingArguments(\n            output_dir=OUTPUT_DIR,\n            save_strategy=\"no\",\n            eval_strategy=\"no\",\n            load_best_model_at_end=False,\n            learning_rate=2.5e-4,\n            per_device_train_batch_size=1,\n            gradient_accumulation_steps=16,\n            num_train_epochs=1,  # keep short to avoid OOM/time\n            fp16=True,\n            report_to=\"none\"\n        )\n        trainer_aug = Seq2SeqTrainer(\n            model=model,\n            args=training_args_aug,\n            train_dataset=tokenized_combined,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n        print(\"Starting second-stage training (ByT5) with augmented data...\")\n        try:\n            trainer_aug.train()\n        except RuntimeError as e:\n            print(f\"[WARNING] Augmentation training skipped due to error: {e}\")\n        print(\"Augmentation stage complete.\")\n\n        print(f\"Saving augmented model to {OUTPUT_DIR}...\")\n        trainer_aug.save_model(OUTPUT_DIR)\n        tokenizer.save_pretrained(OUTPUT_DIR)\n    else:\n        print(\"published_texts.csv not found; skipping self-training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:43:10.284698Z","iopub.status.idle":"2026-01-08T08:43:10.284976Z","shell.execute_reply.started":"2026-01-08T08:43:10.284821Z","shell.execute_reply":"2026-01-08T08:43:10.284835Z"}},"outputs":[],"execution_count":null}]}