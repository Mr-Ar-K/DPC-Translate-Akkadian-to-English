{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:13:42.877535Z",
     "iopub.status.busy": "2025-12-20T17:13:42.877253Z",
     "iopub.status.idle": "2025-12-20T17:13:43.042772Z",
     "shell.execute_reply": "2025-12-20T17:13:43.041941Z",
     "shell.execute_reply.started": "2025-12-20T17:13:42.877504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:13:43.045555Z",
     "iopub.status.busy": "2025-12-20T17:13:43.045096Z",
     "iopub.status.idle": "2025-12-20T17:13:48.057972Z",
     "shell.execute_reply": "2025-12-20T17:13:48.057158Z",
     "shell.execute_reply.started": "2025-12-20T17:13:43.045506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:13:48.059638Z",
     "iopub.status.busy": "2025-12-20T17:13:48.059313Z",
     "iopub.status.idle": "2025-12-20T17:14:16.286938Z",
     "shell.execute_reply": "2025-12-20T17:14:16.286147Z",
     "shell.execute_reply.started": "2025-12-20T17:13:48.059601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    " )\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/t5-base\"\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "OUTPUT_DIR = \"/kaggle/working/t5-base-fine-tuned\"\n",
    "\n",
    "MAX_LENGTH = 266 \n",
    "PREFIX = \"translate Akkadian to English: \"\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2. Data Loading & Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-20T17:14:16.288551Z",
     "iopub.status.busy": "2025-12-20T17:14:16.287892Z",
     "iopub.status.idle": "2025-12-20T17:14:16.489976Z",
     "shell.execute_reply": "2025-12-20T17:14:16.489349Z",
     "shell.execute_reply.started": "2025-12-20T17:14:16.288524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SUBSCRIPT_TRANS = str.maketrans({\"₀\": \"0\", \"₁\": \"1\", \"₂\": \"2\", \"₃\": \"3\", \"₄\": \"4\", \"₅\": \"5\", \"₆\": \"6\", \"₇\": \"7\", \"₈\": \"8\", \"₉\": \"9\", \"ₓ\": \"x\"})\n",
    "\n",
    "def normalize_subscripts(text: str) -> str:\n",
    "    return text.translate(SUBSCRIPT_TRANS)\n",
    "\n",
    "def clean_translit(text):\n",
    "    \"\"\"Normalize transliteration by stripping scribal marks and gaps.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = normalize_subscripts(text)\n",
    "    text = text.replace(\"…\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\\\.\\\\.\\\\.+\", \" <big_gap> \", text)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n",
    "    text = re.sub(r\"<<[^>]*>>\", \" \", text)\n",
    "    text = re.sub(r\"[˹˺]\", \" \", text)\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "    text = re.sub(r\"\\{([^}]*)\\}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[!?/:·]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def filter_quality(df):\n",
    "    df[\"src_len\"] = df[\"src\"].str.split().str.len()\n",
    "    df[\"tgt_len\"] = df[\"tgt\"].str.split().str.len()\n",
    "    df = df[(df[\"src_len\"] >= 3) & (df[\"tgt_len\"] >= 3)]\n",
    "    ratio = (df[\"src_len\"] / df[\"tgt_len\"]).clip(upper=6)\n",
    "    df = df[(ratio >= 0.2) & (ratio <= 5)]\n",
    "    df = df.drop_duplicates(subset=[\"src\", \"tgt\"])\n",
    "    return df.drop(columns=[\"src_len\", \"tgt_len\"])\n",
    "\n",
    "def load_and_align_data(filepath):\n",
    "    \"\"\"\n",
    "    Aligns Akkadian transliterations to English translations.\n",
    "    Tries to split by line/sentence; falls back to document-level if counts mismatch.\n",
    "    Filters low-quality pairs.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    aligned_rows = []\n",
    "\n",
    "    print(f\"Raw documents: {len(df)}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        src = clean_translit(row.get(\"transliteration\", \"\"))\n",
    "        tgt = clean_translation(row.get(\"translation\", \"\"))\n",
    "\n",
    "        src_lines = [s.strip() for s in src.split(\"\\n\") if len(s.strip()) > 1]\n",
    "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if len(t.strip()) > 1]\n",
    "\n",
    "        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n",
    "            for s, t in zip(src_lines, tgt_sents):\n",
    "                aligned_rows.append({\"src\": s, \"tgt\": t})\n",
    "        else:\n",
    "            merged_src = src.replace(\"\\n\", \" \")\n",
    "            if len(merged_src) > 3 and len(tgt) > 3:\n",
    "                aligned_rows.append({\"src\": merged_src, \"tgt\": tgt})\n",
    "\n",
    "    print(f\"Aligned training examples (pre-filter): {len(aligned_rows)}\")\n",
    "    out_df = filter_quality(pd.DataFrame(aligned_rows))\n",
    "    print(f\"Aligned training examples (post-filter): {len(out_df)}\")\n",
    "    return out_df\n",
    "\n",
    "df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:14:16.491041Z",
     "iopub.status.busy": "2025-12-20T17:14:16.490763Z",
     "iopub.status.idle": "2025-12-20T17:14:19.118204Z",
     "shell.execute_reply": "2025-12-20T17:14:19.117631Z",
     "shell.execute_reply.started": "2025-12-20T17:14:16.490999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Add prefix to inputs\n",
    "    inputs = [PREFIX + doc for doc in examples[\"src\"]]\n",
    "    targets = examples[\"tgt\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Replace padding token id with -100 to ignore in loss calculation\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply processing\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:14:19.119385Z",
     "iopub.status.busy": "2025-12-20T17:14:19.119090Z",
     "iopub.status.idle": "2025-12-20T17:14:20.599655Z",
     "shell.execute_reply": "2025-12-20T17:14:20.599055Z",
     "shell.execute_reply.started": "2025-12-20T17:14:19.119358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B5 . Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T17:14:20.601591Z",
     "iopub.status.busy": "2025-12-20T17:14:20.601269Z",
     "iopub.status.idle": "2025-12-20T17:14:20.754682Z",
     "shell.execute_reply": "2025-12-20T17:14:20.754061Z",
     "shell.execute_reply.started": "2025-12-20T17:14:20.601567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- CORRECTED B5. Training Configuration (Disk Space Safe) ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # --- DISK SPACE FIXES ---\n",
    "    save_strategy=\"no\",           # Do NOT save checkpoints during training\n",
    "    eval_strategy=\"epoch\",        # Evaluate every epoch\n",
    "    load_best_model_at_end=False, # Must be False if we aren't saving checkpoints\n",
    "    # ------------------------\n",
    "    \n",
    "    learning_rate=2e-4, \n",
    "    \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    fp16=True, # T5 is safe with fp16\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Accuracy-focused tweaks\n",
    "    label_smoothing_factor=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.04,\n",
    "    generation_max_length=280,\n",
    "    generation_num_beams=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B6. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-20T17:14:51.455Z",
     "iopub.execute_input": "2025-12-20T17:14:20.755863Z",
     "iopub.status.busy": "2025-12-20T17:14:20.755560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear cache before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting T5-Base Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation BLEU/chrF on held-out split\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def dedup_repeats(text: str) -> str:\n",
    "    toks = text.split()\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(out) >= 2 and t == out[-1] == out[-2]:\n",
    "            continue\n",
    "        out.append(t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def postprocess_text(preds):\n",
    "    out = []\n",
    "    for p in preds:\n",
    "        p = p.strip()\n",
    "        p = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", p)\n",
    "        p = re.sub(r\"([.,!?;:])([A-Za-z])\", r\"\\1 \\2\", p)\n",
    "        p = dedup_repeats(p)\n",
    "        if p and p[0].islower():\n",
    "            p = p[0].upper() + p[1:]\n",
    "        if p and p[-1] not in \".!?\":\n",
    "            p += \".\"\n",
    "        p = re.sub(r\"([.!?]){2,}\", \".\", p)\n",
    "        out.append(p.strip())\n",
    "    return out\n",
    "\n",
    "val_texts = dataset[\"test\"][\"src\"]\n",
    "val_refs = [[t] for t in dataset[\"test\"][\"tgt\"]]\n",
    "\n",
    "def generate_batch(texts):\n",
    "    batch_inputs = [PREFIX + doc for doc in texts]\n",
    "    enc = tokenizer(batch_inputs, max_length=MAX_LENGTH, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    gen = model.generate(\n",
    "        **enc,\n",
    "        max_length=MAX_LENGTH,\n",
    "        min_length=6,\n",
    "        num_beams=6,\n",
    "        no_repeat_ngram_size=3,\n",
    "        length_penalty=1.05,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "preds = []\n",
    "for i in range(0, len(val_texts), 16):\n",
    "    preds.extend(generate_batch(val_texts[i:i+16]))\n",
    "\n",
    "preds = postprocess_text(preds)\n",
    "bleu = metric_bleu.compute(predictions=preds, references=val_refs)\n",
    "chrf = metric_chrf.compute(predictions=preds, references=val_refs)\n",
    "print({\"bleu\": bleu[\"score\"], \"chrf\": chrf[\"score\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-20T17:14:51.456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Notebook B (T5) Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9082937,
     "sourceId": 14236819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
