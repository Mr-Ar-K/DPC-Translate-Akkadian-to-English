{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Set constants (DO NOT change yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/kaggle/input/models-for-dpc/pretrained_models/byt5-base\"\n",
    "DATA_DIR = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "OUTPUT_DIR = \"./byt5-base-saved\"\n",
    "\n",
    "# ByT5 is character-based, so sequences are longer. \n",
    "# Akkadian texts can be long, but 512 is usually a safe upper limit for sentence-level.\n",
    "MAX_LENGTH = 400 \n",
    "PREFIX = \"translate Akkadian to English: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_translit(text):\n",
    "    \"\"\"Normalize transliteration by stripping scribal marks and gaps.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" <big_gap> \")\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \" <big_gap> \", text)\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", text)\n",
    "    text = re.sub(r\"<([^>]*)>\", r\"\\1\", text)\n",
    "    text = re.sub(r\"[!?/]\", \" \", text)\n",
    "    text = re.sub(r\"[:]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_translation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"…\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_align_data(filepath):\n",
    "    \"\"\"\n",
    "    Reads the csv and aligns transliterations to translations at the sentence level.\n",
    "    This is critical because the raw data is document-level.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    aligned_rows = []\n",
    "\n",
    "    print(f\"Raw documents: {len(df)}\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        src = clean_translit(row.get(\"transliteration\", \"\"))\n",
    "        tgt = clean_translation(row.get(\"translation\", \"\"))\n",
    "\n",
    "        # Split source by newlines (traditional for cuneiform transliteration)\n",
    "        src_lines = [s.strip() for s in src.split(\"\\n\") if s.strip()]\n",
    "        \n",
    "        # Split target by sentence punctuation\n",
    "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n",
    "\n",
    "        # If the counts match roughly, we assume 1:1 mapping (High Quality Data)\n",
    "        if len(src_lines) == len(tgt_sents) and len(src_lines) > 1:\n",
    "            for s, t in zip(src_lines, tgt_sents):\n",
    "                # Filter out very short garbage fragments\n",
    "                if len(s) > 3 and len(t) > 3:\n",
    "                    aligned_rows.append({\"transliteration\": s, \"translation\": t})\n",
    "        else:\n",
    "            # Fallback: Use the whole document if we can't align perfectly\n",
    "            # This ensures we don't lose data, even if it's noisy\n",
    "            aligned_rows.append({\"transliteration\": src.replace(\"\\n\", \" \"), \"translation\": tgt})\n",
    "\n",
    "    print(f\"Aligned training examples: {len(aligned_rows)}\")\n",
    "    return pd.DataFrame(aligned_rows)\n",
    "\n",
    "# Load data\n",
    "train_df = load_and_align_data(f\"{DATA_DIR}/train.csv\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "# Split: 95% Train, 5% Validation (we need max data for training)\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5 . Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading Tokenizer from:\", MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"transliteration\"]]\n",
    "    targets = examples[\"translation\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\" # Consistent padding helps training stability\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Replace padding token id with -100 so it's ignored by the loss function\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Process datasets\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_val = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading Model from:\", MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Data Collator handles dynamic padding during batching\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 5. Training Arguments ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # --- CRITICAL FIX: Stop saving checkpoints to save disk space ---\n",
    "    save_strategy=\"no\",           # Do not save every epoch\n",
    "    eval_strategy=\"epoch\",        # Still evaluate to see progress\n",
    "    load_best_model_at_end=False,  # Must be False if save_strategy is \"no\"\n",
    "    # ---------------------------------------------------------------\n",
    "    \n",
    "    learning_rate=3e-4,\n",
    "    \n",
    "    # Keep your memory fixes\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    num_train_epochs=9, #increased to 9 from 5\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Accuracy-focused tweaks\n",
    "    label_smoothing_factor=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.04,\n",
    "    generation_max_length=420,\n",
    "    generation_num_beams=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A8. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A9. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting Training with Memory Fixes...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A10. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Notebook A Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14976537,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9082937,
     "sourceId": 14236819,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
