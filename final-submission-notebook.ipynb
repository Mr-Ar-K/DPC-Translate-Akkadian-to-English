{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-25T12:14:36.175832Z",
     "iopub.status.busy": "2025-12-25T12:14:36.175605Z",
     "iopub.status.idle": "2025-12-25T12:15:40.422643Z",
     "shell.execute_reply": "2025-12-25T12:15:40.422026Z",
     "shell.execute_reply.started": "2025-12-25T12:14:36.175811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Deep Past Neural Ensemble Inference ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 12:14:51.095337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766664891.298313      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766664891.357816      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766664891.851037      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851082      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851085      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766664891.851088      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286a0586afeb435ea31d31b4777fc8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference byt5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453f6b3dd3304ef99924faf6e3a6a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference t5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9f6a77e75d47d4adc7de4402091a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference marian:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview:\n",
      "   id                                        translation\n",
      "0   0  Kà-ar-ma ú big_gap da-tim aí-ip-ri-ni Akkadian...\n",
      "1   1  -ni i-na né-mì-lim da-aùr ú-lá e-WA ia-ra-tí-a...\n",
      "2   2  -it a-aí-im au-um-au ia-tí aé-bi„-lá-nim Trans...\n",
      "3   3  É-bi„-lá KÙ. AN Translate Akkadian to English:...\n"
     ]
    }
   ],
   "source": [
    "# DEEP PAST CHALLENGE - ENSEMBLE SUBMISSION\n",
    "\n",
    "# Install sacremoses for MarianMT (required in offline mode)\n",
    "try:\n",
    "    import sacremoses\n",
    "    print(\"✓ sacremoses already installed\")\n",
    "except ImportError:\n",
    "    import os\n",
    "    wheel_dir = \"/kaggle/input/sacremoses-wheel\"\n",
    "    wheel_glob = \"sacremoses-*.whl\"\n",
    "    if os.path.exists(wheel_dir):\n",
    "        print(\"Installing sacremoses from local wheel...\")\n",
    "        import glob, sys, subprocess\n",
    "        wheels = glob.glob(f\"{wheel_dir}/{wheel_glob}\")\n",
    "        if wheels:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheels[0], \"--no-deps\", \"-q\"])\n",
    "            import sacremoses\n",
    "            print(\"✓ sacremoses installed from wheel\")\n",
    "        else:\n",
    "            print(\"⚠️ No sacremoses wheel found in dataset folder\")\n",
    "    else:\n",
    "        print(\"⚠️ sacremoses not found - MarianMT may fail\")\n",
    "        print(\"To fix: Upload sacremoses wheel as dataset and run:\")\n",
    "        print(\"!pip install /kaggle/input/sacremoses-wheel/sacremoses-*.whl --no-deps\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL1_PATH = os.getenv(\"MODEL1_PATH\", \"/kaggle/input/notebook-a-byt5/byt5-base-saved\")\n",
    "MODEL2_PATH = os.getenv(\"MODEL2_PATH\", \"/kaggle/input/notebook-b-t5/t5-base-fine-tuned\")\n",
    "MODEL3_PATH = os.getenv(\"MODEL3_PATH\", \"/kaggle/input/notebook-c-marian-mt/marian-mt-saved\")\n",
    "\n",
    "TEST_DATA_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENSEMBLE STRATEGY: AUTO-DETECT\n",
    "# -----------------------------------------------------------------------------\n",
    "# This notebook automatically chooses the best strategy:\n",
    "# 1. Weight averaging if all models have same architecture\n",
    "# 2. Voting ensemble if models have different architectures\n",
    "# 3. Best single model as fallback\n",
    "\n",
    "ENSEMBLE_MODE = os.getenv(\"ENSEMBLE_MODE\", \"auto\")  # \"auto\", \"voting\", or \"averaging\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODEL CONFIGURATIONS & WEIGHTS\n",
    "# -----------------------------------------------------------------------------\n",
    "# Adjust these weights based on validation scores from find-optimal-weights.ipynb\n",
    "# Or use equal weights as baseline: {\"weight\": 0.333}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"byt5\": {\n",
    "        \"path\": MODEL1_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.35,  # Adjust based on validation\n",
    "        \"num_beams\": 4\n",
    "    },\n",
    "    \"t5\": {\n",
    "        \"path\": MODEL2_PATH,\n",
    "        \"prefix\": \"translate Akkadian to English: \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.40,  # Usually best performer\n",
    "        \"num_beams\": 4\n",
    "    },\n",
    "    \"marian\": {\n",
    "        \"path\": MODEL3_PATH,\n",
    "        \"prefix\": \">>eng<< \",\n",
    "        \"max_length\": 512,\n",
    "        \"weight\": 0.25,  # Adjust based on validation\n",
    "        \"num_beams\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Ensemble Mode: {ENSEMBLE_MODE}\")\n",
    "print(f\"\\nModel Weights:\")\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    exists = \"✓\" if os.path.exists(config[\"path\"]) else \"✗\"\n",
    "    print(f\"  {name:10s} {exists} weight={config['weight']:.2f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GAP REPLACEMENT FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def replace_gaps(text):\n",
    "    \"\"\"Replace various gap notations with standardized tokens\"\"\"\n",
    "    if pd.isna(text): \n",
    "        return text\n",
    "    \n",
    "    # Complex gap patterns (order matters)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+\\s+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.3(?:\\s+\\.3)+\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3}(?:\\s+\\.{3})+', '<big_gap>', text)\n",
    "\n",
    "    # Simple gap patterns\n",
    "    text = re.sub(r'xx', '<gap>', text)\n",
    "    text = re.sub(r' x ', ' <gap> ', text)\n",
    "    text = re.sub(r'……', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.\\.\\.\\.', '<big_gap>', text)\n",
    "    text = re.sub(r'…', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.\\.\\.', '<big_gap>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD TEST DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "test_df['transliteration'] = test_df['transliteration'].apply(replace_gaps)\n",
    "test_inputs = test_df['transliteration'].astype(str).tolist()\n",
    "source_lengths = [len(t.split()) for t in test_inputs]\n",
    "\n",
    "print(f\"✓ Loaded {len(test_df)} test samples\")\n",
    "print(f\"✓ Average source length: {sum(source_lengths)/len(source_lengths):.1f} words\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# INFERENCE DATASET\n",
    "# -----------------------------------------------------------------------------\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length, prefix=\"\"):\n",
    "        self.texts = [prefix + str(t) for t in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override per-model generation configs to match training lengths and improve quality\n",
    "MODEL_CONFIGS['byt5']['max_length'] = 256\n",
    "MODEL_CONFIGS['t5']['max_length'] = 128\n",
    "MODEL_CONFIGS['marian']['max_length'] = 160\n",
    "for k in MODEL_CONFIGS.keys():\n",
    "    MODEL_CONFIGS[k]['num_beams'] = 6\n",
    "print(\"Adjusted MODEL_CONFIGS: max_length per model and num_beams=6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SUBMISSION GUIDE\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Train all 3 models** using the training notebooks\n",
    "2. **Save each as a Kaggle dataset**\n",
    "3. **Add datasets as inputs** to this notebook\n",
    "4. **Adjust weights** in MODEL_CONFIGS (use find-optimal-weights.ipynb)\n",
    "5. **Run this notebook** → generates submission.csv\n",
    "\n",
    "## Ensemble Modes\n",
    "\n",
    "Set `ENSEMBLE_MODE` in the first cell:\n",
    "- **\"auto\"** (default) - Tries weight averaging, falls back to voting\n",
    "- **\"voting\"** - Always use voting ensemble (slower but works with any models)\n",
    "- **\"averaging\"** - Only use weight averaging (faster but requires compatible models)\n",
    "\n",
    "## How to Find Optimal Weights\n",
    "\n",
    "Run `find-optimal-weights.ipynb` to automatically:\n",
    "1. Evaluate each model on validation set\n",
    "2. Calculate BLEU scores\n",
    "3. Grid search for best weight combination\n",
    "4. Output optimal weights\n",
    "\n",
    "Then update MODEL_CONFIGS with the results.\n",
    "\n",
    "## Default Weights\n",
    "\n",
    "Current configuration (adjust based on your validation):\n",
    "- **ByT5**: 0.35 (character-level, good for morphology)\n",
    "- **T5**: 0.40 (usually best overall)\n",
    "- **MarianMT**: 0.25 (translation-focused)\n",
    "\n",
    "## Model Paths\n",
    "\n",
    "Update these if your datasets have different names:\n",
    "- MODEL1_PATH: `/kaggle/input/notebook-a-byt5/byt5-base-saved`\n",
    "- MODEL2_PATH: `/kaggle/input/notebook-b-t5/t5-base-fine-tuned`\n",
    "- MODEL3_PATH: `/kaggle/input/notebook-c-marian-mt/marian-mt-saved`\n",
    "\n",
    "## Tips for Better Performance\n",
    "\n",
    "1. **Use validation scores** to set weights (don't guess!)\n",
    "2. **Increase num_beams** to 6 or 8 for better quality (slower)\n",
    "3. **Adjust max_length** per model based on typical output length\n",
    "4. **Add repetition_penalty=1.2** if outputs are too repetitive\n",
    "5. **Post-process** predictions for capitalization and punctuation\n",
    "\n",
    "## Expected Behavior\n",
    "\n",
    "- If all models have **same architecture** → Uses weight averaging (faster)\n",
    "- If models have **different architectures** → Uses voting ensemble (more robust)\n",
    "- If only **1 model** available → Uses single model\n",
    "- Automatically handles missing models gracefully\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Out of Memory?**\n",
    "- Reduce BATCH_SIZE from 8 to 4 or 2\n",
    "- Use ENSEMBLE_MODE=\"averaging\" (loads only 1 model at a time)\n",
    "\n",
    "**Low scores?**\n",
    "- Run find-optimal-weights.ipynb to get better weights\n",
    "- Check that gap replacement is working (should see `<gap>` and `<big_gap>`)\n",
    "- Verify all models trained properly\n",
    "\n",
    "**Models won't merge?**\n",
    "- Normal! ByT5, T5, and MarianMT have different architectures\n",
    "- Notebook will automatically use voting ensemble instead\n",
    "- This is actually better for diverse models\n",
    "\n",
    "## Architecture Compatibility\n",
    "\n",
    "| Models | Compatible? | Strategy |\n",
    "|--------|-------------|----------|\n",
    "| ByT5 + T5 | Maybe ✓ | Can try averaging |\n",
    "| ByT5 + MarianMT | No ✗ | Use voting |\n",
    "| T5 + MarianMT | No ✗ | Use voting |\n",
    "| All 3 | No ✗ | Use voting |\n",
    "\n",
    "Voting ensemble is **more powerful** for different architectures anyway!\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9084006,
     "sourceId": 14290093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084005,
     "sourceId": 14290098,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9084224,
     "sourceId": 14290100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
